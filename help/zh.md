# Meta-Lingo 使用手册

欢迎使用 Meta-Lingo，一款现代化多模态语料库研究软件。本手册将帮助您掌握软件的各项功能。

# 关于 Meta-Lingo

## 应用简介

![Meta-Lingo](assets/Background2.jpg)

**Meta-Lingo** 是一款现代化多模态语料库研究平台，为语言学研究提供全流程智能化解决方案。

### 核心特点

- **本地运行**：完全本地化运行，无需联网
- **隐私安全**：数据存储在本地，保护研究数据隐私
- **免费使用**：面向学术研究，免费提供
- **多模态支持**：支持文本、音频、视频等多种语料类型
- **智能标注**：集成 SpaCy、Whisper、YOLO、CLIP 等先进模型

### 技术架构

Meta-Lingo 基于以下技术栈构建：

- **前端**：Electron + React + TypeScript + Material-UI
- **后端**：Python + FastAPI
- **NLP**：SpaCy、NLTK、Transformers
- **多模态**：Whisper（语音转录）、YOLO（物体检测）、CLIP（语义分类）
- **主题建模**：BERTopic、LDA、LSA、NMF

## 功能模块概览

Meta-Lingo 提供 12 大核心功能模块：

| 模块 | 功能描述 |
|------|----------|
| **语料库管理** | 上传、组织和管理多模态语料数据 |
| **词频统计** | 词频分析、词性筛选、多维可视化 |
| **同义词分析** | 基于 WordNet 的同义词关系分析 |
| **关键词提取** | TF-IDF、TextRank、YAKE、RAKE 等多种算法 |
| **N-gram 分析** | 2-6 gram 组合频率统计和模式发现 |
| **共现关系** | KWIC 搜索、CQL 查询、搭配分析 |
| **语义分析** | USAS 语义域分类和统计 |
| **词图分析** | Word Sketch 语法搭配分析 |
| **文献可视化** | 文献数据导入和可视化分析 |
| **标注模式** | 多层级文本标注和多模态标注 |
| **主题建模** | BERTopic、LDA、LSA、NMF 主题发现 |
| **词典查询** | 多词典集成查询 |

## 开发者简介

![开发者头像](assets/avatar.png)

### Tommy Leo

Meta-Lingo 的开发者，2025 年入学广东外语外贸大学商务英语语言研究专业的硕士研究生。

### 开发初衷

在使用传统语言研究工具时，我发现了一些不足之处：

- 操作繁琐，学习成本高
- 功能分散在不同软件中，需要频繁切换
- 缺乏统一的数据管理和分析流程
- 对多模态语料的支持有限

### 开发愿景

打造一个 **All-in-One** 的语言研究软件，将词频分析、搭配分析、语义分析、主题建模等功能整合在一个平台中。Meta-Lingo 结合了最新的 NLP 模型和大语言模型技术，实现完全本地化运行，既保证数据隐私，又提供强大的分析能力。

### 联系方式

如果您在使用过程中有任何问题或建议，欢迎通过以下方式联系我：

- **Bilibili**：[https://space.bilibili.com/294707614](https://space.bilibili.com/294707614)
- **小红书**：[https://www.xiaohongshu.com/user/profile/5af2c7734eacab77509ec3af](https://www.xiaohongshu.com/user/profile/5af2c7734eacab77509ec3af)
- **YouTube**：[https://www.youtube.com/@metalingo2026](https://www.youtube.com/@metalingo2026)
- **邮箱**：1683619168tl@gmail.com

## 重要声明

本软件为个人开发项目，非商业用途，非广东外语外贸大学官方软件。使用本软件即表示您同意遵守相关许可协议。

---

# 语料库管理

语料库管理模块是 Meta-Lingo 的核心功能，用于上传、组织和管理您的多模态语料数据。

## 界面概览

语料库管理模块包含三个主要标签页：

- **上传语料**：创建新语料库或向现有语料库上传文件
- **语料库列表**：浏览和管理所有语料库
- **语料库详情**：查看语料库的详细信息和文本列表

## 创建语料库

### 创建新语料库

1. 进入「上传语料」标签页
2. 勾选「创建新语料库」选项
3. 填写语料库元数据：
   - **名称**（必填）：为语料库设置一个有意义的名称
   - **语言**：选择语料库的主要语言（支持中文和英文）
   - **文本类型**：选择文本类型，用于 USAS 语义域标注的消歧
   - **来源**：选择或自定义语料来源
   - **作者**：填写语料作者信息
   - **日期**（必填）：选择文本的创建或发布日期
   - **描述**：添加语料库的详细描述
   - **标签**：添加标签便于分类和检索

### 向现有语料库上传

1. 取消勾选「创建新语料库」
2. 从下拉列表选择目标语料库
3. 选择文本类型（会应用到所有上传的文件）
4. 选择日期（必填，会应用到所有上传的文件）

## 上传文件

### 支持的文件格式

Meta-Lingo 支持多种文件格式：

| 类型 | 支持格式 |
|------|----------|
| 文本 | .txt |
| 音频 | .mp3, .wav, .m4a, .flac, .ogg |
| 视频 | .mp4, .avi, .mkv, .mov, .webm |

### 上传方式

- **拖拽上传**：将文件直接拖拽到上传区域
- **点击上传**：点击上传区域，从文件管理器选择文件

支持批量上传多个文件。

### 处理选项

上传音视频文件时，可以配置以下处理选项：

#### 音频转录 (Whisper)

启用后，系统将使用 Whisper Large V3 Turbo 模型自动转录音频内容：
- 支持词级时间戳
- 自动进行 SpaCy、USAS、MIPVU 语言标注
- 转录结果按句分割保存

#### 英语音频强制对齐 (Wav2Vec2)

对于英语音频，系统会在 Whisper 转录完成后自动执行：
- **Wav2Vec2 强制对齐**：使用 `wav2vec2-base-960h` 模型生成词级时间戳
- **TorchCrepe 音高提取**：使用 `full.pth` 模型提取基频 (F0) 数据
- 这些数据用于多模态标注中的波形可视化

> **注意**：中文音频不支持强制对齐和音高提取，中文音频只能在纯文本标注模式下进行标注。

#### YOLO 物体检测

针对视频文件，启用后将使用 YOLOv8 进行：
- 物体检测
- 目标追踪
- 检测结果以 JSON 格式保存

#### CLIP 语义分类

针对视频文件，启用后将使用 CLIP 模型进行逐帧语义分类：

- **预设标签分类**：
  - 物体：person, animal, vehicle, food, text, logo
  - 场景：indoor, outdoor, nature, building, urban, rural
  - 氛围：bright, dark, colorful, monochrome, warm, cool
  - 动态：action, static, crowded, empty, fast, slow

- **自定义标签**：可以添加任意自定义分类标签

- **帧间隔设置**：
  - 默认 30 帧进行一次分类
  - 设为 1 可进行逐帧标注
  - 较小的帧间隔会增加处理时间

### 上传进度

上传音视频文件后，系统会在后台异步处理，您可以实时查看：

- **处理阶段**：初始化 → 提取音频 → 转录 → SpaCy 标注 → YOLO 检测 → CLIP 分类 → 保存
- **进度百分比**：实时显示当前处理进度
- **状态消息**：显示当前正在执行的操作

处理完成后会自动刷新语料库详情。

## 语料库列表

### 视图模式

- **卡片视图**：以卡片形式展示语料库，显示统计信息和标签
- **列表视图**：以表格形式展示，便于批量操作

### 筛选功能

- **搜索**：按名称、描述、标签搜索语料库
- **语言筛选**：按语言过滤语料库
- **标签筛选**：按标签过滤语料库

### 语料库操作

右键点击或点击菜单按钮可进行：

- **查看详情**：进入语料库详情页面
- **编辑**：修改语料库元数据
- **删除**：删除语料库及其所有内容（不可恢复）

## 语料库详情

### 元数据面板

左侧显示语料库的基本信息：
- 语言
- 作者
- 来源
- 文本类型
- 描述
- 标签

### 文本列表

右侧表格显示语料库中的所有文本：

| 列名 | 说明 |
|------|------|
| 类型 | 文本/音频/视频图标 |
| 文件名 | 原始文件名及处理状态标签 |
| 词数 | 文本或转录的词数统计 |
| 时长 | 音视频的时长 |
| 日期 | 文本的日期元数据 |
| 作者 | 文本作者 |
| 来源 | 文本来源 |
| 文本类型 | 文本类型 |
| 标签 | 文本标签 |
| 操作 | 编辑、查看、删除按钮 |

### 文本筛选

- **搜索框**：按文件名或标签搜索
- **类型筛选**：筛选文本、音频或视频文件

### 批量重新标注

选择一个或多个文本后，可以进行批量重新标注：

#### SpaCy 重新标注
- 重新进行词性标注 (POS)
- 命名实体识别 (NER)
- 依存句法分析

#### YOLO 重新标注
- 仅适用于视频文件
- 重新进行物体检测和追踪

#### CLIP 重新标注
- 仅适用于视频文件
- 可重新选择分类标签
- 可调整帧间隔

#### USAS 重新标注
- 重新进行语义域标注
- 使用最新的优先语义域设置

### 查看文本内容

点击文本行可查看详细内容：

- **文本文件**：显示纯文本内容
- **音频文件**：显示音频播放器和转录文本
- **视频文件**：显示视频播放器和转录文本

转录文本以时间戳形式显示，点击段落可定位播放。

### 编辑功能

点击「编辑」按钮可以：

#### 直接编辑
- 文本语料：直接编辑文本内容
- 音视频转录：逐句编辑转录文本

#### 查找/替换
- 支持普通文本搜索
- 支持正则表达式
- 支持区分大小写

#### 实体提取
从文本中提取并移除：
- 邮箱地址
- 网址链接
- 电话号码
- IP 地址

#### 文本标准化
- Unicode 标准化（NFKC）
- 空白字符规范化
- 合并连续换行
- 移除特殊符号
- 移除 HTML 标签
- 移除控制字符

### 标签管理

- **添加标签**：点击文本行的加号按钮
- **删除标签**：点击标签的删除图标

### 元数据编辑

点击编辑图标可修改文本的元数据：
- 日期
- 作者
- 来源

## 自动标注功能

### SpaCy 自动标注

上传文本时，系统自动进行 SpaCy 标注：
- **词性标注 (POS)**：使用 Universal POS 标签集
- **命名实体识别 (NER)**：识别人名、地名、组织等
- **依存句法分析**：分析词语间的句法关系
- **词形还原 (Lemma)**：获取词语的原形

### USAS 语义域标注

系统自动进行 USAS 语义域标注：
- 基于 PyMUSAS 标注引擎
- 支持中文和英文
- 话语域识别消歧
- 一文一义消歧
- 可在设置中配置优先语义域

## 数据存储

### 存储位置

- **元数据**：存储在 SQLite 数据库 (`data/database.sqlite`)
- **文本文件**：存储在 `data/corpora/{语料库名称}/files/`
- **音频文件**：存储在 `data/corpora/{语料库名称}/audios/`
- **视频文件**：存储在 `data/corpora/{语料库名称}/videos/`
- **标注数据**：以 JSON 格式存储在对应目录

### 备份建议

定期备份 `data/` 目录以保护您的语料数据。

## 常见问题

### 上传失败怎么办？

1. 检查文件格式是否支持
2. 检查文件大小是否过大
3. 确保后端服务正常运行
4. 查看控制台错误信息

### 转录结果不准确？

1. 确保音频质量清晰
2. 选择正确的转录语言
3. 可以手动编辑转录文本

### 处理进度卡住？

1. 音视频处理需要较长时间，请耐心等待
2. 可以刷新页面查看最新状态
3. 如果长时间无响应，检查后端服务

## 使用建议

### 大语料分批上传

对于较大的语料库，建议将语料分成多份分批上传：

- **原因**：上传时间过长可能导致前后端连接超时断开
- **建议**：每批上传 50-100 个文本文件，或总大小不超过 500MB
- **音视频**：由于需要转录和标注，建议每批上传 5-10 个文件

### 上传卡住的处理

如果上传过程中出现卡住现象（通常是因为连接超时）：

1. **重启应用**：关闭并重新打开 Meta-Lingo
2. **检查标注状态**：在语料库详情中查看文本的标注状态
3. **重新标注**：
   - 对于 SpaCy 标注未完成的文本，选中后点击「SpaCy 重新标注」按钮
   - 对于 USAS 标注未完成的文本，选中后点击「USAS 重新标注」按钮
4. **批量处理**：可以多选文本后批量重新标注

### 数据保留说明

**重要提示**：在以下情况下，您的语料库数据会被保留：

- **删除应用**：直接删除应用程序不会删除语料库数据
- **重装新版本**：升级或重新安装新版本时，语料库数据会自动保留
- **数据位置**：语料库数据存储在 `data/` 目录中，与应用程序分离

**注意**：只有使用「应用设置」中的「恢复出厂设置」功能并勾选相应选项时，才会删除语料库数据。

# 词频分析

## 概述

词频分析模块基于 SpaCy 标注数据，提供全面的词频统计功能。支持词性筛选、多种搜索模式、频率范围过滤，并提供丰富的可视化展示。

## 界面布局

词频分析模块采用左右分栏布局：

- **左侧面板**（400px）：配置面板，包含语料选择、词性筛选、搜索配置等
- **右侧面板**（弹性宽度）：结果展示区域，包含两个标签页
  - **结果表格**：显示词频统计结果
  - **可视化**：提供多种图表展示

## 语料选择

### 选择语料库

1. 在左侧面板顶部的下拉菜单中选择目标语料库
2. 系统会显示语料库的文本数量
3. 选择语料库后，系统会自动加载该语料库中的所有文本

### 文本选择模式

系统提供三种文本选择模式：

#### 全部文本
- 选择「全部文本」模式
- 分析将包含语料库中的所有文本
- 显示文本总数

#### 按标签筛选
- 选择「按标签筛选」模式
- 从下拉菜单中选择一个或多个标签
- 系统会筛选出包含这些标签的所有文本
- 显示筛选后的文本数量

#### 手动选择
- 选择「手动选择」模式
- 使用搜索框按文件名搜索文本
- 在文本列表中勾选需要分析的文本
- 支持「全选」和「清空」快捷操作
- 显示已选择的文本数量

### 选择状态提示

选择完成后，系统会显示：
- 已选择的文本数量
- 成功/警告提示（根据选择数量）

## 词性筛选

词性筛选基于 SpaCy Universal POS 标签集，允许您只分析特定词性的词语。

### 筛选模式

#### 保留模式
- 选择「保留模式」
- 只统计您选择的词性标签
- 例如：选择 NOUN（名词）和 VERB（动词），则只统计名词和动词

#### 过滤模式
- 选择「过滤模式」
- 排除您选择的词性标签
- 例如：选择 PUNCT（标点），则排除所有标点符号

### 词性标签分类

系统将词性标签分为三类，便于选择：

#### 实词（Content Words）
- **NOUN**：名词
- **VERB**：动词
- **ADJ**：形容词
- **ADV**：副词
- **PROPN**：专有名词

#### 虚词（Function Words）
- **ADP**：介词
- **AUX**：助动词
- **CCONJ**：并列连词
- **DET**：限定词
- **PART**：助词
- **PRON**：代词
- **SCONJ**：从属连词

#### 其他（Other）
- **INTJ**：感叹词
- **NUM**：数词
- **PUNCT**：标点
- **SYM**：符号
- **X**：其他

### 快捷操作

- **全选**：快速选择所有词性标签
- **清空**：清除所有选择

### 使用建议

- 分析实词时，建议使用「保留模式」选择实词类别
- 分析动词时，选择「保留模式」并只选择 VERB
- 排除标点符号时，使用「过滤模式」选择 PUNCT

## 搜索配置

搜索配置面板提供多种过滤和搜索选项，帮助您精确定位目标词语。

### 频率范围

设置词语的频率范围：

- **最小频率**：只显示出现次数大于等于此值的词语（默认：1）
- **最大频率**：只显示出现次数小于等于此值的词语（可选，设为 0 表示无限制）

### 大小写处理

- **转换为小写**：勾选后，所有词语统一转换为小写进行统计
- 建议：分析英文时建议开启，以合并大小写变体

### 搜索目标

选择统计的目标形式：

- **词形（Word）**：使用词语的原始形式（如 "running", "runs", "ran" 分别统计）
- **词根（Lemma）**：使用词语的词根形式（如 "running", "runs", "ran" 统一统计为 "run"）

**使用建议**：
- 词形模式：适合分析词语的具体使用形式
- 词根模式：适合分析词语的语义分布，合并同一词的不同变体

### 搜索类型

#### 全部
- 不进行搜索过滤，统计所有符合条件的词语

#### 开头匹配（Starts）
- 只统计以指定字符串开头的词语
- 例如：输入 "un" 可统计 "unhappy", "unclear" 等

#### 结尾匹配（Ends）
- 只统计以指定字符串结尾的词语
- 例如：输入 "ing" 可统计 "running", "playing" 等

#### 包含匹配（Contains）
- 只统计包含指定字符串的词语
- 例如：输入 "tion" 可统计 "action", "education" 等

#### 正则表达式（Regex）
- 使用正则表达式进行高级匹配
- 支持完整的正则表达式语法
- 例如：`^[A-Z].*` 匹配所有首字母大写的词语

#### 词表匹配（Wordlist）
- 输入一个词语列表（每行一个）
- 只统计列表中的词语
- 适合分析特定词汇集合

### 排除词语

在「排除词语」文本框中输入要排除的词语（每行一个），这些词语将不会出现在统计结果中。

**使用场景**：
- 排除停用词
- 排除特定干扰词
- 排除人名、地名等专有名词

## 运行分析

配置完成后，点击「开始分析」按钮：

1. 系统会显示加载进度
2. 分析完成后，结果会自动显示在右侧面板
3. 如果出现错误，会在左侧面板显示错误信息

## 结果表格

### 统计信息

表格顶部显示统计摘要：

- **总词数**：所有符合条件的词语总数
- **唯一词数**：不重复的词语数量
- **已选择**：当前选中的词语数量（如果有）

### 表格列

| 列名 | 说明 |
|------|------|
| 排名 | 按频率排序的排名 |
| 词语 | 词语本身 |
| 频率 | 出现次数 |
| 百分比 | 占总词数的百分比 |
| 操作 | 快捷操作菜单（如果启用） |

### 排序功能

点击列标题可进行排序：

- **排名**：按频率排名排序
- **词语**：按字母顺序排序
- **频率**：按出现次数排序
- **百分比**：按百分比排序

支持升序/降序切换。

### 表格筛选

在表格顶部的搜索框中输入关键词，可实时筛选表格中的词语。

### 选择功能

- **单选**：点击表格行选择/取消选择词语
- **全选当前页**：点击表头的复选框选择当前页所有词语
- **全选所有**：点击工具栏的「全选」按钮选择所有词语

### 快捷操作

表格工具栏提供以下操作：

- **全选/取消全选**：快速选择或取消所有词语
- **复制选中**：将选中的词语列表复制到剪贴板（每行一个）
- **导出 CSV**：将结果导出为 CSV 文件
  - 如果选中了词语，只导出选中的词语
  - 否则导出所有结果

### 分页

- 支持 10、25、50、100 条/页
- 可在表格底部切换页码和每页显示数量

### 跨模块链接

如果启用了跨模块链接功能，表格中会显示「操作」列，提供：

- **共现分析**：跳转到共现分析模块，分析该词语的共现关系
- **词速写**：跳转到词速写模块，查看该词语的语法模式

## 可视化

可视化面板提供三种图表类型，帮助您直观地理解词频分布。

### 图表类型切换

通过顶部标签页切换图表类型：

- **柱状图**：适合展示前 N 个高频词
- **饼图**：适合展示词语占比分布
- **词云图**：适合展示整体词汇分布

### 柱状图

#### 配置选项

- **最大显示数量**：设置显示前多少个词语（默认：20，范围：5-50）
- **颜色方案**：选择柱状图的颜色主题（蓝色、绿色、紫色、橙色、红色）
- **显示百分比**：是否在柱状上显示百分比标签

#### 交互功能

- 点击柱状可跳转到结果表格，并选中该词语
- 图表高度会根据显示数量自动调整

### 饼图

#### 配置选项

- **最大显示数量**：设置显示前多少个词语（默认：10，范围：5-20）
- **颜色方案**：选择饼图的颜色主题
- **显示百分比**：是否在饼图上显示百分比标签

#### 图表特点

- 使用环形图（Donut Chart）设计
- 显示图例，便于识别
- 点击扇形可跳转到结果表格

### 词云图

#### 配置选项

- **最大词语数**：设置词云中显示的词语数量（默认：100，范围：5-500）
- **颜色映射**：选择词云的颜色方案
  - 支持多种预设方案：viridis、inferno、plasma、autumn、winter、rainbow、ocean、forest、sunset

#### 图表特点

- 词语大小根据频率自动调整
- 颜色根据频率映射到不同色阶
- 点击词语可跳转到结果表格

### 导出功能

所有图表都支持导出：

- **导出 SVG**：导出为矢量图格式，适合打印和编辑
- **导出 PNG**：导出为位图格式，适合插入文档

导出按钮位于图表设置栏右侧。

## 使用技巧

### 高效分析流程

1. **选择语料库**：根据研究目标选择合适语料库
2. **筛选文本**：使用标签或手动选择，聚焦目标文本
3. **设置词性**：根据研究问题选择相关词性
4. **配置搜索**：使用搜索和排除功能，精确定位目标词语
5. **查看结果**：在表格中查看详细数据
6. **可视化分析**：使用图表发现模式和趋势
7. **导出数据**：将结果导出用于进一步分析

### 常见分析场景

#### 分析高频实词
1. 选择「保留模式」词性筛选
2. 选择实词类别（NOUN、VERB、ADJ、ADV）
3. 设置最小频率（如 10）
4. 开启「转换为小写」
5. 选择「词根」模式

#### 分析特定词缀
1. 选择「结尾匹配」搜索类型
2. 输入词缀（如 "tion"）
3. 设置合适的频率范围

#### 分析词表词汇
1. 准备词语列表（每行一个）
2. 选择「词表匹配」搜索类型
3. 粘贴词语列表
4. 运行分析

### 注意事项

- 词频分析基于 SpaCy 标注数据，确保文本已完成 SpaCy 标注
- 大语料库分析可能需要较长时间，请耐心等待
- 使用词根模式时，同一词的不同变体会合并统计
- 正则表达式搜索需要一定的正则表达式知识
- 导出 CSV 时，如果数据量大，可能需要一些时间

# 同义词分析

## 概述

同义词分析模块基于 NLTK WordNet 词典，自动识别语料库中词语的同义词关系。该模块可以帮助您发现词语的语义关联，理解词汇的语义网络，并支持多种可视化展示方式。

## 分析原理

### 技术基础

同义词分析模块使用以下技术组件：

- **NLTK WordNet**：普林斯顿大学开发的英语词汇数据库
- **Open Multilingual Wordnet (omw-1.4)**：多语言 WordNet 扩展
- **SpaCy 词性标注**：用于确定词语的 WordNet 词性

### 同义词匹配流程

1. **词性映射**：将 SpaCy 的 Universal POS 标签映射到 WordNet 词性
   - NOUN → n（名词）
   - VERB → v（动词）
   - ADJ → a（形容词）
   - ADV → r（副词）

2. **同义词集查询**：根据词语和词性查询 WordNet 的 synsets（同义词集）

3. **词元提取**：从每个 synset 中提取所有 lemmas（词元）作为同义词

4. **语料库过滤**：**只保留在当前语料库中实际出现的同义词**
   - 这意味着分析结果中的同义词都是您的语料库中存在的词语
   - 可以用于发现语料库中的词汇替换模式和语义关联

### 重要说明

- **同义词来源**：同义词来自 WordNet 词典，而非语料库内部计算
- **结果过滤**：只显示在语料库中实际存在的同义词对
- **词性限制**：同义词匹配考虑词性，确保语义准确性
- **频率统计**：显示的频率是同义词在语料库中的出现次数

### 参考文献

- Miller, G. A. (1995). WordNet: A Lexical Database for English. *Communications of the ACM*, 38(11), 39-41.
- Fellbaum, C. (1998). *WordNet: An Electronic Lexical Database*. MIT Press.

## 界面布局

同义词分析模块采用左右分栏布局：

- **左侧面板**（400px）：配置面板，包含语料选择、词性筛选、搜索配置等
- **右侧面板**（弹性宽度）：结果展示区域，包含两个标签页
  - **结果表格**：显示同义词分析结果
  - **可视化**：提供网络图、树状图、列表视图三种可视化方式

## 语料选择

### 选择语料库

1. 在左侧面板顶部的下拉菜单中选择目标语料库
2. 系统会显示语料库的文本数量
3. 选择语料库后，系统会自动加载该语料库中的所有文本

### 文本选择模式

系统提供三种文本选择模式，与词频分析模块相同：

#### 全部文本
- 选择「全部文本」模式
- 分析将包含语料库中的所有文本

#### 按标签筛选
- 选择「按标签筛选」模式
- 从下拉菜单中选择一个或多个标签
- 系统会筛选出包含这些标签的所有文本

#### 手动选择
- 选择「手动选择」模式
- 使用搜索框按文件名搜索文本
- 在文本列表中勾选需要分析的文本
- 支持「全选」和「清空」快捷操作

## 词性筛选

词性筛选用于限制分析范围，只分析特定词性的词语。

### 词性选项

- **自动（Auto）**：系统根据 SpaCy 标注自动识别词性（推荐）
- **形容词（Adjective）**：只分析形容词
- **副词（Adverb）**：只分析副词
- **名词（Noun）**：只分析名词
- **动词（Verb）**：只分析动词
- **代词（Pronoun）**：只分析代词（注意：WordNet 对代词的支持有限）

### 使用建议

- **自动模式**：适合大多数场景，系统会根据词语在语料中的实际词性进行同义词查找
- **特定词性**：当您想专门分析某一类词语时使用，例如只分析动词的同义词关系
- **代词注意**：WordNet 对代词的同义词支持较少，选择代词模式可能返回较少结果

## 搜索配置

### 搜索查询

在「搜索查询」文本框中输入关键词，系统会只分析包含该关键词的词语。

- **支持模糊匹配**：输入 "happy" 会匹配 "happy", "unhappy", "happiness" 等
- **留空表示全部**：不输入任何内容则分析所有符合条件的词语

### 频率设置

- **最小频率**：只分析出现次数大于等于此值的词语（默认：1）
- **最大结果数**：限制返回的结果数量（默认：100，范围：10-500）

### 大小写处理

- **转换为小写**：勾选后，所有词语统一转换为小写进行统计
- 建议：分析英文时建议开启，以合并大小写变体

## 运行分析

配置完成后，点击「开始分析」按钮：

1. 系统会显示加载进度
2. 分析完成后，结果会自动显示在右侧面板
3. 如果出现错误，会在左侧面板显示错误信息

**注意**：同义词分析基于 WordNet 词典，主要支持英文。对于中文语料，系统会尝试查找对应的英文同义词，但结果可能有限。

## 结果表格

### 统计信息

表格顶部显示统计摘要：

- **总词数**：所有符合条件的词语总数
- **唯一词数**：不重复的词语数量
- **已选择**：当前选中的词语数量（如果有）

### 表格列

| 列名 | 说明 |
|------|------|
| 词语 | 语料库中的词语 |
| 频率 | 该词语在语料库中的出现次数 |
| 词性标签 | 该词语在语料库中的词性（可能有多个） |
| 同义词数量 | 找到的同义词总数 |
| 同义词 | 同义词列表（前5个，点击展开查看全部） |
| 操作 | 快捷操作菜单（如果启用） |

### 展开行详情

点击表格行左侧的展开按钮，可以查看详细的同义词集（Synset）信息：

#### 所有同义词汇总
- 显示该词语的所有同义词（去重后）
- 以标签形式展示，便于浏览

#### 同义词集详情
每个同义词集包含：
- **同义词集名称**：WordNet 中的同义词集标识
- **词性**：该同义词集的词性标签
- **定义**：该同义词集的语义定义
- **例句**：使用示例（如果有）
- **同义词列表**：该同义词集中的所有同义词

### 排序功能

点击列标题可进行排序：

- **词语**：按字母顺序排序
- **频率**：按出现次数排序
- **同义词数量**：按同义词数量排序

支持升序/降序切换。

### 表格筛选

在表格顶部的搜索框中输入关键词，可实时筛选表格中的词语或同义词。

### 选择功能

- **单选**：点击表格行的复选框选择/取消选择词语
- **全选**：点击表头的复选框或工具栏的「全选」按钮

### 快捷操作

表格工具栏提供以下操作：

- **全选/取消全选**：快速选择或取消所有词语
- **复制选中**：将选中的词语列表复制到剪贴板（每行一个）
- **导出 CSV**：将结果导出为 CSV 文件
  - 包含词语、频率、词性标签、同义词数量、同义词列表等信息

### 分页

- 支持 10、25、50、100 条/页
- 可在表格底部切换页码和每页显示数量

### 跨模块链接

如果启用了跨模块链接功能，表格中会显示「操作」列，提供：

- **共现分析**：跳转到共现分析模块，分析该词语的共现关系
- **词速写**：跳转到词速写模块，查看该词语的语法模式

## 可视化

可视化面板提供三种视图类型，帮助您直观地理解同义词关系。

### 视图类型切换

通过顶部标签页切换视图类型：

- **网络图**：以力导向图展示词语-同义词关系网络
- **树状图**：以树形结构展示词语的同义词层级关系
- **列表视图**：以列表形式展示所有词语及其同义词

### 网络图

网络图使用力导向布局，展示词语和同义词之间的关联关系。

#### 配置选项

- **最大节点数**：设置显示的词语数量（默认：50，范围：5-200）
- **颜色方案**：选择网络图的颜色主题
  - 默认（Default）
  - 分类（Category）
  - 柔和（Pastel）
  - 暖色（Warm）
  - 冷色（Cool）
- **显示定义**：是否在鼠标悬停时显示词语的语义定义

#### 交互功能

- **拖拽节点**：可以拖拽节点调整位置
- **缩放**：使用鼠标滚轮缩放视图
- **点击词语节点**：点击词语节点可跳转到结果表格并选中该词语
- **悬停提示**：鼠标悬停在节点上显示词语信息和定义（如果启用）

#### 节点类型

- **词语节点**（较大圆圈）：语料库中的词语，颜色较深
- **同义词节点**（较小圆圈）：WordNet 中的同义词，颜色较浅

### 树状图

树状图以层级结构展示词语的同义词关系，从根节点到词语，再到同义词集，最后到具体同义词。

#### 配置选项

- **最大节点数**：设置显示的词语数量（默认：5，范围：5-1000）
- **颜色方案**：选择树状图的颜色主题
- **显示定义**：是否在鼠标悬停时显示同义词集的语义定义

#### 交互功能

- **缩放**：使用鼠标滚轮缩放视图
- **点击词语节点**：点击词语节点可跳转到结果表格
- **悬停提示**：鼠标悬停在节点上显示定义（如果启用）

#### 层级结构

- **根节点**：所有同义词的根节点
- **词语节点**：语料库中的词语（显示频率）
- **同义词集节点**：WordNet 同义词集
- **同义词节点**：具体的同义词

### 列表视图

列表视图以卡片形式展示所有词语及其同义词，适合快速浏览。

#### 配置选项

- **最大显示数量**：设置显示的词语数量（默认：200，范围：5-1000）

#### 显示内容

每个卡片显示：
- **词语名称**：语料库中的词语
- **频率**：出现次数
- **同义词数量**：找到的同义词总数
- **同义词列表**：前15个同义词（以标签形式展示）
- **定义**：第一个同义词集的语义定义（如果有）

#### 交互功能

- **点击卡片**：点击卡片可跳转到结果表格并选中该词语

### 导出功能

所有视图都支持导出：

- **导出 SVG**：导出为矢量图格式，适合打印和编辑
- **导出 PNG**：导出为位图格式，适合插入文档

导出按钮位于视图设置栏右侧。

**注意**：
- 网络图和树状图导出时会包含所有可见内容
- 列表视图导出时会包含所有列表项（可能需要滚动查看）

## 使用技巧

### 高效分析流程

1. **选择语料库**：根据研究目标选择合适语料库
2. **筛选文本**：使用标签或手动选择，聚焦目标文本
3. **设置词性**：根据研究问题选择相关词性（或使用自动模式）
4. **配置搜索**：使用搜索查询和频率设置，精确定位目标词语
5. **查看结果**：在表格中查看详细数据，展开行查看同义词集详情
6. **可视化分析**：使用网络图或树状图发现语义关联模式
7. **导出数据**：将结果导出用于进一步分析

### 常见分析场景

#### 分析动词同义词
1. 选择「动词」词性筛选
2. 设置最小频率（如 5）
3. 运行分析
4. 在网络图中查看动词的同义词关系网络

#### 分析特定主题词汇
1. 在搜索查询中输入主题关键词（如 "happy"）
2. 使用自动词性模式
3. 运行分析
4. 在树状图中查看该主题的同义词层级结构

#### 构建词汇语义网络
1. 选择全部文本
2. 设置较大的最大结果数（如 200）
3. 运行分析
4. 在网络图中查看整体语义网络
5. 调整最大节点数以控制网络复杂度

### 注意事项

- 同义词分析基于 NLTK WordNet 词典，主要支持英文
- 中文语料的同义词查找结果可能有限
- WordNet 对某些词性（如代词）的支持较少
- 大语料库分析可能需要较长时间，请耐心等待
- 网络图和树状图在节点数量较多时可能显示较慢
- 同义词集可能包含多个语义，需要结合定义理解
- 某些词语可能没有同义词，这是正常现象

# 关键词提取

## 概述

关键词提取模块提供两种关键词提取方法：单文档算法和关键性对比。单文档算法从单个语料库中提取关键词，关键性对比通过对比研究语料库和参照语料库来识别具有统计显著性的关键词。

## 理论基础与参考文献

### 单文档关键词提取算法

#### TF-IDF（词频-逆文档频率）

**原理**：TF-IDF 通过计算词在单个文档中的频率（TF）和在整个语料库中的稀有程度（IDF）的乘积来衡量词的重要性。

$$\text{TF-IDF}(t, d, D) = \text{TF}(t, d) \times \text{IDF}(t, D)$$

其中 $\text{IDF}(t, D) = \log \frac{|D|}{|\{d \in D : t \in d\}|}$

**参考文献**：
- Salton, G., & McGill, M. J. (1983). *Introduction to Modern Information Retrieval*. McGraw-Hill.
- 实现基于 scikit-learn 的 TfidfVectorizer

#### TextRank

**原理**：TextRank 是一种基于图的排序算法，受 PageRank 启发。将文本中的词作为图的节点，词语之间的共现关系作为边，通过迭代计算节点的重要性分数。

$$WS(V_i) = (1-d) + d \times \sum_{V_j \in In(V_i)} \frac{w_{ji}}{\sum_{V_k \in Out(V_j)} w_{jk}} WS(V_j)$$

**参考文献**：
- Mihalcea, R., & Tarau, P. (2004). TextRank: Bringing Order into Texts. *Proceedings of EMNLP 2004*, 404-411.

#### YAKE!

**原理**：YAKE! 是一种无监督关键词提取方法，综合考虑词的多种统计特征：词频、位置、词语长度、大小写、与上下文词的关系等。

**参考文献**：
- Campos, R., Mangaravite, V., Pasquali, A., Jorge, A., Nunes, C., & Jatowt, A. (2020). YAKE! Keyword extraction from single documents using multiple local features. *Information Sciences*, 509, 257-289.

#### RAKE

**原理**：RAKE (Rapid Automatic Keyword Extraction) 通过识别停用词和标点来分割文本，将剩余的词语序列作为候选关键短语，然后根据词的度（degree）和频率（frequency）计算分数。

$$\text{Score}(w) = \frac{\text{deg}(w)}{\text{freq}(w)}$$

**参考文献**：
- Rose, S., Engel, D., Cramer, N., & Cowley, W. (2010). Automatic Keyword Extraction from Individual Documents. In *Text Mining: Applications and Theory* (pp. 1-20). Wiley.

### 关键性对比统计方法

#### Log-Likelihood (G²)

**原理**：对数似然比检验，用于比较两个语料库中词语分布的差异，是语料库语言学中最常用的关键性检验方法。

$$G^2 = 2 \sum_{i} O_i \ln\frac{O_i}{E_i}$$

**参考文献**：
- Dunning, T. (1993). Accurate Methods for the Statistics of Surprise and Coincidence. *Computational Linguistics*, 19(1), 61-74.

#### Chi-Square (χ²)

**原理**：卡方检验，比较观察频率和期望频率的差异。

$$\chi^2 = \sum \frac{(O - E)^2}{E}$$

**参考文献**：
- Rayson, P., & Garside, R. (2000). Comparing corpora using frequency profiling. *Proceedings of the Workshop on Comparing Corpora*, 1-6.

#### Fisher Exact Test

**原理**：精确检验，适用于小样本数据，基于超几何分布计算精确概率。

**参考文献**：
- Pedersen, T. (1996). Fishing for exactness. *Proceedings of the South-Central SAS Users Group Conference*.

#### 效应量指标

- **%DIFF**：百分比差异
- **Ratio**：频率比
- **Odds Ratio**：优势比
- **Dice Coefficient**：Dice 相似系数
- **Jaccard Index**：Jaccard 相似指数
- **MI (Mutual Information)**：互信息

**参考文献**：
- Gabrielatos, C. (2018). Keyness Analysis: nature, metrics and techniques. In *Corpus Approaches to Discourse* (pp. 225-258). Routledge.

## 界面布局

关键词提取模块采用标签页设计：

- **顶部标签页**：切换「单文档算法」和「关键性对比」两种模式
- **左侧面板**（400-420px）：配置面板，包含语料选择、词性筛选、算法/统计方法配置等
- **右侧面板**（弹性宽度）：结果展示区域，包含两个标签页
  - **结果表格**：显示关键词提取结果
  - **可视化**：提供柱状图、饼图、词云图三种可视化方式

## 单文档算法

单文档算法从选定的语料库中提取关键词，无需参照语料库。

### 语料选择

与词频分析模块相同，支持三种文本选择模式：
- **全部文本**：分析语料库中的所有文本
- **按标签筛选**：根据标签筛选文本
- **手动选择**：手动选择特定文本

### 词性筛选

基于 SpaCy Universal POS 标签集，支持保留/过滤模式，与词频分析模块相同。

### 排除词

与词频统计模块相同的设计：

- **移除停用词**：开启后，根据语料库语言自动移除常见停用词（使用 NLTK 停用词库）
  - 开启时会显示当前语料库的语言标签
  - 支持中文、英文等 20+ 种语言
- **排除词列表**：自定义需要排除的词语
  - 每行输入一个词
  - 这些词语将从分析结果中移除

**使用建议**：
- 分析内容词时建议开启停用词过滤
- 分析功能词或语法特征时建议关闭
- 可通过排除词列表移除特定领域的高频无意义词

### 算法选择

系统提供四种单文档关键词提取算法：

#### TF-IDF（词频-逆文档频率）

基于词在文档内和文档间的频率衡量词语重要性。

**参数配置**：
- **最大关键词数**：提取的关键词数量上限（默认：50，范围：5-500）
- **最小文档频率**：词语在文档中出现的最小比例（默认：0.01，范围：0.01-0.5）
- **最大文档频率**：词语在文档中出现的最大比例（默认：0.95，范围：0.5-1.0）
- **N-gram 范围**：关键词的词语组合范围（默认：1-2，即单个词和双词组合）

**适用场景**：
- 提取文档集合中的代表性关键词
- 识别文档特有的重要词汇
- 适合多文档语料库

#### TextRank（基于图的排序算法）

受 PageRank 启发的图排序算法，将词作为节点，共现关系作为边构建图。

**参数配置**：
- **Top N 关键词**：返回前 N 个关键词（默认：50，范围：5-500）
- **窗口大小**：共现窗口大小（默认：4，范围：2-10）
- **阻尼因子**：PageRank 算法的阻尼因子（默认：0.85，范围：0.5-0.99）
- **最大迭代次数**：算法迭代次数上限（默认：100，范围：10-500）

**适用场景**：
- 提取单个文档的关键词
- 识别语义相关的关键词
- 适合长文档

#### YAKE!（Yet Another Keyword Extractor）

基于多种统计特征的无监督方法，无需外部语料库。

**参数配置**：
- **Top N 关键词**：提取的关键词数量（默认：50，范围：5-500）
- **最大 N-gram 大小**：关键词的最大词语数（默认：3，范围：1-5）
- **去重阈值**：关键词去重的相似度阈值（默认：0.9，范围：0.1-1.0）
- **窗口大小**：特征提取的窗口大小（默认：2，范围：1-5）

**适用场景**：
- 快速提取关键词
- 无需训练数据
- 适合单文档或多文档

#### RAKE（快速自动关键词提取）

使用停用词和标点识别候选短语。

**参数配置**：
- **Top N 关键词**：提取的关键词数量（默认：50，范围：5-500）
- **最小长度**：关键词的最小词语数（默认：1，范围：1-5）
- **最大长度**：关键词的最大词语数（默认：3，范围：1-10）
- **最小频率**：词语的最小出现次数（默认：1，范围：1-10）

**适用场景**：
- 提取短语关键词
- 识别技术术语和专有名词
- 适合专业领域文档

### 大小写处理

- **转换为小写**：勾选后，所有词语统一转换为小写进行统计
- 建议：分析英文时建议开启，以合并大小写变体

### 运行分析

配置完成后，点击「提取关键词」按钮开始分析。

## 关键性对比

关键性对比通过对比研究语料库和参照语料库，识别在研究语料库中显著过用或欠用的词语。

### 语料库选择

需要选择两个语料库：

#### 研究语料库（目标语料库）

要分析的目标语料库，系统会识别该语料库中相对于参照语料库显著过用或欠用的词语。

- 支持三种文本选择模式（全部、按标签、手动选择）
- 显示已选择的文本数量

#### 参照语料库（对比基准）

作为对比基准的语料库，用于计算词语的期望频率。提供两种参照语料库选择方式：

**方式一：选择语料库文本**

- 支持三种文本选择模式（全部、按标签、手动选择）
- 显示已选择的文本数量
- 适合使用自己导入的语料库进行对比

**方式二：语料库资源**

系统内置多个预处理的语料库词频数据，可直接用于关键性对比：

- **启用方式**：打开「语料库资源」开关
- **默认资源**：OANC（Open American National Corpus）总体语料库
- **资源选择**：点击资源卡片打开选择窗口

**语料库资源选择窗口**：

- **搜索功能**：输入关键词搜索语料库名称
- **语料库类型筛选**：按语料库类型（BNC、Brown、NOW、OANC）筛选
- **标签筛选**：通过标签过滤，支持多选（如：新闻、小说、口语等）
- **资源卡片**：显示语料库名称、词汇数量、文件大小等信息
- **一次选择一个**：选择后关闭窗口并更新显示

**可用语料库资源**：

| 语料库 | 说明 | 标签 |
|--------|------|------|
| BNC | 英国国家语料库 | 口语、书面语、各领域 |
| Brown | Brown 语料库 | 新闻、小说、学术等 |
| NOW | News on the Web | 新闻（按国家分类） |
| OANC | 开放美国国家语料库 | 各类文本体裁 |

**使用建议**：
- 研究语料库应该是您关注的目标语料
- 参照语料库应该是具有代表性的基准语料（如通用语料库）
- 两个语料库的语言应该一致
- 使用语料库资源可以快速对比，无需自己导入参照语料

### 词性筛选

与单文档算法相同，支持保留/过滤模式。

### 排除词

与单文档算法相同，与词频统计模块设计一致：

- **移除停用词**：根据语料库语言自动移除常见停用词（NLTK）
- **排除词列表**：自定义需要排除的词语，每行一个

建议在进行关键性对比时启用停用词过滤，以便聚焦于内容词的差异。

### 统计方法选择

系统提供九种统计方法用于关键性分析：

#### Log-Likelihood (G²)（对数似然比）

语料库对比中最可靠的显著性检验，特别适合语料库大小差异较大的情况。

**特点**：
- 对语料库大小差异不敏感
- 提供统计显著性检验
- 推荐用于大多数场景

#### Chi-squared (χ²)（卡方检验）

带 Yates 校正的经典统计检验，检验观察频率与期望频率的差异。

**特点**：
- 经典的统计检验方法
- 适用于大样本
- 提供显著性检验

#### Log Ratio（对数比率）

纯效应量指标，显示词在研究语料库中频率是参照语料库的多少倍。

**特点**：
- 直观显示效应大小
- 正值表示过用，负值表示欠用
- 可设置效应量阈值

#### Dice Coefficient（Dice 系数）

衡量词与语料库关联强度的指标，取值范围 [0,1]。

**特点**：
- 取值范围明确
- 适合衡量关联强度
- 不考虑统计显著性

#### Mutual Information（互信息）

信息论指标，衡量词提供多少关于语料库成员的信息。

**特点**：
- 偏向低频词
- 适合发现罕见但重要的词
- 不考虑统计显著性

#### MI³（立方互信息）

MI 的立方形式，减少对低频词的偏好。

**特点**：
- 平衡高频和低频词
- 减少对罕见词的过度偏好
- 不考虑统计显著性

#### T-score（T 分数）

偏向高频词，适合识别典型搭配词。

**特点**：
- 偏向高频词
- 适合识别典型搭配
- 不考虑统计显著性

#### Simple Keyness（简单关键性）

语料库间简单的标准化频率比值。

**特点**：
- 计算简单直观
- 不考虑统计显著性
- 适合快速筛选

#### Fisher's Exact（Fisher 精确检验）

适用于小样本的精确检验，计算精确 p 值。

**特点**：
- 适用于小样本
- 计算精确 p 值
- 计算时间较长

### 频率阈值配置

- **研究语料库最小频率**：词语在研究语料库中的最小出现次数（默认：3）
- **参照语料库最小频率**：词语在参照语料库中的最小出现次数（默认：3）

### 效应量阈值（仅 Log Ratio）

当选择 Log Ratio 统计方法时，可以设置效应量阈值：
- **效应量阈值**：最小效应量（默认：0，范围：0-5）
- 只显示效应量大于此值的关键词

### 统计阈值

启用统计阈值可以根据学术统计标准过滤结果：

- **启用方式**：打开「统计阈值」开关
- **最小分数**：设置结果必须达到的最小统计得分
- **最大 p 值**：设置结果必须低于的最大显著性 p 值

**学术标准参考**：

| 统计方法 | 建议最小分数 | 说明 |
|----------|--------------|------|
| Log-Likelihood (G) | 6.63 | p < 0.01 |
| Log-Likelihood (G) | 3.84 | p < 0.05 |
| Chi-squared | 6.63 | p < 0.01 |
| Chi-squared | 3.84 | p < 0.05 |
| Log Ratio | 1.0 | 表示有意义的效应量 |

**p 值标准**：
- 标准显著性水平：p < 0.05
- 严格显著性水平：p < 0.01
- 高度显著：p < 0.001

**使用建议**：
- 学术研究建议使用 p < 0.01 或 LL > 6.63
- 探索性分析可使用较宽松的阈值
- Log Ratio 建议关注 |LR| > 1 的结果

### 显示负关键词

- **显示负关键词**：勾选后，会显示在研究语料库中显著欠用的词语（负关键词）
- 负关键词表示该词在研究语料库中的使用频率显著低于参照语料库

### 大小写处理

与单文档算法相同，支持转换为小写选项。

### 运行分析

配置完成后，点击「分析关键性」按钮开始分析。

## 结果表格

### 统计信息

表格顶部显示统计摘要：

**单文档算法**：
- **总关键词数**：提取的关键词总数

**关键性对比**：
- **总关键词数**：识别出的关键词总数
- **研究语料库大小**：研究语料库的总词数
- **参照语料库大小**：参照语料库的总词数

### 表格列

**单文档算法结果**：

| 列名 | 说明 |
|------|------|
| 排名 | 按得分排序的排名 |
| 关键词 | 提取的关键词 |
| 得分 | 算法计算的关键词得分 |
| 频率 | 关键词在语料库中的出现次数 |
| 操作 | 快捷操作菜单（如果启用） |

**关键性对比结果**：

| 列名 | 说明 |
|------|------|
| 排名 | 按得分排序的排名 |
| 关键词 | 关键词 |
| 研究频率 | 在研究语料库中的出现次数 |
| 参照频率 | 在参照语料库中的出现次数 |
| 研究标准化 | 研究语料库中的标准化频率（每百万词） |
| 参照标准化 | 参照语料库中的标准化频率（每百万词） |
| 得分 | 统计方法计算的得分 |
| 效应量 | 效应量（Log Ratio） |
| P 值 | 统计显著性 p 值（如果适用） |
| 显著性 | 显著性水平标记（***, **, *, 空） |
| 方向 | 过用（positive）或欠用（negative） |
| 操作 | 快捷操作菜单（如果启用） |

### 排序功能

点击列标题可进行排序：
- **单文档算法**：按排名、关键词、得分、频率排序
- **关键性对比**：按排名、关键词、研究频率、参照频率、得分、效应量、P 值等排序

支持升序/降序切换。

### 表格筛选

在表格顶部的搜索框中输入关键词，可实时筛选表格中的关键词。

### 选择功能

- **单选**：点击表格行的复选框选择/取消选择关键词
- **全选**：点击表头的复选框或工具栏的「全选」按钮

### 快捷操作

表格工具栏提供以下操作：

- **全选/取消全选**：快速选择或取消所有关键词
- **复制选中**：将选中的关键词列表复制到剪贴板（每行一个）
- **导出 CSV**：将结果导出为 CSV 文件

### 分页

- 支持 10、25、50、100 条/页
- 可在表格底部切换页码和每页显示数量

### 跨模块链接

如果启用了跨模块链接功能，表格中会显示「操作」列，提供：

- **共现分析**：跳转到共现分析模块，分析该关键词的共现关系
- **词速写**：跳转到词速写模块，查看该关键词的语法模式

## 可视化

可视化面板提供三种图表类型，帮助您直观地理解关键词分布。

### 图表类型切换

通过顶部标签页切换图表类型：

- **柱状图**：适合展示前 N 个关键词
- **饼图**：适合展示关键词占比分布
- **词云图**：适合展示整体关键词分布

### 柱状图

#### 配置选项

- **最大显示数量**：设置显示前多少个关键词（默认：20，范围：5-50）
- **颜色方案**：选择柱状图的颜色主题（蓝色、绿色、紫色、橙色、红色）
- **显示百分比**：是否在柱状上显示百分比标签

#### 交互功能

- 点击柱状可跳转到结果表格，并选中该关键词
- 图表高度会根据显示数量自动调整

**关键性对比特殊显示**：
- 正关键词（过用）和负关键词（欠用）使用不同颜色区分
- 可以分别查看正关键词和负关键词的分布

### 饼图

#### 配置选项

- **最大显示数量**：设置显示前多少个关键词（默认：10，范围：5-20）
- **颜色方案**：选择饼图的颜色主题
- **显示百分比**：是否在饼图上显示百分比标签

#### 图表特点

- 使用环形图（Donut Chart）设计
- 显示图例，便于识别
- 点击扇形可跳转到结果表格

### 词云图

#### 配置选项

- **最大词语数**：设置词云中显示的词语数量（默认：100，范围：5-500）
- **颜色映射**：选择词云的颜色方案
  - 支持多种预设方案：viridis、inferno、plasma、autumn、winter、rainbow、ocean、forest、sunset

#### 图表特点

- 词语大小根据得分或频率自动调整
- 颜色根据得分映射到不同色阶
- 点击词语可跳转到结果表格

**关键性对比特殊显示**：
- 正关键词和负关键词可以使用不同颜色方案区分

### 导出功能

所有图表都支持导出：

- **导出 SVG**：导出为矢量图格式，适合打印和编辑
- **导出 PNG**：导出为位图格式，适合插入文档

导出按钮位于图表设置栏右侧。

## 使用技巧

### 单文档算法选择建议

#### 选择 TF-IDF 当：
- 您有多个文档，想找出每个文档的代表性关键词
- 需要识别文档特有的重要词汇
- 语料库包含多个主题的文档

#### 选择 TextRank 当：
- 您有单个长文档，想提取关键词
- 需要识别语义相关的关键词
- 关注词语之间的共现关系

#### 选择 YAKE! 当：
- 需要快速提取关键词
- 没有外部训练数据
- 适合单文档或多文档

#### 选择 RAKE 当：
- 需要提取短语关键词
- 识别技术术语和专有名词
- 处理专业领域文档

### 关键性对比使用建议

#### 统计方法选择

- **一般推荐**：Log-Likelihood（对数似然比），适合大多数场景
- **需要效应量**：Log Ratio（对数比率），直观显示效应大小
- **关注显著性**：Chi-squared（卡方检验）或 Fisher's Exact（Fisher 精确检验）
- **关注关联强度**：Dice Coefficient（Dice 系数）
- **发现罕见词**：Mutual Information（互信息）
- **平衡高频低频**：MI³（立方互信息）

#### 语料库选择建议

- **研究语料库**：选择您关注的目标语料（如特定主题、特定作者、特定时期）
- **参照语料库**：选择具有代表性的基准语料（如通用语料库、平衡语料库）
- **语料库大小**：两个语料库的大小差异不影响 Log-Likelihood 的结果
- **语言一致性**：确保两个语料库使用相同的语言

#### 频率阈值设置

- **最小频率**：设置合适的频率阈值可以过滤掉偶然出现的词语
- **研究语料库最小频率**：建议设置为 3-5
- **参照语料库最小频率**：可以设置为 0（如果参照语料库很大）或 1-3

#### 负关键词分析

- 启用「显示负关键词」可以发现研究语料库中显著欠用的词语
- 负关键词分析有助于理解语料库的特征和差异
- 例如：学术语料库中可能欠用口语词汇

### 常见分析场景

#### 提取学术论文关键词
1. 选择「单文档算法」标签页
2. 选择算法：TF-IDF 或 TextRank
3. 选择学术论文语料库
4. 设置词性筛选：保留 NOUN、ADJ、VERB
5. 运行分析

#### 对比不同时期语料库
1. 选择「关键性对比」标签页
2. 研究语料库：选择近期语料库
3. 参照语料库：选择早期语料库
4. 统计方法：Log-Likelihood
5. 运行分析，查看新出现的词汇

#### 识别专业领域术语
1. 选择「单文档算法」标签页
2. 选择算法：RAKE
3. 设置 N-gram 范围：2-3（提取短语）
4. 选择专业领域语料库
5. 运行分析

#### 分析语域特征
1. 选择「关键性对比」标签页
2. 研究语料库：选择特定语域语料（如新闻、小说）
3. 参照语料库：选择通用语料库
4. 统计方法：Log Ratio
5. 启用「显示负关键词」
6. 运行分析，查看语域特征词汇

### 注意事项

- 关键词提取基于 SpaCy 标注数据，确保文本已完成 SpaCy 标注
- 大语料库分析可能需要较长时间，请耐心等待
- 不同算法可能产生不同的结果，建议根据具体需求选择合适的算法
- 关键性对比需要两个语料库，确保两个语料库都已准备好
- 统计方法的显著性水平标记：*** (p<0.001), ** (p<0.01), * (p<0.05)
- 负关键词表示在研究语料库中显著欠用的词语，有助于理解语料库特征
- 导出 CSV 时，如果数据量大，可能需要一些时间

# N-gram 分析

## 概述

N-gram 分析模块基于 SpaCy 标注数据，统计语料库中连续 N 个词语的组合频率。N-gram 分析可以帮助您发现常见的词语搭配、短语模式、语言习惯等，是语料库语言学的重要研究工具。

## 理论基础

### 什么是 N-gram？

**N-gram** 是指文本中连续出现的 N 个词语（或字符）序列。在语料库语言学中，N-gram 分析是研究词语搭配和语言模式的基础工具。

| N 值 | 名称 | 示例 |
|------|------|------|
| 2 | Bigram（二元组） | "natural language", "语言研究" |
| 3 | Trigram（三元组） | "natural language processing", "语言研究方法" |
| 4 | 4-gram（四元组） | "in the United States" |
| 5 | 5-gram（五元组） | "at the end of the" |
| 6 | 6-gram（六元组） | "at the beginning of the year" |

### 语言学应用

N-gram 分析在语料库语言学中有多种应用：

1. **搭配分析（Collocation Analysis）**：识别词语之间的习惯性搭配
2. **短语模式发现**：发现固定短语和表达式
3. **语言风格分析**：比较不同语体或作者的语言习惯
4. **语法模式研究**：研究词性序列和句法模式
5. **语言习得研究**：分析学习者语言中的错误模式

### 频率与分布

N-gram 的频率分布通常遵循 **Zipf 定律**：少数 N-gram 具有很高的频率，而大多数 N-gram 只出现一次或几次。

$$f(r) \approx \frac{C}{r^a}$$

其中 $f(r)$ 是排名为 $r$ 的 N-gram 的频率，$C$ 和 $a$ 是常数。

### Nest N-gram（嵌套分组）

Meta-Lingo 支持 **Nest N-gram** 功能，将包含关系的 N-gram 按层级分组显示：

- 例如：如果存在 "natural language" (2-gram) 和 "natural language processing" (3-gram)
- Nest 模式会将它们分组显示，便于发现短语的扩展模式

### 参考文献

- Jurafsky, D., & Martin, J. H. (2023). *Speech and Language Processing* (3rd ed.). [https://web.stanford.edu/~jurafsky/slp3/](https://web.stanford.edu/~jurafsky/slp3/)
- Manning, C. D., & Schutze, H. (1999). *Foundations of Statistical Natural Language Processing*. MIT Press.
- Sinclair, J. (1991). *Corpus, Concordance, Collocation*. Oxford University Press.
- Biber, D., Conrad, S., & Reppen, R. (1998). *Corpus Linguistics: Investigating Language Structure and Use*. Cambridge University Press.

## 界面布局

N-gram 分析模块采用左右分栏布局：

- **左侧面板**（400px）：配置面板，包含语料选择、N 值选择、词性筛选、搜索配置等
- **右侧面板**（弹性宽度）：结果展示区域，包含两个标签页
  - **结果表格**：显示 N-gram 统计结果
  - **可视化**：提供柱状图、网络图、桑基图、词云图四种可视化方式

## 语料选择

### 选择语料库

1. 在左侧面板顶部的下拉菜单中选择目标语料库
2. 系统会显示语料库的文本数量
3. 选择语料库后，系统会自动加载该语料库中的所有文本

### 文本选择模式

系统提供三种文本选择模式，与词频分析模块相同：

#### 全部文本
- 选择「全部文本」模式
- 分析将包含语料库中的所有文本
- 显示文本总数

#### 按标签筛选
- 选择「按标签筛选」模式
- 从下拉菜单中选择一个或多个标签
- 系统会筛选出包含这些标签的所有文本
- 显示筛选后的文本数量

#### 手动选择
- 选择「手动选择」模式
- 使用搜索框按文件名搜索文本
- 在文本列表中勾选需要分析的文本
- 支持「全选」和「清空」快捷操作
- 显示已选择的文本数量

### 选择状态提示

选择完成后，系统会显示：
- 已选择的文本数量
- 成功/警告提示（根据选择数量）

## N 值选择

N-gram 中的 N 表示连续词语的数量。系统支持 2-6 gram 的分析。

### 可选的 N 值

- **Bigram (2-gram)**：双词组合，如 "the cat", "in the"
- **Trigram (3-gram)**：三词组合，如 "the cat sat", "in the house"
- **4-gram**：四词组合
- **5-gram**：五词组合
- **6-gram**：六词组合

### 多 N 值选择

- 可以同时选择多个 N 值进行分析
- 系统会为每个 N 值分别统计 N-gram
- 结果会合并显示，每个 N-gram 会标注其 N 值

**使用建议**：
- **Bigram (2-gram)**：最常用，适合分析常见词语搭配
- **Trigram (3-gram)**：适合分析短语模式
- **4-6 gram**：适合分析长短语和固定表达

### Nest N-gram 分组

启用「Nest N-gram 分组」功能后，系统会将较短的 N-gram 分组到包含它的较长 N-gram 下。

**功能说明**：
- 例如：如果 "the cat" 是 bigram，"the cat sat" 是 trigram，启用分组后，"the cat" 会显示在 "the cat sat" 的展开行中
- 只有当选择了多个 N 值时才可用
- 有助于理解 N-gram 的层级关系

**使用场景**：
- 分析固定短语的组成部分
- 理解词语搭配的层级结构
- 发现短语的扩展模式

## 词性筛选

词性筛选基于 SpaCy Universal POS 标签集，允许您只分析特定词性的词语。

### 筛选模式

#### 保留模式
- 选择「保留模式」
- 只统计包含您选择的词性标签的 N-gram
- 例如：选择 NOUN（名词）和 VERB（动词），则只统计包含名词和动词的 N-gram

#### 过滤模式
- 选择「过滤模式」
- 排除包含您选择的词性标签的 N-gram
- 例如：选择 PUNCT（标点），则排除包含标点的 N-gram

**注意**：N-gram 的词性筛选要求 N-gram 中的所有词语都满足筛选条件。

### 词性标签分类

系统将词性标签分为三类，便于选择：

#### 实词（Content Words）
- **NOUN**：名词
- **VERB**：动词
- **ADJ**：形容词
- **ADV**：副词
- **PROPN**：专有名词

#### 虚词（Function Words）
- **ADP**：介词
- **AUX**：助动词
- **CCONJ**：并列连词
- **DET**：限定词
- **PART**：助词
- **PRON**：代词
- **SCONJ**：从属连词

#### 其他（Other）
- **INTJ**：感叹词
- **NUM**：数词
- **PUNCT**：标点
- **SYM**：符号
- **X**：其他

### 快捷操作

- **全选**：快速选择所有词性标签
- **清空**：清除所有选择

## 搜索配置

搜索配置面板提供多种过滤和搜索选项，帮助您精确定位目标 N-gram。

### 频率范围

设置 N-gram 的频率范围：

- **最小频率**：只显示出现次数大于等于此值的 N-gram（默认：2）
- **最大频率**：只显示出现次数小于等于此值的 N-gram（可选，设为 0 表示无限制）

### 最小词长

- **最小词长**：设置 N-gram 中每个词语的最小字符长度（默认：1）
- 例如：设置为 2 时，会排除单字符词语（如 "a", "I"）

### 大小写处理

- **转换为小写**：勾选后，所有词语统一转换为小写进行统计
- 建议：分析英文时建议开启，以合并大小写变体

### 搜索类型

#### 全部
- 不进行搜索过滤，统计所有符合条件的 N-gram

#### 开头匹配（Starts）
- 只统计以指定字符串开头的 N-gram
- 例如：输入 "the" 可统计 "the cat", "the house" 等

#### 结尾匹配（Ends）
- 只统计以指定字符串结尾的 N-gram
- 例如：输入 "cat" 可统计 "the cat", "a cat" 等

#### 包含匹配（Contains）
- 只统计包含指定字符串的 N-gram
- 例如：输入 "cat" 可统计 "the cat", "cat sat", "the cat sat" 等

#### 包含词语（Contains Word）
- 只统计包含指定词语的 N-gram（完整词语匹配）
- 例如：输入 "cat" 可统计 "the cat", "cat sat" 等，但不会匹配 "category" 中的 "cat"

#### 正则表达式（Regex）
- 使用正则表达式进行高级匹配
- 支持完整的正则表达式语法
- 例如：`^the\s+\w+` 匹配以 "the" 开头后跟一个词语的 N-gram

#### 词表匹配（Wordlist）
- 输入一个词语列表（每行一个）
- 只统计包含列表中词语的 N-gram
- 适合分析特定词汇集合的搭配

### 排除词语

在「排除词语」文本框中输入要排除的词语（每行一个），包含这些词语的 N-gram 将不会出现在统计结果中。

**使用场景**：
- 排除停用词
- 排除特定干扰词
- 排除人名、地名等专有名词

## 运行分析

配置完成后，点击「开始分析」按钮：

1. 系统会显示加载进度
2. 分析完成后，结果会自动显示在右侧面板
3. 如果出现错误，会在左侧面板显示错误信息

**注意**：
- 大语料库或多 N 值分析可能需要较长时间
- Nest N-gram 分组会增加处理时间
- 建议先使用较小的 N 值或较少的文本进行测试

## 结果表格

### 统计信息

表格顶部显示统计摘要：

- **总 N-gram 数**：所有符合条件的 N-gram 总数
- **唯一 N-gram 数**：不重复的 N-gram 数量
- **已选择**：当前选中的 N-gram 数量（如果有）

### 表格列

| 列名 | 说明 |
|------|------|
| 排名 | 按频率排序的排名 |
| N-gram | N-gram 本身 |
| N 值标签 | 显示该 N-gram 的 N 值（如 "2-gram", "3-gram"） |
| 频率 | 出现次数 |
| 百分比 | 占总 N-gram 数的百分比 |
| 操作 | 快捷操作菜单（如果启用） |

### Nest N-gram 展开

如果启用了 Nest N-gram 分组：

- 表格左侧会显示展开按钮（▼/▲）
- 点击展开按钮可以查看该 N-gram 包含的较短 N-gram
- 展开行中显示子 N-gram 及其频率
- 有助于理解 N-gram 的层级关系

### 排序功能

点击列标题可进行排序：

- **排名**：按频率排名排序
- **N-gram**：按字母顺序排序
- **频率**：按出现次数排序
- **百分比**：按百分比排序
- **N 值**：按 N 值排序

支持升序/降序切换。

### 表格筛选

在表格顶部的搜索框中输入关键词，可实时筛选表格中的 N-gram。

### 选择功能

- **单选**：点击表格行的复选框选择/取消选择 N-gram
- **全选**：点击表头的复选框或工具栏的「全选」按钮

### 快捷操作

表格工具栏提供以下操作：

- **全选/取消全选**：快速选择或取消所有 N-gram
- **复制选中**：将选中的 N-gram 列表复制到剪贴板（格式：N-gram\t频率\t百分比）
- **导出 CSV**：将结果导出为 CSV 文件
  - 包含 N-gram、N 值、频率、百分比等信息

### 分页

- 支持 10、25、50、100 条/页
- 可在表格底部切换页码和每页显示数量

### 跨模块链接

如果启用了跨模块链接功能，表格中会显示「操作」列，提供：

- **共现分析**：跳转到共现分析模块，分析该 N-gram 的共现关系

## 可视化

可视化面板提供四种图表类型，帮助您直观地理解 N-gram 分布和关系。

### 图表类型切换

通过顶部标签页切换图表类型：

- **柱状图**：适合展示前 N 个高频 N-gram
- **网络图**：适合展示 N-gram 之间的关联关系
- **桑基图**：适合展示 N-gram 的流动和转换
- **词云图**：适合展示整体 N-gram 分布

### 柱状图

#### 配置选项

- **最大显示数量**：设置显示前多少个 N-gram（默认：20，范围：5-50）
- **颜色方案**：选择柱状图的颜色主题（蓝色、绿色、紫色、橙色、红色、青色）
- **显示百分比**：是否在柱状上显示百分比标签

#### 交互功能

- 点击柱状可跳转到结果表格，并选中该 N-gram
- 图表高度会根据显示数量自动调整

### 网络图

网络图使用力导向布局，展示 N-gram 之间的关联关系。

#### 配置选项

- **最大节点数**：设置显示的 N-gram 数量（默认：50，范围：5-300）
- **颜色方案**：选择网络图的颜色主题

#### 交互功能

- **拖拽节点**：可以拖拽节点调整位置
- **缩放**：使用鼠标滚轮缩放视图
- **点击节点**：点击节点可跳转到结果表格并选中该 N-gram

#### 节点关系

- 节点之间的连线表示 N-gram 之间的关联
- 节点大小根据频率自动调整
- 不同 N 值可能使用不同颜色区分

### 桑基图

桑基图展示 N-gram 的流动和转换模式，特别适合展示多 N 值分析的结果。

#### 配置选项

- **最大节点数**：设置显示的 N-gram 数量（默认：50，范围：5-100）
- **颜色方案**：选择桑基图的颜色主题

#### 交互功能

- **缩放**：使用鼠标滚轮缩放视图
- **拖拽**：可以拖拽图表调整位置
- **点击节点**：点击节点可跳转到结果表格

#### 图表特点

- 左侧节点表示 N-gram 的起始部分
- 右侧节点表示 N-gram 的结束部分
- 流线的粗细表示频率大小
- 适合分析 N-gram 的转换模式

### 词云图

#### 配置选项

- **最大词语数**：设置词云中显示的 N-gram 数量（默认：100，范围：5-500）
- **颜色映射**：选择词云的颜色方案
  - 支持多种预设方案：viridis、inferno、plasma、autumn、winter、rainbow、ocean、forest、sunset

#### 图表特点

- N-gram 大小根据频率自动调整
- 颜色根据频率映射到不同色阶
- 点击 N-gram 可跳转到结果表格

### 导出功能

所有图表都支持导出：

- **导出 SVG**：导出为矢量图格式，适合打印和编辑
- **导出 PNG**：导出为位图格式，适合插入文档

导出按钮位于图表设置栏右侧。

## 使用技巧

### 高效分析流程

1. **选择语料库**：根据研究目标选择合适语料库
2. **筛选文本**：使用标签或手动选择，聚焦目标文本
3. **选择 N 值**：根据研究问题选择合适的 N 值（建议从 2-gram 开始）
4. **设置词性**：根据研究问题选择相关词性
5. **配置搜索**：使用搜索和排除功能，精确定位目标 N-gram
6. **查看结果**：在表格中查看详细数据
7. **可视化分析**：使用图表发现模式和趋势
8. **导出数据**：将结果导出用于进一步分析

### 常见分析场景

#### 分析常见词语搭配
1. 选择 Bigram (2-gram)
2. 设置词性筛选：保留实词（NOUN、VERB、ADJ、ADV）
3. 设置最小频率（如 10）
4. 开启「转换为小写」
5. 运行分析

#### 分析固定短语
1. 选择 Trigram 或 4-gram
2. 设置最小频率（如 5）
3. 启用 Nest N-gram 分组
4. 运行分析
5. 展开长 N-gram 查看其组成部分

#### 分析特定词汇的搭配
1. 选择「包含词语」搜索类型
2. 输入目标词语（如 "cat"）
3. 选择多个 N 值（2-4）
4. 运行分析
5. 在网络图中查看该词语的搭配网络

#### 分析语域特征短语
1. 选择多个 N 值（2-4）
2. 设置词性筛选：保留实词
3. 设置合适的频率范围
4. 运行分析
5. 在桑基图中查看短语转换模式

### N 值选择建议

- **Bigram (2-gram)**：
  - 最常用，适合分析常见词语搭配
  - 结果数量适中，易于分析
  - 适合作为起点

- **Trigram (3-gram)**：
  - 适合分析短语模式
  - 可以发现固定表达
  - 结果数量较多

- **4-6 gram**：
  - 适合分析长短语和固定表达
  - 结果数量可能非常大
  - 建议设置较高的最小频率阈值

### Nest N-gram 使用建议

- **何时启用**：
  - 选择了多个 N 值
  - 想理解 N-gram 的层级关系
  - 分析固定短语的组成部分

- **注意事项**：
  - 启用后会增加处理时间
  - 表格中会显示更多信息
  - 适合深入分析特定 N-gram

### 注意事项

- N-gram 分析基于 SpaCy 标注数据，确保文本已完成 SpaCy 标注
- 大语料库或多 N 值分析可能需要较长时间，请耐心等待
- N 值越大，可能的 N-gram 组合数量呈指数增长
- 建议设置合适的最小频率阈值，过滤掉偶然出现的组合
- 词性筛选要求 N-gram 中的所有词语都满足筛选条件
- Nest N-gram 分组功能需要选择多个 N 值
- 网络图和桑基图在节点数量较多时可能显示较慢
- 导出 CSV 时，如果数据量大，可能需要一些时间

# 共现关系

## 概述

共现关系分析模块提供 KWIC（Key Word In Context，关键词在语境中）搜索功能，帮助您查找和分析语料库中特定词语或模式的出现情况。该模块支持多种搜索模式、CQL（Corpus Query Language，语料库查询语言）查询、词性筛选、结果排序和可视化等功能。

## CQL 语法详细指南

Meta-Lingo 实现了自建的 CQL（Corpus Query Language，语料库查询语言）引擎，参考 Sketch Engine 标准。CQL 是语料库语言学中用于高级查询的标准语言。

### 基础语法

#### Token 匹配

每个 token（词语）用方括号 `[]` 表示，内部指定匹配条件：

| 属性 | 说明 | 示例 |
|------|------|------|
| `word` | 词形（原文形式） | `[word="running"]` |
| `lemma` | 词元（词典原形） | `[lemma="run"]` |
| `pos` | 词性（Universal POS） | `[pos="NOUN"]` |
| `tag` | 细粒度词性（Penn Treebank） | `[tag="NNS"]` |
| `dep` | 依存关系 | `[dep="nsubj"]` |

#### 任意 Token

- `[]`：匹配任意一个 token
- `[]{n}`：匹配 n 个任意 token
- `[]{m,n}`：匹配 m 到 n 个任意 token

### 逻辑运算符

| 运算符 | 说明 | 示例 |
|--------|------|------|
| `&` | AND（与） | `[pos="NOUN" & lemma="test"]` |
| `\|` | OR（或） | `[pos="NOUN" \| pos="VERB"]` |
| `!` | NOT（非） | `[!pos="PUNCT"]` |

### 正则表达式

CQL 支持在属性值中使用正则表达式：

| 模式 | 说明 | 示例 |
|------|------|------|
| `.*` | 匹配任意字符序列 | `[word=".*ing"]` 匹配以 -ing 结尾的词 |
| `^` | 匹配开头 | `[word="^pre.*"]` 匹配以 pre- 开头的词 |
| `$` | 匹配结尾 | `[lemma=".*tion$"]` 匹配以 -tion 结尾的词元 |
| `[A-Z]` | 字符范围 | `[word="^[A-Z].*"]` 匹配大写开头的词 |
| `?` | 可选字符 | `[word="colou?r"]` 匹配 color 或 colour |

### 序列查询示例

#### 基本序列

```cql
[lemma="make"] [pos="DET"] [pos="NOUN"]
```

匹配：make + 冠词 + 名词（如 "make a decision"）

#### 带间隔的序列

```cql
[lemma="look"] []{0,2} [word="at"]
```

匹配：look + 0-2 个任意词 + at（如 "look at", "look carefully at"）

#### 复杂组合

```cql
[pos="VERB" & !lemma="be"] [] [pos="ADJ"] [pos="NOUN"]
```

匹配：非 be 动词 + 任意词 + 形容词 + 名词

### 词性标签参考（Universal POS）

| 标签 | 说明 | 示例 |
|------|------|------|
| ADJ | 形容词 | big, old, green |
| ADP | 介词 | in, to, during |
| ADV | 副词 | very, well, exactly |
| AUX | 助动词 | is, has, will |
| CCONJ | 并列连词 | and, or, but |
| DET | 限定词 | a, the, this |
| INTJ | 感叹词 | oh, wow, yes |
| NOUN | 名词 | girl, cat, tree |
| NUM | 数词 | 1, 2019, one |
| PART | 助词 | 's, not, to |
| PRON | 代词 | I, you, he |
| PROPN | 专有名词 | Mary, London, NASA |
| PUNCT | 标点 | ., !, ? |
| SCONJ | 从属连词 | if, while, that |
| SYM | 符号 | $, %, + |
| VERB | 动词 | run, eat, write |
| X | 其他 | - |

### 常用查询模板

#### 搭配模式

```cql
// 形容词 + 名词
[pos="ADJ"] [pos="NOUN"]

// 动词 + 名词宾语
[pos="VERB"] [pos="DET"]? [pos="NOUN"]

// 介词短语
[pos="ADP"] [pos="DET"]? [pos="ADJ"]* [pos="NOUN"]
```

#### 语法结构

```cql
// 被动语态
[lemma="be"] [pos="VERB" & word=".*ed"]

// 进行时
[lemma="be"] [word=".*ing"]

// 不定式
[word="to"] [pos="VERB"]
```

#### 词汇模式

```cql
// 以 -ly 结尾的副词
[pos="ADV" & word=".*ly"]

// 以 -tion 结尾的名词
[pos="NOUN" & word=".*tion"]

// 复合词模式
[pos="NOUN"] [pos="NOUN"]
```

### CQL 构建器

如果您不熟悉 CQL 语法，可以使用 **CQL 构建器**：

1. 点击搜索面板中的「CQL 构建器」按钮
2. 使用可视化界面添加 token 条件
3. 选择属性（word/lemma/pos 等）和值
4. 添加逻辑运算符
5. 实时预览生成的 CQL 查询
6. 可保存常用模板供后续使用

### 参考资料

- Sketch Engine CQL 文档：[https://www.sketchengine.eu/documentation/corpus-querying/](https://www.sketchengine.eu/documentation/corpus-querying/)
- CQP Query Language Tutorial：[https://cwb.sourceforge.io/files/CQP_Tutorial/](https://cwb.sourceforge.io/files/CQP_Tutorial/)

## 界面布局

共现关系分析模块采用左右分栏布局：

- **左侧面板**（400px）：配置面板，包含语料选择、词性筛选、搜索配置等
- **右侧面板**（弹性宽度）：结果展示区域，包含两个标签页
  - **结果表格**：显示 KWIC 搜索结果
  - **可视化**：提供密度分布图和分组山脊图两种可视化方式

## 语料选择

### 选择语料库

1. 在左侧面板顶部的下拉菜单中选择目标语料库
2. 系统会显示语料库的文本数量
3. 选择语料库后，系统会自动加载该语料库中的所有文本

### 文本选择模式

系统提供三种文本选择模式，与其他分析模块相同：

#### 全部文本
- 选择「全部文本」模式
- 搜索将包含语料库中的所有文本
- 显示文本总数

#### 按标签筛选
- 选择「按标签筛选」模式
- 从下拉菜单中选择一个或多个标签
- 系统会筛选出包含这些标签的所有文本
- 显示筛选后的文本数量

#### 手动选择
- 选择「手动选择」模式
- 使用搜索框按文件名搜索文本
- 在文本列表中勾选需要搜索的文本
- 支持「全选」和「清空」快捷操作
- 显示已选择的文本数量

### 选择状态提示

选择完成后，系统会显示：
- 已选择的文本数量
- 成功/警告提示（根据选择数量）

## 词性筛选

词性筛选允许您限制搜索结果只包含特定词性的词语。

### 筛选类型

系统支持三种词性标签集：

#### Universal POS（通用词性）
- 基于 SpaCy Universal POS 标签集
- 包括：NOUN、VERB、ADJ、ADV、PRON、DET、ADP、AUX、CCONJ、SCONJ、PART、INTJ、NUM、PUNCT、SYM、X
- 适合大多数分析场景

#### Penn Treebank（宾州树库）
- 基于 Penn Treebank 标签集
- 更细粒度的词性分类
- 适合需要精确词性匹配的场景

#### Dependency Relations（依存关系）
- 基于 SpaCy 依存关系标签
- 包括：nsubj、dobj、amod、det、prep 等
- 适合语法分析场景

### 筛选模式

- **保留模式**：只显示包含所选词性的结果
- **过滤模式**：排除包含所选词性的结果

## 搜索配置

### 搜索模式

系统提供六种搜索模式，满足不同的搜索需求：

#### Simple（简单搜索）

最简单的搜索模式，支持通配符和基本语法。

**支持的通配符**：
- `*`：匹配任意字符（包括零个字符）
- `?`：匹配单个字符
- `|`：或运算符，匹配多个选项之一
- `--`：连字符变体，匹配带或不带连字符的词语

**示例**：
- `m*`：匹配以 "m" 开头的所有词语（如 "make", "many", "more"）
- `???t`：匹配四个字符且以 "t" 结尾的词语（如 "test", "that", "what"）
- `return|go back`：匹配 "return" 或 "go back"
- `multi--billion`：匹配 "multibillion" 或 "multi-billion"

#### Lemma（词元搜索）

基于词元（词根形式）进行搜索，可以匹配词语的所有变体。

**特点**：
- 支持正则表达式
- 自动匹配词语的所有形态变化

**示例**：
- `go`：匹配 "go", "goes", "went", "going", "gone"
- `b.*`：匹配词元以 "b" 开头的所有词语

#### Phrase（短语搜索）

精确匹配短语，支持正则表达式。

**特点**：
- 匹配完整的短语序列
- 支持多行输入（用于长短语）
- 支持正则表达式

**示例**：
- `the cat sat`：精确匹配短语 "the cat sat"
- `.*ing.*`：匹配包含 "ing" 的短语

#### Word（词形搜索）

精确匹配词形（word form），区分大小写。

**特点**：
- 区分大小写
- 精确匹配，不进行词形变化
- 支持正则表达式

**示例**：
- `Test`：只匹配 "Test"（不匹配 "test"）
- `test`：只匹配 "test"（不匹配 "Test"）

#### Character（字符搜索）

搜索包含特定字符或字符串的词语。

**特点**：
- 匹配词语中包含指定字符的部分
- 不区分大小写

**示例**：
- `ck`：匹配包含 "ck" 的词语（如 "check", "back", "lock"）
- `ing`：匹配包含 "ing" 的词语（如 "going", "singing", "thing"）

#### CQL（语料库查询语言）

使用 CQL（Corpus Query Language）进行高级查询，支持复杂的语法和语义匹配。

**CQL 语法**：
- `[word="text"]`：匹配词形
- `[lemma="go"]`：匹配词元
- `[pos="NOUN"]`：匹配词性（Universal POS）
- `[tag="NN"]`：匹配细粒度词性（Penn Treebank）
- `[dep="nsubj"]`：匹配依存关系

**运算符**：
- `&`：AND（与）
- `|`：OR（或）
- `!`：NOT（非）
- `[]`：任意 token
- `[]{2}`：2 个任意 token
- `[]{1,3}`：1-3 个任意 token

**示例**：
- `[word="test"]`：匹配词形 "test"
- `[pos="NOUN" & lemma="test"]`：匹配词性为名词且词元为 "test" 的词语
- `[lemma="make"] [] [pos="NOUN"]`：匹配 "make" + 任意词 + 名词
- `[word=".*ing"]`：匹配以 "ing" 结尾的词形

**CQL 构建器**：
- 点击搜索面板中的「CQL 构建器」按钮（或标题栏的构建器图标）
- 使用可视化界面构建 CQL 查询
- 支持模板管理和实时预览

### 搜索值

根据选择的搜索模式，在搜索框中输入相应的搜索值：

- **Simple**：输入搜索词和通配符
- **Lemma**：输入词元（支持正则表达式）
- **Phrase**：输入短语（支持多行）
- **Word**：输入词形（支持正则表达式）
- **Character**：输入字符或字符串
- **CQL**：输入 CQL 查询表达式（支持多行，带语法验证）

### 上下文长度

设置 KWIC 结果显示的上下文长度（左右各显示多少个词语）：

- **范围**：1-15 个词语
- **默认值**：5 个词语
- **建议**：根据研究需求调整，较长的上下文有助于理解语境

### 大小写处理

- **转换为小写**：勾选后，搜索不区分大小写（CQL 模式不支持此选项）
- 建议：分析英文时建议开启，以匹配所有大小写变体

## 运行搜索

配置完成后，点击「开始搜索」按钮：

1. 系统会显示加载进度
2. 搜索完成后，结果会自动显示在右侧面板
3. 如果出现错误，会在左侧面板显示错误信息

**注意**：
- 大语料库或复杂查询可能需要较长时间
- CQL 查询会进行实时语法验证
- 搜索结果数量可能很大，建议使用筛选和排序功能

## 结果表格

### 统计信息

表格顶部显示搜索统计：

- **总计**：搜索结果总数
- **显示**：当前页面显示的结果范围
- **筛选后**：应用筛选后的结果数量（如果有筛选）

### 表格列

| 列名 | 说明 |
|------|------|
| # | 结果序号 |
| 来源 | 文本文件名 |
| 左侧上下文 | 关键词左侧的上下文（右对齐） |
| 关键词 | 匹配的关键词（高亮显示） |
| 右侧上下文 | 关键词右侧的上下文（左对齐） |
| 词性 | 关键词的词性标签 |
| 展开 | 展开/收起按钮（查看扩展上下文） |

### 上下文颜色标记

为了便于识别上下文位置，系统使用颜色标记：

- **左侧上下文**：距离关键词最近的 3 个词语使用不同颜色（红色、绿色、紫色）
- **右侧上下文**：距离关键词最近的 3 个词语使用不同颜色（红色、绿色、紫色）
- 其他词语使用默认颜色

### 高亮隐喻

系统支持在 KWIC 结果中高亮显示隐喻词：

**启用方式**：
- 在工具栏中找到「高亮隐喻」开关
- 点击开关启用或禁用高亮

**显示效果**：
- 启用后，如果搜索词被标注为隐喻，将以**琥珀色背景**高亮显示
- 有助于快速识别隐喻用法的上下文

**技术说明**：
- 此功能依赖隐喻标注数据
- 确保语料库已完成隐喻标注（仅限英语）
- 高亮仅应用于搜索词本身

### 扩展上下文

点击表格行的展开按钮，可以查看更长的上下文：

- **默认范围**：±200 字符
- **扩展功能**：可以点击「显示更多上文」或「显示更多下文」按钮扩展上下文
- **高亮显示**：关键词在扩展上下文中会高亮显示
- **来源信息**：显示文本来源、位置和上下文范围

### 排序功能

点击工具栏的「排序」按钮，打开排序对话框：

#### 排序方式

- **左侧上下文**：按左侧上下文排序
- **右侧上下文**：按右侧上下文排序
- **频率**：按关键词频率排序（需要后端支持）

#### 排序级别

可以设置多个排序级别（最多 3 级）：

1. **位置选择**：
   - `3L`, `2L`, `1L`：左侧上下文的位置（3L 最远，1L 最近）
   - `KWIC`：关键词本身
   - `1R`, `2R`, `3R`：右侧上下文的位置（1R 最近，3R 最远）

2. **属性选择**：
   - `word`：词形
   - `lemma`：词元
   - `pos`：词性

3. **选项**：
   - `ignoreCase`：忽略大小写
   - `retrograde`：反向排序

#### 排序方向

- **升序**：从小到大排序
- **降序**：从大到小排序

**使用建议**：
- 按左侧上下文排序有助于发现词语的常见搭配
- 按右侧上下文排序有助于发现词语的后续模式
- 多级排序可以更精确地组织结果

### 筛选功能

点击工具栏的「筛选」按钮，打开筛选对话框：

#### 快速筛选

- **隐藏子匹配**：隐藏包含在其他匹配中的结果
- **仅每文档首次匹配**：每个文档只显示第一个匹配结果

#### 查询筛选

使用查询条件进一步筛选结果：

1. **查询类型**：选择查询类型（Simple、Word、Lemma、Phrase、Character、CQL）
2. **查询值**：输入查询条件
3. **范围类型**：
   - **Token 范围**：指定左右各多少个词语的范围
   - **句子范围**：在整个句子范围内搜索
   - **自定义范围**：自定义搜索范围
4. **包含/排除**：
   - **包含**：只显示包含查询条件的结果
   - **排除**：排除包含查询条件的结果
5. **排除关键词**：是否在搜索范围中包含关键词本身

### 导出功能

点击工具栏的「导出 CSV」按钮，将结果导出为 CSV 文件：

- 包含所有列的信息
- 支持中文编码（UTF-8 with BOM）
- 文件名包含日期信息

### 分页

- 支持 10、20、50、100 条/页
- 可在表格底部切换页码和每页显示数量

## 可视化

可视化面板提供两种图表类型，帮助您直观地理解搜索结果的分布模式。

### 图表类型切换

通过顶部标签页切换图表类型：

- **密度分布图**：显示关键词在文本中的位置分布
- **分组山脊图**：按文档分组显示关键词分布

### 密度分布图

密度分布图显示关键词在文本中的位置分布密度。

#### 图表特点

- X 轴：文本中的相对位置（0-100%）
- Y 轴：密度值
- 曲线：显示关键词在不同位置的分布密度
- 颜色：根据配色方案显示

#### 配置选项

- **配色方案**：选择图表的颜色主题（蓝色、绿色、紫色、橙色、红色）

#### 使用场景

- 分析关键词在文本中的分布模式
- 发现关键词的典型出现位置
- 识别文本结构特征

### 分组山脊图

分组山脊图按文档分组显示关键词的分布情况。

#### 图表特点

- X 轴：文本中的相对位置（0-100%）
- Y 轴：文档分组
- 山脊：每个文档的分布曲线
- 颜色：根据配色方案显示

#### 配置选项

- **配色方案**：选择图表的颜色主题
- **显示文档数**：设置最多显示多少个文档（默认：10，范围：1-50）

#### 使用场景

- 比较不同文档中关键词的分布
- 发现文档间的分布差异
- 识别特定文档的特征

### 导出功能

所有图表都支持导出：

- **导出 SVG**：导出为矢量图格式，适合打印和编辑
- **导出 PNG**：导出为位图格式，适合插入文档

导出按钮位于图表设置栏右侧。

## 使用技巧

### 高效搜索流程

1. **选择语料库**：根据研究目标选择合适语料库
2. **筛选文本**：使用标签或手动选择，聚焦目标文本
3. **选择搜索模式**：根据搜索需求选择合适的模式
4. **输入搜索值**：输入搜索词或查询表达式
5. **设置上下文**：调整上下文长度，平衡信息量和可读性
6. **运行搜索**：点击搜索按钮
7. **查看结果**：在表格中查看 KWIC 结果
8. **排序和筛选**：使用排序和筛选功能精确定位目标结果
9. **可视化分析**：使用图表发现分布模式
10. **导出数据**：将结果导出用于进一步分析

### 搜索模式选择建议

#### 使用 Simple 模式当：
- 需要快速搜索常见词语
- 需要使用通配符进行模糊匹配
- 搜索需求相对简单

#### 使用 Lemma 模式当：
- 需要匹配词语的所有形态变化
- 分析词语的语法变体
- 不关心具体词形

#### 使用 Phrase 模式当：
- 需要精确匹配短语
- 搜索固定表达
- 分析多词组合

#### 使用 Word 模式当：
- 需要区分大小写
- 精确匹配特定词形
- 分析专有名词

#### 使用 Character 模式当：
- 需要搜索包含特定字符的词语
- 分析词语的拼写特征
- 发现词语模式

#### 使用 CQL 模式当：
- 需要复杂的语法和语义匹配
- 需要组合多个条件
- 进行高级语料库查询

### CQL 查询技巧

#### 基本查询

- `[word="test"]`：匹配词形 "test"
- `[lemma="go"]`：匹配词元 "go" 的所有变体
- `[pos="NOUN"]`：匹配所有名词

#### 组合查询

- `[pos="NOUN" & lemma="test"]`：名词且词元为 "test"
- `[pos="NOUN" | pos="VERB"]`：名词或动词
- `[!pos="PUNCT"]`：非标点符号

#### 序列查询

- `[lemma="make"] [] [pos="NOUN"]`：make + 任意词 + 名词
- `[word="the"] []{2} [pos="NOUN"]`：the + 2 个任意词 + 名词
- `[lemma="go"] []{1,3} [pos="NOUN"]`：go + 1-3 个任意词 + 名词

#### 正则表达式

- `[word=".*ing"]`：以 "ing" 结尾的词形
- `[lemma="b.*"]`：词元以 "b" 开头
- `[word="^[A-Z].*"]`：以大写字母开头的词形

### 常见分析场景

#### 分析词语搭配
1. 使用 Simple 或 Lemma 模式搜索目标词语
2. 设置合适的上下文长度（5-10）
3. 按左侧或右侧上下文排序
4. 查看扩展上下文了解完整语境

#### 分析语法模式
1. 使用 CQL 模式构建语法查询
2. 例如：`[pos="VERB"] [] [pos="NOUN"]` 查找动词+名词模式
3. 使用词性筛选进一步限制结果

#### 分析短语使用
1. 使用 Phrase 模式搜索目标短语
2. 查看密度分布图了解短语在文本中的分布
3. 使用分组山脊图比较不同文档的使用情况

#### 发现语言模式
1. 使用 Character 模式搜索特定字符组合
2. 使用正则表达式在 CQL 模式中搜索模式
3. 结合排序和筛选功能发现规律

### 注意事项

- KWIC 搜索基于 SpaCy 标注数据，确保文本已完成 SpaCy 标注
- 大语料库或复杂查询可能需要较长时间，请耐心等待
- CQL 查询会进行实时语法验证，错误的查询会显示错误信息
- 搜索结果数量可能很大，建议使用筛选和排序功能
- 上下文长度影响结果的可读性，建议根据研究需求调整
- 扩展上下文功能会增加服务器负载，建议适度使用
- 导出 CSV 时，如果数据量大，可能需要一些时间
- 可视化图表在结果数量很大时可能显示较慢

# 语义分析

本模块提供基于 USAS 语义标注系统的语义分析功能。

## 理论基础与参考文献

### USAS 语义标注系统

USAS（UCREL Semantic Analysis System）是由兰卡斯特大学 UCREL 研究中心开发的语义标注系统，将词语归类到 21 个主要语义域和数百个子域中。

**官方文档与工具**：
- PyMUSAS 官方文档：[https://ucrel.github.io/pymusas/](https://ucrel.github.io/pymusas/)

### Hybrid 标注方法

Meta-Lingo 支持混合标注模式（Hybrid），结合规则方法和神经网络方法进行语义消歧。

**参考论文**：
- Hybrid 方法论文：[https://arxiv.org/pdf/2601.09648](https://arxiv.org/pdf/2601.09648)

### MIPVU 隐喻标注方法

MIPVU（Metaphor Identification Procedure VU）是由阿姆斯特丹自由大学开发的隐喻识别方法，是语料库语言学中最广泛使用的隐喻标注标准。

**核心概念**：
- **基本义（Basic Meaning）**：词语在其他上下文中更具体、身体相关或历史更早的含义
- **上下文义（Contextual Meaning）**：词语在当前上下文中的实际含义
- **隐喻判定**：如果上下文义与基本义不同，但可以通过对比理解，则判定为隐喻

**参考文献**：
- Steen, G., Dorst, L., Herrmann, J., Kaal, A., Krennmayr, T., & Pasma, T. (2010). *A method for linguistic metaphor identification: From MIP to MIPVU*. John Benjamins Publishing.

---

## 隐喻分析

### 概述

隐喻分析模块基于 MIPVU（Metaphor Identification Procedure VU）标注方法，使用混合检测方案自动识别英语文本中的隐喻词汇。该模块结合规则过滤和深度学习模型，能够高效、准确地标注文本中的隐喻表达。

**支持语言**：目前仅支持英语

### 理论基础与参考文献

#### MIPVU 标注方法

MIPVU（Metaphor Identification Procedure VU）是由阿姆斯特丹自由大学开发的隐喻识别方法，是语料库语言学中最广泛使用的隐喻标注标准。该方法基于词汇的"基本义"（basic meaning）和"上下文义"（contextual meaning）之间的对比来判断词语是否具有隐喻用法。

**核心原则**：
1. 确定词语在上下文中的含义
2. 判断该词语是否有更基本的含义（在其他上下文中更具体、身体相关、历史更早）
3. 如果上下文义与基本义不同，但可以通过对比理解，则判定为隐喻

**参考文献**：
- Steen, G., Dorst, L., Herrmann, J., Kaal, A., Krennmayr, T., & Pasma, T. (2010). *A method for linguistic metaphor identification: From MIP to MIPVU*. John Benjamins Publishing.

#### HiTZ 隐喻检测模型

本系统使用 HiTZ 研究团队（巴斯克大学）开发的预训练隐喻检测模型作为核心组件。

**模型信息**：
- **模型名称**：deberta-large-metaphor-detection-en
- **基础架构**：DeBERTa-large
- **任务类型**：Token Classification（序列标注）
- **模型来源**：[HuggingFace - HiTZ/deberta-large-metaphor-detection-en](https://huggingface.co/HiTZ/deberta-large-metaphor-detection-en)

**参考文献**：
- Sanchez-Bayona, E., & Agerri, R. (2022). Leveraging a New Spanish Corpus for Multilingual and Cross-lingual Metaphor Detection. *Proceedings of the 26th Conference on Computational Natural Language Learning (CoNLL)*, 228-240.

#### DeBERTa 模型

**参考文献**：
- He, P., Liu, X., Gao, J., & Chen, W. (2021). DeBERTa: Decoding-enhanced BERT with Disentangled Attention. *International Conference on Learning Representations*.

### 检测方法

Meta-Lingo 采用四步混合检测方案，结合规则过滤和双模型集成：

#### 第 1 步：词形过滤

使用 MIPVU 映射词表（`metaphor_filter.json`）进行初步过滤：
- **词表来源**：基于 VUA 语料库训练集统计
- **筛选标准**：出现次数 > 10 且 100% 为非隐喻的词语
- **词表规模**：1,098 个高频非隐喻词/词组
- **匹配方式**：词形精确匹配（小写）
- **输出**：匹配的词语直接标注为非隐喻

#### 第 2 步：SpaCy 规则过滤

基于 SpaCy 标注数据进行语法规则过滤：

**词性过滤**：
- CD（基数词）：如数字、年份
- NNP（专有名词）：如人名、地名
- SYM（符号）：如标点、特殊符号

**语法模式过滤**：
- "to" + 动词：不定式中的 "to" 为非隐喻

**超高置信度规则**：
基于依存关系和词形的组合规则（字面率 > 99%）：

| 依存关系 | 适用词语 | 说明 |
|---------|---------|------|
| det | the, a, an, some, no, any | 限定词 |
| neg | n't, not, never | 否定词 |
| mark | if, as, cos, because | 从句标记 |
| advmod | so, just, when, very, only, really, too, more | 副词修饰 |
| predet | all | 前置限定词 |
| intj | well | 感叹词 |
| agent | *（任意）* | 被动语态施事 |

**注意**：指示代词（this, that, these, those）的隐喻率高达 73-87%，**不会**被规则过滤。

#### 第 3 步：HiTZ 模型预测

使用 HiTZ 预训练模型进行隐喻判断：
- **输入**：完整句子
- **输出**：每个词的隐喻标签
- **标签映射**：
  - LABEL_0 / LABEL_1 -> 隐喻
  - LABEL_2 -> 非隐喻（待定）

#### 第 4 步：IDRRP 模型二次检测

针对 HiTZ 模型在功能词上的局限性，使用专门针对 IN（介词）、DT（限定词）、RB（副词）、RP（小品词）微调的 IDRRP 模型进行二次检测：

**触发条件**：
- 第 3 步判断为非隐喻
- 词性为 IN（介词）、DT（限定词）、RB（副词）或 RP（小品词）

**IDRRP 模型**：
- **模型名称**：deberta-v3-large-metaphor-in-dt-rb-rp
- **基础架构**：microsoft/deberta-v3-large
- **目标词性**：IN、DT、RB、RP
- **判断阈值**：P(隐喻) >= 0.4
- **模型来源**：[HuggingFace - tommyleo2077/deberta-v3-large-metaphor-in-dt-rb-rp](https://huggingface.co/tommyleo2077/deberta-v3-large-metaphor-in-dt-rb-rp)

### 检测可靠性

基于 VUA 语料库测试集（10 个文件，23,588 词）的评估结果：

#### 整体性能对比

| 方案 | F1 分数 | 精确率 | 召回率 |
|------|--------|--------|--------|
| HiTZ 单模型 | 60.6% | 89.90% | 45.67% |
| **混合方案** | **78.7%** | 76.86% | **80.53%** |
| **提升** | **+18.1%** | -13.04% | **+34.86%** |

#### 按词性分析

| 评估范围 | F1 分数 | 精确率 | 召回率 |
|---------|--------|--------|--------|
| HiTZ 在 IN/DT/RB/RP | 9.0% | 93.22% | 4.74% |
| IDRRP 在 IN/DT/RB/RP | **72.1%** | 65.45% | 80.17% |
| HiTZ 在其他词性 | 84.5% | 89.74% | 79.76% |

#### 关键发现

1. **HiTZ 模型局限**：在介词、限定词、副词、小品词上的 F1 仅为 9.0%
2. **混合方案优势**：通过专门的 IDRRP 模型，将这些词性的 F1 提升至 72.1%
3. **整体提升**：所有 10 个测试文件的混合方案均优于单模型

### 界面布局

隐喻分析模块采用三栏布局：

- **左侧面板**（300px）：
  - 语料库选择
  - 文本选择
  - 词性筛选器（可选择显示特定词性的隐喻）

- **中间面板**（弹性宽度）：
  - **表格视图**：逐词显示隐喻标注结果
  - **文本视图**：在原文中高亮显示隐喻词

- **右侧面板**（350px）：
  - 统计信息（隐喻数量、比例等）
  - 可视化图表（柱状图、饼图、词云）

### 语料选择

#### 选择语料库

1. 在左侧面板顶部的下拉菜单中选择目标语料库
2. 系统会显示语料库的文本数量
3. **注意**：隐喻分析仅支持英语语料库

#### 文本选择模式

系统提供三种文本选择模式：

- **全部文本**：分析语料库中的所有文本
- **按标签筛选**：根据标签筛选文本
- **手动选择**：手动选择特定文本

### 词性筛选

左侧面板提供词性筛选器，可以选择只显示特定词性的隐喻结果：

- **全部词性**：显示所有隐喻
- **名词（NOUN）**：只显示名词隐喻
- **动词（VERB）**：只显示动词隐喻
- **形容词（ADJ）**：只显示形容词隐喻
- **副词（ADV）**：只显示副词隐喻
- **介词（ADP）**：只显示介词隐喻
- **其他**：限定词、小品词等

### 运行分析

配置完成后，点击「开始分析」按钮：

1. 系统会显示加载进度
2. 分析完成后，结果会自动显示在中间和右侧面板
3. 如果出现错误，会显示错误信息

**注意事项**：
- 隐喻分析基于 SpaCy 标注数据，确保文本已完成 SpaCy 标注
- 大语料库分析可能需要较长时间，请耐心等待
- 仅支持英语文本

### 结果展示

#### 表格视图

表格显示每个被标注的隐喻词：

| 列名 | 说明 |
|------|------|
| 词语 | 隐喻词的原文形式 |
| 词元 | 词语的词典形式 |
| 词性 | SpaCy Universal POS 标签 |
| 来源 | 词语所在的文本文件 |
| 位置 | 词语在文本中的位置 |

#### 文本视图

在原文中高亮显示隐喻词：
- 隐喻词以琥珀色背景高亮
- 点击高亮词可查看详细信息

### 统计信息

右侧面板显示统计摘要：

- **总词数**：分析的总词数
- **隐喻数**：检测到的隐喻数量
- **隐喻率**：隐喻词占总词数的百分比
- **按词性分布**：各词性的隐喻数量

### 可视化

系统提供三种可视化图表：

#### 柱状图

- 显示各词性的隐喻数量分布
- 支持颜色方案选择
- 点击柱状可跳转到对应词性的结果

#### 饼图

- 显示各词性隐喻的占比
- 使用环形图设计
- 悬停显示详细数据

#### 词云图

- 显示高频隐喻词
- 词语大小根据频率调整
- 支持颜色映射选择

### 导出功能

- **导出 CSV**：导出隐喻标注结果为 CSV 文件
- **导出图表**：支持 SVG 和 PNG 格式

### 集成功能

#### 重新标注按钮

在语料库详情页面，可以对文本进行隐喻重新标注：
- 选择文本后点击「隐喻重新标注」按钮
- 系统会重新运行隐喻检测流程

#### 自动标注

上传新文本时，系统会自动进行隐喻标注（仅限英语文本）。

#### 编辑后重新标注

在文本编辑保存后，系统会自动触发隐喻重新标注。

### 使用技巧

#### 高效分析流程

1. **选择语料库**：选择英语语料库
2. **筛选文本**：使用标签或手动选择聚焦目标文本
3. **运行分析**：点击分析按钮
4. **查看结果**：在表格和文本视图中查看隐喻标注
5. **筛选词性**：使用词性筛选器关注特定类型的隐喻
6. **可视化分析**：使用图表了解隐喻分布
7. **导出数据**：导出结果用于进一步分析

#### 常见分析场景

**分析学术文本的隐喻使用**：
1. 选择学术文本语料库
2. 运行隐喻分析
3. 查看名词和动词隐喻的分布
4. 关注介词隐喻（如 "in terms of"）

**分析新闻报道的隐喻**：
1. 选择新闻语料库
2. 运行隐喻分析
3. 使用词云图发现高频隐喻词
4. 对比不同主题新闻的隐喻率

**分析文学作品的隐喻风格**：
1. 选择文学作品语料库
2. 运行隐喻分析
3. 查看整体隐喻率
4. 分析不同词性隐喻的使用偏好

### 注意事项

- 隐喻分析仅支持英语文本
- 分析基于 SpaCy 标注数据，确保文本已完成 SpaCy 标注
- 检测结果为自动标注，可能存在误判
- 某些隐喻表达（如死隐喻）可能不被检测到
- 大语料库分析可能需要较长时间
- 建议结合人工校验使用

---

## 语义域分析

### 概述

语义域分析模块基于 USAS（UCREL Semantic Analysis System）标注数据，对语料库中的词语进行语义域分类和统计分析。USAS 是一个多层次的语义标注系统，将词语归类到不同的语义域中，帮助研究者理解语料库的语义特征和主题分布。

### 界面布局

语义域分析模块采用左右分栏布局：

- **左侧面板**（400px）：配置面板，包含语料选择、结果模式选择、词性筛选、搜索配置等
- **右侧面板**（弹性宽度）：结果展示区域，包含两个标签页
  - **结果表格**：显示语义域统计结果
  - **可视化**：提供柱状图和饼图两种可视化方式

### 语料选择

#### 选择语料库

1. 在左侧面板顶部的下拉菜单中选择目标语料库
2. 系统会显示语料库的文本数量
3. 选择语料库后，系统会自动加载该语料库中的所有文本

#### 文本选择模式

系统提供三种文本选择模式，与其他分析模块相同：

- **全部文本**
  - 选择「全部文本」模式
  - 分析将包含语料库中的所有文本
  - 显示文本总数

- **按标签筛选**
  - 选择「按标签筛选」模式
  - 从下拉菜单中选择一个或多个标签
  - 系统会筛选出包含这些标签的所有文本
  - 显示筛选后的文本数量

- **手动选择**
  - 选择「手动选择」模式
  - 使用搜索框按文件名搜索文本
  - 在文本列表中勾选需要分析的文本
  - 支持「全选」和「清空」快捷操作
  - 显示已选择的文本数量

#### 选择状态提示

选择完成后，系统会显示：
- 已选择的文本数量
- 成功/警告提示（根据选择数量）

### 结果模式选择

系统提供两种结果展示模式：

#### 按语义域（By Domain）

统计每个语义域的出现频率和占比。

**特点**：
- 显示语义域代码、名称、所属大类
- 统计每个语义域的总频率和百分比
- 可以查看每个语义域包含的所有词语
- 适合分析语料库的整体语义分布

**表格列**：
- 排名
- 语义域代码
- 语义域名称
- 所属大类
- 频率
- 百分比
- 操作（查看词语）

#### 按词语（By Word）

统计每个词语所属的语义域及其频率。

**特点**：
- 显示词语、所属语义域、词性
- 统计每个词语-语义域组合的频率和百分比
- 可以跳转到其他分析模块（共现分析、词速写）
- 适合分析特定词语的语义特征

**表格列**：
- 排名
- 词语
- 语义域代码
- 语义域名称
- 词性
- 频率
- 百分比
- 操作（跨模块链接）

#### 高亮隐喻词

在「按词语」模式下，系统支持高亮显示隐喻词功能：

**启用方式**：
- 在工具栏中找到「高亮隐喻词」开关
- 点击开关启用或禁用高亮

**显示效果**：
- 启用后，隐喻词会以**绿色加粗**样式显示
- 表格中的隐喻词一目了然

**CSV 导出**：
- 导出 CSV 时，隐喻词会用 `**word**` 标记
- 例如：`**metaphorical_word**`

**技术说明**：
- 此功能依赖后端返回的 `is_metaphor` 字段
- 确保文本已完成隐喻标注
- 仅在「按词语」模式下可用

**使用建议**：
- **按语义域**：适合分析语料库的整体语义特征和主题分布
- **按词语**：适合分析特定词语的语义分类和用法

### 词性筛选

词性筛选基于 SpaCy Universal POS 标签集，允许您只分析特定词性的词语。

#### 筛选模式

- **保留模式**：只统计包含您选择的词性标签的词语。例如：选择 NOUN（名词）和 VERB（动词），则只统计名词和动词的语义域
- **过滤模式**：排除包含您选择的词性标签的词语。例如：选择 PUNCT（标点），则排除标点的语义域

#### 词性标签分类

系统将词性标签分为三类，便于选择：

**实词（Content Words）**：
- **NOUN**：名词
- **VERB**：动词
- **ADJ**：形容词
- **ADV**：副词
- **PROPN**：专有名词

**虚词（Function Words）**：
- **ADP**：介词
- **AUX**：助动词
- **CCONJ**：并列连词
- **DET**：限定词
- **PART**：助词
- **PRON**：代词
- **SCONJ**：从属连词

**其他（Other）**：
- **INTJ**：感叹词
- **NUM**：数词
- **PUNCT**：标点
- **SYM**：符号
- **X**：其他

#### 快捷操作

- **全选**：快速选择所有词性标签
- **清空**：清除所有选择

### 搜索配置

搜索配置面板提供多种过滤选项，帮助您精确定位目标语义域或词语。

#### 频率范围

设置语义域或词语的频率范围：

- **最小频率**：只显示出现次数大于等于此值的语义域/词语（默认：1）
- **最大频率**：只显示出现次数小于等于此值的语义域/词语（可选，设为 0 表示无限制）

#### 大小写处理

- **转换为小写**：勾选后，所有词语统一转换为小写进行统计
- 建议：分析英文时建议开启，以合并大小写变体

#### 搜索类型

- **全部**：不进行搜索过滤，统计所有符合条件的语义域/词语
- **开头匹配（Starts）**：只统计以指定字符串开头的语义域代码或词语。例如：输入 "A" 可统计所有以 "A" 开头的语义域（如 "A1", "A2"）
- **结尾匹配（Ends）**：只统计以指定字符串结尾的语义域代码或词语。例如：输入 "1" 可统计所有以 "1" 结尾的语义域（如 "A1", "B1"）
- **包含匹配（Contains）**：只统计包含指定字符串的语义域代码或词语。例如：输入 "time" 可统计包含 "time" 的词语
- **正则表达式（Regex）**：使用正则表达式进行高级匹配，支持完整的正则表达式语法。例如：`^A.*` 匹配以 "A" 开头的语义域代码
- **词表匹配（Wordlist）**：输入一个词语列表（每行一个），只统计包含列表中词语的结果，适合分析特定词汇集合的语义域分布

#### 排除词语

在「排除词语」文本框中输入要排除的词语（每行一个），包含这些词语的结果将不会出现在统计结果中。

**使用场景**：
- 排除停用词
- 排除特定干扰词
- 排除人名、地名等专有名词

### 运行分析

配置完成后，点击「开始分析」按钮：

1. 系统会显示加载进度
2. 分析完成后，结果会自动显示在右侧面板
3. 如果出现错误，会在左侧面板显示错误信息

**注意**：
- 语义域分析基于 USAS 标注数据，确保文本已完成 USAS 标注
- 大语料库分析可能需要较长时间，请耐心等待
- 建议先使用较小的文本集合进行测试

### 结果表格

#### 统计信息

表格顶部显示统计摘要：

- **总词数**：所有符合条件的词语总数
- **唯一语义域数**：不重复的语义域数量
- **唯一词语数**：不重复的词语数量
- **结果数量**：当前显示的结果总数

#### 表格列

**按语义域模式**：

| 列名 | 说明 |
|------|------|
| 排名 | 按频率排序的排名 |
| 语义域代码 | USAS 语义域代码（如 "A1", "B2"） |
| 语义域名称 | 语义域的完整名称 |
| 所属大类 | 语义域所属的 USAS 大类（21 个大类） |
| 频率 | 该语义域的出现次数 |
| 百分比 | 占总词数的百分比 |
| 操作 | 查看该语义域包含的所有词语 |

**按词语模式**：

| 列名 | 说明 |
|------|------|
| 排名 | 按频率排序的排名 |
| 词语 | 词语本身 |
| 语义域代码 | 该词语所属的语义域代码 |
| 语义域名称 | 该词语所属的语义域名称 |
| 词性 | 词语的词性标签 |
| 频率 | 该词语-语义域组合的出现次数 |
| 百分比 | 占总词数的百分比 |
| 操作 | 跨模块链接（共现分析、词速写） |

#### 排序功能

点击列标题可进行排序：

- **排名**：按频率排名排序
- **语义域/词语**：按字母顺序排序
- **频率**：按出现次数排序
- **百分比**：按百分比排序

支持升序/降序切换。

#### 查看语义域词语

在「按语义域」模式下，点击表格行的「查看词语」按钮（信息图标），可以查看该语义域包含的所有词语：

- 显示对话框，列出该语义域下的所有词语
- 显示每个词语的频率
- 可以跳转到其他分析模块（共现分析、词速写）
- 显示该语义域的总词语数

#### 跨模块链接

在「按词语」模式下，表格中会显示「操作」列，提供：

- **共现分析**：跳转到共现分析模块，分析该词语的共现关系
- **词速写**：跳转到词速写模块，查看该词语的语法模式

#### 导出功能

点击表格顶部的「导出」按钮，将结果导出为 CSV 文件：

- **按语义域模式**：包含排名、语义域代码、语义域名称、所属大类、频率、百分比
- **按词语模式**：包含排名、词语、语义域代码、语义域名称、词性、频率、百分比
- 文件名包含日期和模式信息

#### 分页

- 支持 10、25、50、100 条/页
- 可在表格底部切换页码和每页显示数量

### 可视化

可视化面板提供两种图表类型，帮助您直观地理解语义域分布。

#### 图表类型切换

通过顶部标签页切换图表类型：

- **柱状图**：适合展示前 N 个高频语义域
- **饼图**：适合展示语义域占比分布

#### 柱状图

**配置选项**：
- **最大显示数量**：设置显示前多少个语义域（默认：20，范围：5-50）
- **颜色方案**：选择柱状图的颜色主题（蓝色、绿色、紫色、橙色、红色）
- **显示百分比**：是否在柱状上显示百分比标签

**交互功能**：
- 点击柱状可跳转到结果表格（如果启用了点击回调）
- 图表高度会根据显示数量自动调整

**图表特点**：
- X 轴：语义域代码或名称
- Y 轴：频率或百分比
- 柱状颜色：根据配色方案显示
- 标签：显示频率或百分比（如果启用）

#### 饼图

**配置选项**：
- **最大显示数量**：设置显示前多少个语义域（默认：10，范围：5-20）
- **颜色方案**：选择饼图的颜色主题
- **显示百分比**：是否在饼图上显示百分比标签

**图表特点**：
- 使用环形图（Donut Chart）设计
- 显示图例，便于识别
- 点击扇形可跳转到结果表格（如果启用了点击回调）
- 颜色根据配色方案自动分配

#### 导出功能

所有图表都支持导出：

- **导出 SVG**：导出为矢量图格式，适合打印和编辑
- **导出 PNG**：导出为位图格式，适合插入文档

导出按钮位于图表设置栏右侧。

### USAS 语义域系统

#### 概述

USAS（UCREL Semantic Analysis System）是一个多层次的语义标注系统，将词语归类到不同的语义域中。系统包含 21 个主要大类，每个大类下包含多个子语义域。

#### 21 个主要大类

1. **A**: General & Abstract Terms（通用和抽象术语）
2. **B**: The Body & The Individual（身体和个体）
3. **C**: Arts & Crafts（艺术和工艺）
4. **E**: Emotional Actions, States & Processes（情感行动、状态和过程）
5. **F**: Food & Farming（食物和农业）
6. **G**: Government & Public（政府和公共）
7. **H**: Architecture, Housing & The Home（建筑、住房和家庭）
8. **I**: Money & Commerce（金钱和商业）
9. **K**: Entertainment, Sports & Games（娱乐、体育和游戏）
10. **L**: Life & Living Things（生命和生物）
11. **M**: Movement, Location, Travel & Transport（移动、位置、旅行和交通）
12. **N**: Numbers & Measurement（数字和测量）
13. **O**: Substances, Materials, Objects & Equipment（物质、材料、物体和设备）
14. **P**: Education（教育）
15. **Q**: Language & Communication（语言和通信）
16. **S**: Social Actions, States & Processes（社会行动、状态和过程）
17. **T**: Time（时间）
18. **W**: The World & Our Environment（世界和我们的环境）
19. **X**: Psychological Actions, States & Processes（心理行动、状态和过程）
20. **Y**: Science & Technology（科学和技术）
21. **Z**: Names & Grammatical Words（名称和语法词）

#### 语义域代码格式

语义域代码通常由一个大写字母（表示大类）和一个数字（表示子类）组成，例如：
- **A1**: General actions, making, etc.（一般行动、制作等）
- **B1**: Anatomy & Physiology（解剖学和生理学）
- **E1**: Happy（快乐）

某些语义域可能包含更细的分类，使用小数点或附加字母表示。

#### 语义标签后缀

USAS 语义标签可能包含特殊后缀，用于表达额外的语义信息：

| 后缀 | 含义 | 示例 | 说明 |
|------|------|------|------|
| **+** | 正面/积极 | `A5.1+` | 表示正面评价或积极含义，如 "good"、"excellent" |
| **-** | 负面/消极 | `A5.1-` | 表示负面评价或消极含义，如 "bad"、"terrible" |
| **++** | 强烈正面 | `E2++` | 表示强烈的正面情感，如 "love"、"adore" |
| **--** | 强烈负面 | `E2--` | 表示强烈的负面情感，如 "hate"、"despise" |
| **_MWE** | 多词表达式 | `A1.1.1_MWE` | 表示该标签来自多词表达式识别，如 "give up"、"in front of" |

**后缀使用说明**：

1. **极性后缀（+/-）**：主要用于表达评价性词语的情感极性
   - 无后缀：中性或极性不明显
   - `+`：正面含义
   - `-`：负面含义
   - `++`/`--`：强烈的正面/负面含义

2. **MWE 后缀**：
   - 仅在规则消歧模式和混合模式中出现
   - 神经网络模式不识别多词表达式，因此不会产生 `_MWE` 后缀
   - 多词表达式中的所有词语共享同一个带 `_MWE` 后缀的标签

3. **复合标签**：
   - 某些词语可能被分配复合标签，如 `N3.8+/A2.1`
   - 斜杠 `/` 分隔的多个语义域表示该词同时属于多个语义范畴
   - 在语义域分析统计中，复合标签的每个组成部分会被分别计数

### USAS 标注模式

系统支持三种 USAS 标注模式，可以在「应用设置」中进行选择。不同的标注模式具有不同的技术特点和适用场景。

#### 规则消歧模式 (Rule-Based)

规则消歧模式是系统的默认标注方式，基于 PyMUSAS 规则标注器实现。

**技术原理**：

1. **词典查询**：PyMUSAS 使用预编译的语义词典（包含单词和多词表达式），根据词语的词形和词性在词典中查找候选语义标签。每个词语可能匹配多个候选标签。

2. **多词表达式识别（MWE）**：系统能够识别多词表达式（如 "give up"、"in front of"），这些表达式作为整体分配语义标签，而非拆分为独立的单词。MWE 标签会添加 `_MWE` 后缀以便区分。

3. **消歧处理**：当一个词语有多个候选标签时，系统按照以下**严格的优先级顺序**依次尝试消歧策略。一旦某个策略成功选择标签，后续策略将不再执行：

   **第一优先级 - 文本类型优先**：
   - 检查上传时选择的文本类型（如医学、法律、体育等）
   - 根据文本类型对应的优先语义域列表，按顺序检查候选标签
   - 如果某个候选标签属于优先语义域，立即选择该标签，消歧结束
   - 例如：文本类型为「医学」时，B1（解剖学）、B2（健康与疾病）等标签具有优先权
   
   **第二优先级 - 话语域识别**：
   - 仅当文本类型优先未能选择标签时执行
   - 统计整篇文本中所有词语的语义标签分布
   - 计算 21 个大类（A-Z）的出现比例
   - 识别主导语义域（占比超过 20% 的大类）
   - 如果某个候选标签属于主导语义域对应的大类，选择该标签
   - 例如：文本中 N 类（数字测量）占比 35%，则优先选择 N 类相关的候选标签
   
   **第三优先级 - 一文一义规则**：
   - 仅当前两个策略均未能选择标签时执行
   - 基于「同一词元（lemma）在同一篇文本中应保持相同语义」的语言学假设
   - 统计同一词元在文本中的所有语义标签投票
   - 如果某个标签的投票占比超过 50%，且该标签在当前候选列表中，选择该标签
   - 例如：词语「bank」在文本中出现 5 次，其中 4 次被标注为 I1.1（金融），则优先选择 I1.1
   
   **默认选择**：
   - 如果以上三个策略均未能选择标签，选择候选列表中的第一个标签

4. **复合标签处理**：某些词语可能被标注为复合标签（如 `A3+/Q2.2`），系统会将其拆分为独立的标签进行统计。

**优点**：
- 支持多词表达式（MWE）识别
- 处理速度快，资源消耗低
- 标注结果可解释性强
- 支持复合标签和细粒度语义分类

**缺点**：
- 依赖词典覆盖率，未收录的词语无法标注（标记为 Z99 未知）
- 消歧规则可能在某些情况下不够准确

#### 神经网络模式 (Neural)

神经网络模式使用预训练的深度学习模型 PyMUSAS-Neural-Multilingual-Base-BEM 进行标注。

**技术原理**：

1. **语义嵌入**：该模型基于多语言 ModernBERT 架构微调，能够将词语及其上下文转换为高维语义向量。

2. **预测机制**：模型将语义标签预测任务视为词义消歧（Word Sense Disambiguation）问题。对于每个词语，模型根据其上下文嵌入计算与各语义标签嵌入的相似度，选择相似度最高的标签。

3. **单标签输出**：在此模式下，系统设置 `top_n=1`，每个词语只输出一个语义标签，无需额外消歧处理。

4. **上下文感知**：神经网络能够利用词语的上下文信息进行预测，即使词语不在词典中，也能根据上下文推断其语义。

5. **按句子处理**：为保证上下文完整性和处理长文本，系统按句子为单位调用神经网络。每个句子的词语在其完整的句子上下文中进行预测。

**优点**：
- 覆盖率高，能够为所有词语预测标签
- 利用上下文信息，对多义词的处理更加准确
- 不依赖词典，能处理新词和未登录词
- 按句子处理，支持任意长度的文本

**缺点**：
- 不支持多词表达式（MWE）识别
- 计算资源消耗较大
- 只输出单一语义标签，不支持复合标签
- 推理速度较慢

#### 混合模式 (Hybrid)

混合模式结合了规则消歧和神经网络两种方法的优点，是三种模式中最全面的方案。

**技术原理**：

1. **规则优先**：首先使用 PyMUSAS 规则标注器对所有词语进行标注，获取候选语义标签（包括多词表达式识别）。

2. **神经匹配**：对于有多个候选标签的词语（非 MWE、非 Z99），使用神经网络进行 `top_n=5` 的预测，按优先级顺序与规则候选进行匹配。匹配时只比较语义域名称（不含后缀如 +、-、_MWE），如果匹配成功则保留规则原有的后缀。神经网络**按句子为单位**处理，保留完整的上下文信息。

3. **未知词回退**：对于标记为 Z99（未知）的词语，在其所在句子的上下文中使用神经网络 `top_n=1` 进行预测。

4. **消歧处理**：对于神经匹配未成功的多候选词语，按照优先级顺序执行消歧策略（文本类型优先 > 话语域识别 > 一文一义规则）。

5. **最终回退**：消歧策略都未成功时，使用神经网络 `top_n=1` 作为最终回退。

**详细工作流程**：

**步骤 1 - 规则标注**：对输入文本使用 PyMUSAS 规则标注器，获取候选语义标签列表（包括多词表达式识别）。

**步骤 2 - 分类处理**：将词语分为三类：
- **Z99 词语**：规则标注器无法识别的未知词
- **多标签词语**：有多个候选标签的多义词（非 MWE）
- **单标签词语**：只有一个候选标签，或 MWE 词语，直接保留

**步骤 3 - 神经网络处理**：
- **Z99 词语**：调用神经网络 `top_n=1`，直接获得最终标签
- **多标签词语**：调用神经网络 `top_n=5`，按优先级顺序与规则候选匹配：
  - 匹配规则：只比较语义域基础名称（去除 +、-、_MWE 等后缀）
  - 匹配成功：选择对应的规则候选标签（保留原有后缀）
  - 匹配失败：标记为「待消歧」

**步骤 4 - 消歧处理**：对于神经匹配失败的词语，按优先级顺序执行消歧（一旦成功即停止）：
- **优先级 1**：文本类型优先 - 匹配成功则选择该标签，否则继续
- **优先级 2**：话语域识别 - 匹配成功则选择该标签，否则继续
- **优先级 3**：一文一义规则 - 投票 >50% 则选择该标签，否则标记为「待回退」

**步骤 5 - 最终回退**：对于消歧策略都未成功的词语，调用神经网络 `top_n=1` 获得最终标签。

**步骤 6 - 输出结果**：整合所有词语的标注结果，输出最终标注文本。

**优点**：
- 兼具规则模式和神经网络模式的优点
- 支持多词表达式识别
- 覆盖率高（神经网络补充未知词和消歧失败的词）
- 三重神经回退机制：多标签匹配 + 未知词回退 + 消歧失败回退
- 神经网络与规则候选协同决策，保留规则的后缀信息
- 标注质量通常优于单一模式

**缺点**：
- 计算资源消耗最大
- 处理速度最慢
- 需要同时加载规则模型和神经网络模型

#### 模式选择建议

| 场景 | 推荐模式 | 理由 |
|------|----------|------|
| 快速分析 | 规则消歧 | 速度快，资源消耗低 |
| 包含大量专业术语 | 混合模式 | 神经网络可补充专业词汇 |
| 需要高覆盖率 | 神经网络 / 混合模式 | 能处理未知词 |
| 关注多词表达式 | 规则消歧 / 混合模式 | 支持 MWE 识别 |
| 资源受限环境 | 规则消歧 | 不需要 GPU |
| 追求最高准确率 | 混合模式 | 综合两种方法的优点 |

#### 模式设置

在「应用设置」页面可以选择 USAS 标注模式：

1. 进入「应用设置」页面
2. 找到「USAS 标注模式」设置面板
3. 选择需要的模式（规则消歧 / 神经网络 / 混合模式）
4. 系统会自动保存设置

设置的模式将应用于：
- 语料上传时的自动 USAS 标注
- 语料详情页的 USAS 重新标注
- 语义域分析模块的标注过程

**注意**：神经网络模式和混合模式需要安装神经网络模型文件。如果模型未安装，这两个选项将显示为不可用状态。

### 使用技巧

#### 高效分析流程

1. **选择语料库**：根据研究目标选择合适语料库
2. **筛选文本**：使用标签或手动选择，聚焦目标文本
3. **选择结果模式**：根据研究问题选择合适的模式（按语义域或按词语）
4. **设置词性筛选**：根据研究问题选择相关词性
5. **配置搜索**：使用搜索和排除功能，精确定位目标语义域或词语
6. **运行分析**：点击分析按钮
7. **查看结果**：在表格中查看详细数据
8. **可视化分析**：使用图表发现语义分布模式
9. **深入分析**：点击语义域查看包含的词语，或跳转到其他分析模块
10. **导出数据**：将结果导出用于进一步分析

#### 结果模式选择建议

**使用「按语义域」模式当**：
- 想了解语料库的整体语义特征
- 分析主题分布和语义倾向
- 比较不同语料库的语义域分布
- 发现语料库的主要语义类别

**使用「按词语」模式当**：
- 想了解特定词语的语义分类
- 分析词语的多义性（一个词语可能属于多个语义域）
- 研究词语的语义用法
- 需要跳转到其他分析模块进行深入分析

#### 常见分析场景

**分析语料库的语义特征**：
1. 选择「按语义域」模式
2. 选择目标语料库
3. 设置词性筛选：保留实词（NOUN、VERB、ADJ、ADV）
4. 运行分析
5. 查看柱状图或饼图了解语义分布
6. 点击高频语义域查看包含的词语

**分析特定词语的语义分类**：
1. 选择「按词语」模式
2. 使用搜索功能筛选目标词语
3. 运行分析
4. 查看词语所属的语义域
5. 点击操作按钮跳转到共现分析或词速写

**比较不同语料库的语义域分布**：
1. 分别对两个语料库进行语义域分析
2. 使用「按语义域」模式
3. 导出结果进行对比
4. 分析语义域分布的差异

**发现语料库的主题倾向**：
1. 选择「按语义域」模式
2. 查看频率最高的语义域
3. 分析这些语义域所属的大类
4. 结合语义域名称理解语料库的主题倾向

### 注意事项

- 语义域分析基于 USAS 标注数据，确保文本已完成 USAS 标注
- 大语料库分析可能需要较长时间，请耐心等待
- USAS 标注可能不完整，某些词语可能没有语义域标签
- 一个词语可能属于多个语义域（多义性），系统会分别统计
- 语义域代码和名称基于 USAS 标准，可以参考 USAS 文档了解详细含义
- 导出 CSV 时，如果数据量大，可能需要一些时间
- 可视化图表在结果数量很大时可能显示较慢
- 查看语义域词语功能需要额外的服务器请求，请耐心等待

# 词图分析

## 概述

词图分析（Word Sketch）模块基于 SpaCy 依存句法标注数据，分析词语的语法搭配模式。该模块提供两种分析模式：**Word Sketch**（单个词语的语法搭配分析）和 **Word Sketch Difference**（两个词语的搭配对比分析），帮助研究者深入理解词语的语法行为和搭配特征。

## 实现原理

### 技术基础

Word Sketch 功能基于以下技术：

- **SpaCy 依存句法分析**：提取词语之间的句法关系
- **Universal Dependencies**：标准化的依存关系标签集
- **logDice 统计**：衡量搭配强度的统计方法

### 依存句法分析

Meta-Lingo 使用 SpaCy 进行依存句法分析，识别词语之间的语法关系。每个词语（token）都有一个 **head**（支配词）和一个 **dep**（依存关系类型）。

例如，在句子 "The quick brown fox jumps over the lazy dog" 中：
- "fox" 是 "jumps" 的主语（nsubj）
- "quick" 和 "brown" 是 "fox" 的修饰语（amod）
- "dog" 是 "over" 的宾语（pobj）

### 语法关系分类

系统将依存关系归类为 **50 种语法关系**，涵盖：

| 类别 | 关系类型 | 示例 |
|------|----------|------|
| 主语关系 | nsubj, nsubjpass, csubj | "fox jumps" (fox 是 jumps 的主语) |
| 宾语关系 | dobj, iobj, pobj | "eat apple" (apple 是 eat 的宾语) |
| 修饰关系 | amod, advmod, nmod | "big house" (big 修饰 house) |
| 介词关系 | prep, pcomp | "look at" (at 是 look 的介词) |
| 从句关系 | advcl, relcl, ccomp | 状语从句、关系从句 |
| 并列关系 | conj, cc | "bread and butter" |
| 其他关系 | det, aux, mark, case | 限定词、助动词等 |

### logDice 得分计算

**logDice** 是衡量搭配强度的统计方法，由 Sketch Engine 提出，具有以下特点：

- 值域：理论上 -∞ 到 14，实际常见范围 0-10
- 不受语料库大小影响
- 易于解释：分数越高，搭配越典型

计算公式：

$$\text{logDice} = 14 + \log_2 \frac{2 \times f_{xy}}{f_x + f_y}$$

其中：
- $f_{xy}$：词语 x 和 y 共现的频率
- $f_x$：词语 x 的频率
- $f_y$：词语 y 的频率

**分数解释**：
| 分数范围 | 搭配强度 |
|----------|----------|
| > 7 | 非常强的搭配 |
| 5-7 | 强搭配 |
| 3-5 | 中等搭配 |
| < 3 | 弱搭配 |

### Word Sketch Difference 原理

Word Sketch Difference 用于对比两个词语的搭配差异：

1. 分别计算两个词语的 Word Sketch
2. 找出共同的语法关系类型
3. 计算每个搭配词在两个词语中的 logDice 差异
4. 按差异程度排序，展示偏向某一词语的搭配

差异得分计算：

$$\text{Diff} = \text{logDice}(w_1, c) - \text{logDice}(w_2, c)$$

其中 $c$ 是搭配词，$w_1$ 和 $w_2$ 是对比的两个词语。

### 参考文献

- Kilgarriff, A., Rychly, P., Smrz, P., & Tugwell, D. (2004). The Sketch Engine. *Proceedings of EURALEX 2004*, 105-116.
- Kilgarriff, A., & Tugwell, D. (2001). Word Sketch: Extraction and Display of Significant Collocations for Lexicography. *Proceedings of ACL Workshop on COLLOCATION*, 32-38.
- Rychly, P. (2008). A Lexicographer-Friendly Association Score. *Proceedings of RASLAN*, 6-9.

## 界面布局

词图分析模块采用顶部标签页设计：

- **顶部标签页**：切换「Word Sketch」和「Word Sketch Difference」两种模式
- **左侧面板**（400px）：配置面板，包含语料选择、搜索配置等
- **右侧面板**（弹性宽度）：结果展示区域，包含两个标签页
  - **分析结果**：显示语法关系卡片或对比表格
  - **可视化**：提供网络图可视化

## Word Sketch（词图分析）

### 语料选择

与其他分析模块相同，支持三种文本选择模式：
- **全部文本**：分析语料库中的所有文本
- **按标签筛选**：根据标签筛选文本
- **手动选择**：手动选择特定文本

### 搜索配置

#### 搜索词语

在「搜索词语」输入框中输入要分析的词语：

- 支持输入词形（word form）或词元（lemma）
- 系统会根据词性筛选自动匹配
- 按 Enter 键可快速运行分析

#### 词性筛选

选择目标词语的词性，帮助系统更准确地识别词语：

- **自动（Auto）**：系统自动识别词语的词性
- **形容词（Adjective）**：限定为形容词
- **副词（Adverb）**：限定为副词
- **名词（Noun）**：限定为名词
- **动词（Verb）**：限定为动词
- **代词（Pronoun）**：限定为代词

**使用建议**：
- 如果词语有多种词性，建议选择「自动」让系统自动识别
- 如果明确知道词语的词性，选择对应词性可以提高准确性

#### 每个关系显示数量

设置每个语法关系卡片中显示的搭配词数量：

- **范围**：5-100（默认：12）
- 控制每个关系卡片中显示的搭配词数量
- 可以通过「显示更多」按钮查看更多结果

#### 最小频率

设置搭配词的最小出现频率：

- **范围**：1-100（默认：2）
- 只显示出现次数大于等于此值的搭配词
- 建议设置为 2-3，过滤掉偶然出现的搭配

#### 最小得分

设置搭配词的最小 logDice 得分：

- **范围**：0-14（默认：0，步长：0.5）
- logDice 得分衡量搭配强度，得分越高表示搭配越强
- 建议设置为 0，查看所有搭配，或设置为 3-5 查看强搭配

### 运行分析

配置完成后，点击「开始分析」按钮：

1. 系统会显示加载进度
2. 分析完成后，结果会自动显示在右侧面板
3. 所有关系卡片默认展开
4. 如果出现错误，会在左侧面板显示错误信息

**注意**：
- 词图分析基于 SpaCy 依存句法标注数据，确保文本已完成 SpaCy 标注
- 大语料库分析可能需要较长时间，请耐心等待
- 如果词语在语料库中出现次数很少，可能无法提取足够的语法关系

## 分析结果

### 统计摘要

结果顶部显示统计摘要：

- **目标词语**：正在分析的词语
- **总实例数**：该词语在语料库中的总出现次数
- **关系数量**：识别出的语法关系类型数量

### 语法关系卡片

系统使用 BERTopic 风格的卡片展示每个语法关系：

#### 卡片结构

每个关系卡片包含：

1. **卡片头部**：
   - **关系名称**：语法关系的名称（如 "主语"、"宾语"、"修饰语" 等）
   - **搭配数量**：显示当前显示的搭配词数量 / 总搭配词数量
   - **展开/收起按钮**：点击可展开或收起卡片内容

2. **卡片内容**（展开时）：
   - **搭配词表格**：显示该关系下的所有搭配词
   - **表格列**：
     - **搭配词**：与目标词语形成该语法关系的词语
     - **频率**：搭配词的出现次数
     - **得分**：logDice 得分（搭配强度）
     - **操作**：快捷操作菜单（如果启用）

3. **显示更多/更少按钮**：
   - 如果搭配词数量超过初始显示数量，会显示「显示更多」按钮
   - 点击可显示更多搭配词（每次增加设定的显示数量）
   - 如果已显示更多，会显示「显示更少」按钮，可恢复到初始显示数量

#### 语法关系类型

系统支持 50 种语法关系模板，根据目标词语的词性自动选择相关关系：

**动词（VERB）关系**（15 种）：
- 主语（Subject）
- 直接宾语（Direct Object）
- 间接宾语（Indirect Object）
- 介词宾语（Prepositional Object）
- 修饰语（Modifier）
- 等等

**名词（NOUN）关系**（15 种）：
- 修饰语（Modifier）
- 所有格（Possessive）
- 介词修饰（Prepositional Modifier）
- 等等

**形容词（ADJ）关系**（10 种）：
- 修饰的名词（Modified Noun）
- 比较对象（Compared With）
- 等等

**副词（ADV）关系**（10 种）：
- 修饰的动词（Modified Verb）
- 修饰的形容词（Modified Adjective）
- 等等

### 排序和筛选

- **按得分排序**：搭配词按 logDice 得分从高到低排序
- **按频率排序**：可以点击表格列标题进行排序
- **最小频率筛选**：通过「最小频率」参数过滤低频搭配
- **最小得分筛选**：通过「最小得分」参数过滤弱搭配

### 快捷操作

在搭配词表格中，每个搭配词都提供快捷操作菜单：

- **共现分析**：跳转到共现分析模块，分析该搭配词的共现关系
- **词图分析**：跳转到词图分析模块，分析该搭配词的语法搭配

### 展开/收起功能

- **展开全部**：点击工具栏的「展开全部」按钮，展开所有关系卡片
- **收起全部**：点击工具栏的「收起全部」按钮，收起所有关系卡片
- **单独展开/收起**：点击每个卡片头部的展开/收起按钮

## Word Sketch Difference（词图对比）

### 语料选择

与 Word Sketch 模式相同，支持三种文本选择模式。

### 搜索配置

#### 词语 1 和词语 2

在「词语 1」和「词语 2」输入框中分别输入要对比的两个词语：

- 支持输入词形或词元
- 系统会根据词性筛选自动匹配
- 两个词语应该具有相同的词性，才能进行有效对比

#### 词性筛选

选择两个词语的词性，与 Word Sketch 模式相同。

#### 最小频率

设置搭配词的最小出现频率，与 Word Sketch 模式相同。

#### 对比模式

选择对比时使用的匹配方式：

- **词元（Lemmas）**：基于词元进行匹配（推荐）
  - 例如："go" 和 "goes" 会被视为同一个词
  - 适合大多数对比场景
- **词形（Word Form）**：基于词形进行匹配
  - 例如："go" 和 "goes" 被视为不同的词
  - 适合需要区分词形的场景

### 运行分析

配置完成后，点击「开始分析」按钮开始对比分析。

## 对比结果

### 统计摘要

结果顶部显示对比统计：

- **词语 1 总关系数**：词语 1 的语法关系类型数量
- **词语 2 总关系数**：词语 2 的语法关系类型数量
- **共有关系数**：两个词语共同拥有的语法关系类型数量

### 语法关系卡片

对比模式下的关系卡片显示两个词语的搭配对比：

#### 卡片结构

每个关系卡片包含：

1. **卡片头部**：
   - **关系名称**：语法关系的名称
   - **统计信息**：显示共有、词语 1 独有、词语 2 独有的搭配数量
   - **展开/收起按钮**

2. **卡片内容**（展开时）：
   - **对比表格**：显示该关系下的所有搭配词及其对比数据
   - **表格列**：
     - **搭配词**：搭配词本身
     - **词语 1 频率**：在词语 1 的搭配中出现次数
     - **词语 1 得分**：与词语 1 的 logDice 得分
     - **词语 2 频率**：在词语 2 的搭配中出现次数
     - **词语 2 得分**：与词语 2 的 logDice 得分
     - **得分差异**：两个得分的差值（正数表示词语 1 更强，负数表示词语 2 更强）
     - **操作**：快捷操作菜单

#### 颜色编码

系统使用颜色编码帮助您快速识别搭配差异：

- **蓝色系**（得分差异 ≥ 2）：词语 1 的搭配更强
  - 深蓝色（≥ 6）：词语 1 明显更强
  - 蓝色（≥ 4）：词语 1 较强
  - 浅蓝色（≥ 2）：词语 1 稍强
- **灰色**（-2 < 得分差异 < 2）：两个词语的搭配强度相近
- **红色系**（得分差异 ≤ -2）：词语 2 的搭配更强
  - 浅红色（≤ -2）：词语 2 稍强
  - 红色（≤ -4）：词语 2 较强
  - 深红色（≤ -6）：词语 2 明显更强

**表格行背景色**：
- 根据得分差异使用不同深浅的背景色
- 蓝色背景表示词语 1 的搭配更强
- 红色背景表示词语 2 的搭配更强
- 透明背景表示搭配强度相近

**表格左侧指示条**：
- 每个表格行左侧有一个彩色指示条
- 颜色与得分差异对应，快速识别搭配差异

#### 搭配分类

对比结果中的搭配词分为三类：

1. **共有搭配**：两个词语都有的搭配词
   - 显示两个词语的频率和得分
   - 通过得分差异判断哪个词语的搭配更强

2. **词语 1 独有**：只有词语 1 有的搭配词
   - 只显示词语 1 的频率和得分
   - 使用蓝色高亮显示

3. **词语 2 独有**：只有词语 2 有的搭配词
   - 只显示词语 2 的频率和得分
   - 使用红色高亮显示

### 排序功能

- **按得分差异排序**：默认按得分差异从高到低排序（词语 1 更强 → 词语 2 更强）
- **按频率排序**：可以点击表格列标题进行排序

### 快捷操作

与 Word Sketch 模式相同，每个搭配词都提供快捷操作菜单。

## 可视化

可视化面板提供网络图可视化，帮助您直观地理解词语的语法搭配关系。

### 网络图

#### 图表特点

- **节点类型**：
  - **中心节点**（蓝色）：目标词语（Word Sketch）或两个对比词语（Word Sketch Difference）
  - **关系节点**（橙色）：语法关系类型
  - **搭配词节点**（绿色）：搭配词

- **连线**：
  - 中心节点 → 关系节点：表示词语具有该语法关系
  - 关系节点 → 搭配词节点：表示该关系下的搭配词
  - 连线粗细表示频率或强度

#### 配置选项

- **关系筛选**：选择要显示的关系（「全部」或特定关系）
- **每个关系最大词语数**：设置每个关系显示的搭配词数量（默认：10，范围：5-30）

#### 交互功能

- **拖拽节点**：可以拖拽节点调整位置
- **缩放**：使用鼠标滚轮缩放视图
- **点击节点**：点击节点可查看详细信息
- **图例**：显示节点类型和颜色对应关系

#### Word Sketch Difference 特殊显示

在对比模式下：

- **中心节点**：显示两个对比词语
- **颜色编码**：搭配词节点根据得分差异使用不同颜色
  - 蓝色：词语 1 的搭配更强
  - 红色：词语 2 的搭配更强
  - 灰色：搭配强度相近

### 导出功能

网络图支持导出：

- **导出 SVG**：导出为矢量图格式，适合打印和编辑
- **导出 PNG**：导出为位图格式，适合插入文档

导出按钮位于图表设置栏右侧。

## logDice 得分

### 概述

logDice 是一种衡量词语搭配强度的统计方法，基于 Dice 系数和频率信息计算。

### 得分范围

- **范围**：通常为 0-14
- **高分**（> 10）：非常强的搭配，词语经常一起出现
- **中分**（5-10）：较强的搭配
- **低分**（< 5）：较弱的搭配，可能是偶然出现

### 得分解释

- **得分越高**：表示两个词语的搭配越强，越可能是固定搭配或习惯用法
- **得分越低**：表示两个词语的搭配越弱，可能是偶然出现

### 使用建议

- 设置「最小得分」为 3-5，可以过滤掉弱搭配，只关注强搭配
- 在对比分析中，得分差异大于 2 通常表示有意义的差异

## 使用技巧

### 高效分析流程

1. **选择语料库**：根据研究目标选择合适语料库
2. **筛选文本**：使用标签或手动选择，聚焦目标文本
3. **输入词语**：输入要分析的词语
4. **选择词性**：如果词语有多种词性，选择合适的词性
5. **设置参数**：调整最小频率和最小得分，平衡结果数量和相关性
6. **运行分析**：点击分析按钮
7. **查看结果**：在关系卡片中查看语法搭配
8. **深入分析**：点击搭配词进行进一步分析
9. **可视化**：使用网络图直观理解搭配关系

### Word Sketch 使用建议

#### 分析单个词语的语法行为
1. 选择「Word Sketch」标签页
2. 输入目标词语
3. 选择合适的词性
4. 运行分析
5. 查看各个语法关系下的搭配词
6. 关注得分高的强搭配

#### 发现固定搭配和习惯用法
1. 设置较高的最小得分（如 5-7）
2. 查看得分高的搭配词
3. 这些通常是固定搭配或习惯用法

#### 分析词语的语法功能
1. 查看不同语法关系下的搭配词
2. 理解词语在不同语法位置的使用
3. 发现词语的典型语法模式

### Word Sketch Difference 使用建议

#### 对比同义词的用法差异
1. 选择「Word Sketch Difference」标签页
2. 输入两个同义词
3. 选择相同的词性
4. 运行对比分析
5. 查看共有搭配和独有搭配
6. 通过得分差异理解用法差异

#### 对比不同词性的同一词根
1. 输入同一词根的不同词性形式
2. 分别选择对应的词性
3. 运行对比分析
4. 理解词性转换对搭配的影响

#### 发现词语的语义差异
1. 对比两个相关词语
2. 查看独有搭配，理解词语的独特用法
3. 查看共有搭配的得分差异，理解用法偏好

### 常见分析场景

#### 分析动词的典型搭配
1. 选择「Word Sketch」模式
2. 输入目标动词
3. 选择「动词」词性
4. 运行分析
5. 查看「直接宾语」、「主语」等关系下的搭配词

#### 分析名词的修饰语
1. 选择「Word Sketch」模式
2. 输入目标名词
3. 选择「名词」词性
4. 运行分析
5. 查看「修饰语」关系下的形容词搭配

#### 对比近义词的用法
1. 选择「Word Sketch Difference」模式
2. 输入两个近义词（如 "big" 和 "large"）
3. 选择相同的词性
4. 运行对比分析
5. 查看独有搭配和得分差异

#### 分析词语的语法变化
1. 选择「Word Sketch Difference」模式
2. 输入同一词语的不同形式（如 "go" 和 "went"）
3. 选择「动词」词性
4. 运行对比分析
5. 理解不同形式的搭配差异

### 注意事项

- 词图分析基于 SpaCy 依存句法标注数据，确保文本已完成 SpaCy 标注
- 大语料库分析可能需要较长时间，请耐心等待
- 如果词语在语料库中出现次数很少，可能无法提取足够的语法关系
- logDice 得分基于统计计算，高分不一定表示语义相关性
- 语法关系的识别依赖于 SpaCy 的依存句法分析准确性
- 某些语法关系可能因为标注错误而无法正确识别
- 对比分析时，确保两个词语具有相同的词性，否则对比结果可能不准确
- 颜色编码仅供参考，具体解释需要结合语言学知识
- 导出图表时，如果节点数量很多，可能需要一些时间

# 文献可视化

## 概述

文献可视化模块用于管理和分析学术文献数据，支持从 Web of Science (WOS) 和中国知网 (CNKI) 导入 Refworks 格式的文献数据，并提供多种可视化分析功能，包括合作网络、关键词共现、时区视图和突增检测等。

## 界面布局

文献可视化模块采用顶部标签页设计：

- **上传**：创建文献库和上传 Refworks 文件
- **文献库列表**：查看和管理所有文献库
- **文献库详情**：查看文献条目列表、筛选和详情
- **可视化**：生成各种可视化图表

## 文献库管理

### 创建文献库

在「上传」标签页中创建新的文献库：

1. **文献库名称**：输入文献库的名称（必填）
2. **数据源类型**：选择数据源类型
   - **Web of Science (WOS)**：适用于从 Web of Science 导出的 Refworks 文件
   - **CNKI（中国知网）**：适用于从中国知网导出的 Refworks 文件
3. **描述**：可选，添加文献库的描述信息
4. 点击「创建」按钮创建文献库

**注意事项**：
- 文献库名称不能为空
- 数据源类型一旦创建后不能修改，请确保选择正确
- 一个文献库只能包含一种数据源类型的文献

### 查看文献库列表

在「文献库列表」标签页中：

- **卡片视图**：每个文献库显示为一个卡片
- **统计信息**：
  - 文献条目数量
  - 创建日期
  - 数据源类型标签
- **操作**：
  - **查看**：点击「查看」按钮进入文献库详情
  - **删除**：点击删除图标删除文献库（会删除所有文献条目）

### 文献库详情

在「文献库详情」标签页中：

- **头部信息**：
  - 文献库名称
  - 数据源类型标签
  - 文献总数
  - 年份范围（如果有）
  - 「添加更多」按钮：上传更多文献到该文献库

- **筛选面板**：提供多种筛选条件（见下文）

- **文献条目表格**：
  - **列**：标题、作者、年份、期刊、引用次数、操作
  - **分页**：支持分页浏览（10/25/50/100 条每页）
  - **操作**：
    - **查看详情**：点击行或查看按钮查看文献详情
    - **删除**：删除单个文献条目

## 上传文献

### 上传流程

1. **选择文献库**：在「上传」标签页中，确保已选择或创建了文献库
2. **选择文件**：
   - 拖拽文件到上传区域，或
   - 点击上传区域选择文件
3. **文件要求**：
   - 文件格式：Refworks (.txt)
   - 文件大小：建议不超过 50MB
4. **上传**：点击「上传」按钮开始上传
5. **查看结果**：上传完成后会显示：
   - 成功添加的文献数量
   - 跳过的文献数量（如果有）
   - 解析错误（如果有）

### 文件格式

系统支持两种 Refworks 格式：

#### Web of Science (WOS) 格式

WOS 导出的 Refworks 文件通常包含以下字段：
- PT（文献类型）
- AU（作者）
- AF（作者全名）
- TI（标题）
- SO（来源/期刊）
- PY（出版年份）
- VL（卷）
- IS（期）
- BP（起始页码）
- EP（结束页码）
- DI（DOI）
- TC（引用次数）
- 等等

#### CNKI 格式

CNKI 导出的 Refworks 文件通常包含以下字段：
- 标题
- 作者
- 机构
- 关键词
- 摘要
- 期刊
- 年份
- 等等

**注意事项**：
- 确保文件格式与创建文献库时选择的数据源类型匹配
- 如果格式不匹配，系统会尝试解析但可能无法正确提取所有信息
- 建议从原始数据源重新导出，确保格式正确

## 筛选功能

在「文献库详情」和「可视化」标签页中，可以使用筛选面板筛选文献：

### 年份范围

使用滑块选择年份范围：

- **最小值**：文献库中最早年份
- **最大值**：文献库中最新年份
- **操作**：拖动滑块两端选择起止年份

### 作者筛选

- **自动完成**：输入作者姓名，系统会显示匹配的作者列表
- **自由输入**：也可以直接输入作者姓名（支持部分匹配）

### 机构筛选

- **自动完成**：输入机构名称，系统会显示匹配的机构列表
- **自由输入**：也可以直接输入机构名称（支持部分匹配）

### 关键词筛选

- **自动完成**：输入关键词，系统会显示匹配的关键词列表
- **自由输入**：也可以直接输入关键词（支持部分匹配）

### 期刊筛选

- **自动完成**：输入期刊名称，系统会显示匹配的期刊列表
- **自由输入**：也可以直接输入期刊名称（支持部分匹配）

### 文献类型筛选

- **下拉选择**：从下拉列表中选择文献类型
- **选项**：根据文献库中的实际文献类型显示

### 国家筛选

- **自动完成**：输入国家名称，系统会显示匹配的国家列表
- **自由输入**：也可以直接输入国家名称（支持部分匹配）

### 清除筛选

- 点击「清除筛选」按钮可以清除所有筛选条件
- 筛选面板会显示「活动筛选」标签，提示当前有筛选条件

## 可视化分析

在「可视化」标签页中，可以生成多种可视化图表：

### 图表类型

系统提供三种主要的图表类型：

1. **网络图（Network）**：合作网络和共现网络
2. **时区视图（Timezone）**：按时间段展示文献分布
3. **突增检测（Burst）**：检测关键词或作者的突增趋势

### 网络图

网络图用于展示合作关系和共现关系，使用力导向图布局。

#### 网络类型

可以选择四种网络类型：

1. **关键词共现网络（Keyword Co-occurrence）**：
   - 展示关键词之间的共现关系
   - 节点：关键词
   - 连线：关键词在同一篇文献中出现
   - 节点大小：关键词出现频率
   - 连线粗细：共现强度

2. **作者合作网络（Co-author Network）**：
   - 展示作者之间的合作关系
   - 节点：作者
   - 连线：作者共同发表文献
   - 节点大小：作者发表文献数量
   - 连线粗细：合作强度

3. **机构合作网络（Co-institution Network）**：
   - 展示机构之间的合作关系
   - 节点：机构
   - 连线：机构共同发表文献
   - 节点大小：机构发表文献数量
   - 连线粗细：合作强度

4. **国家合作网络（Co-country Network）**：
   - 展示国家之间的合作关系
   - 节点：国家
   - 连线：国家共同发表文献
   - 节点大小：国家发表文献数量
   - 连线粗细：合作强度

#### 配置参数

- **最小权重（Min Weight）**：
  - 范围：1-10（默认：1）
  - 只显示权重大于等于此值的连线
  - 用于过滤弱关系，突出强关系

- **最大节点数（Max Nodes）**：
  - 范围：10-300（默认：100）
  - 限制显示的节点数量
  - 系统会优先显示权重高的节点

- **颜色方案（Color Scheme）**：
  - 选择图表的配色方案
  - 选项：蓝色、绿色、紫色、橙色、红色、青色

#### 交互功能

- **拖拽节点**：可以拖拽节点调整位置
- **缩放**：使用鼠标滚轮缩放视图
- **平移**：按住鼠标左键拖动平移视图
- **悬停提示**：鼠标悬停在节点或连线上显示详细信息
- **节点信息**：
  - 节点名称
  - 节点权重（频率或数量）
  - 连接的节点数量

### 时区视图

时区视图按时间段展示文献分布，帮助理解研究主题的时间演变。

#### 配置参数

- **Top N**：
  - 范围：5-50（默认：10）
  - 每个时间段显示前 N 个关键词或作者
  - 用于控制显示的条目数量

- **时间切片（Time Slice）**：
  - 默认：1 年
  - 可以调整时间切片的长度
  - 较长的切片可以平滑数据，较短的切片可以显示更细粒度的时间变化

- **颜色方案**：选择图表的配色方案

#### 图表特点

- **垂直布局**：每个时间段显示为一行
- **颜色编码**：使用颜色深浅表示频率或重要性
- **悬停提示**：鼠标悬停显示详细信息

### 突增检测

突增检测用于识别关键词或作者在特定时间段内的突增趋势，类似于 CiteSpace 的突增检测功能。

#### 突增类型

可以选择两种突增类型：

1. **关键词突增（Keyword Burst）**：
   - 检测关键词在特定时间段内的突增
   - 帮助识别研究热点和趋势变化

2. **作者突增（Author Burst）**：
   - 检测作者在特定时间段内的突增
   - 帮助识别活跃的研究者

#### 配置参数

- **突增类型**：选择「关键词」或「作者」
- **颜色方案**：选择图表的配色方案

#### 图表特点

- **甘特图风格**：使用甘特图展示突增时间段
- **灰色背景**：非突增时间段显示为灰色
- **彩色条**：突增时间段显示为彩色条
- **合并显示**：同一术语的多个突增时间段合并为一行
- **悬停提示**：鼠标悬停显示突增详细信息：
  - 突增开始时间
  - 突增结束时间
  - 突增强度
  - 突增期间的频率

#### 突增强度

突增强度表示突增的显著性：
- **高强度**：表示非常显著的突增
- **中强度**：表示中等程度的突增
- **低强度**：表示轻微的突增

### 筛选面板

在可视化页面中，筛选面板位于图表上方：

- **展开/收起**：点击筛选面板头部可以展开或收起筛选选项
- **活动筛选**：如果有筛选条件，会显示「活动筛选」标签
- **筛选条件**：与文献库详情页面的筛选功能相同

**注意事项**：
- 筛选条件会影响可视化结果
- 修改筛选条件后，图表会自动更新
- 某些筛选条件可能导致没有数据，图表会显示空状态

## 导出功能

所有可视化图表都支持导出：

### 导出 SVG

- **格式**：SVG（矢量图）
- **优点**：可缩放，适合打印和编辑
- **用途**：插入论文、报告等文档

### 导出 PNG

- **格式**：PNG（位图）
- **优点**：兼容性好，适合网页展示
- **用途**：插入演示文稿、网页等

### 导出操作

1. 点击图表设置栏右侧的导出按钮
2. 选择导出格式（SVG 或 PNG）
3. 文件会自动下载

**注意事项**：
- 导出 SVG 时，如果图表很复杂，文件可能较大
- 导出 PNG 时，系统会使用高分辨率（2x）确保清晰度
- 导出大图表时可能需要一些时间

## 使用技巧

### 高效分析流程

1. **创建文献库**：根据数据源类型创建文献库
2. **上传文献**：上传 Refworks 格式的文献文件
3. **查看详情**：在文献库详情中查看和筛选文献
4. **生成可视化**：切换到可视化标签页生成图表
5. **调整参数**：根据需求调整可视化参数
6. **应用筛选**：使用筛选面板聚焦特定文献
7. **导出结果**：导出图表用于报告或论文

### 网络图使用建议

#### 分析合作关系
1. 选择「作者合作网络」或「机构合作网络」
2. 调整「最小权重」过滤弱关系
3. 调整「最大节点数」控制显示数量
4. 观察网络结构，识别核心研究者和机构

#### 分析研究主题
1. 选择「关键词共现网络」
2. 调整参数突出强关系
3. 观察关键词聚类，识别研究主题
4. 结合时区视图了解主题演变

#### 分析国际合作
1. 选择「国家合作网络」
2. 观察国家之间的合作模式
3. 识别国际合作热点地区

### 时区视图使用建议

#### 了解研究趋势
1. 调整「Top N」控制显示的条目数量
2. 观察不同时间段的关键词或作者变化
3. 识别研究热点的兴起和衰落

#### 发现新兴主题
1. 关注最近时间段出现的新关键词
2. 结合突增检测确认新兴主题
3. 分析新兴主题的发展轨迹

### 突增检测使用建议

#### 识别研究热点
1. 选择「关键词突增」
2. 查看突增强度高的关键词
3. 分析突增时间段，了解热点持续时间
4. 结合时区视图了解热点演变

#### 发现活跃研究者
1. 选择「作者突增」
2. 查看突增强度高的作者
3. 分析突增时间段，了解研究活跃期
4. 结合合作网络了解研究团队

### 常见分析场景

#### 文献综述准备
1. 上传相关领域的文献
2. 使用关键词共现网络识别主要研究主题
3. 使用时区视图了解研究发展历程
4. 使用突增检测识别最新研究热点
5. 导出图表用于文献综述

#### 研究团队分析
1. 上传团队成员的文献
2. 使用作者合作网络分析合作关系
3. 使用机构合作网络分析机构合作
4. 使用时区视图了解团队研究历程
5. 使用突增检测识别团队研究热点

#### 领域趋势分析
1. 上传特定领域的文献
2. 使用关键词共现网络识别研究主题
3. 使用时区视图分析主题演变
4. 使用突增检测识别新兴趋势
5. 结合筛选功能聚焦特定时间段或主题

### 注意事项

- 文献可视化基于上传的文献数据，确保数据完整和准确
- 大文献库的可视化可能需要较长时间，请耐心等待
- 网络图在节点数量很多时可能显示较慢，建议调整「最大节点数」
- 筛选条件会影响可视化结果，确保筛选条件正确
- 导出图表时，如果图表很复杂，可能需要一些时间
- Refworks 文件格式必须正确，否则可能无法正确解析
- 不同数据源（WOS/CNKI）的字段可能不同，某些可视化可能不适用于所有数据源
- 突增检测算法基于统计方法，结果仅供参考
- 网络图的布局是自动计算的，每次刷新可能略有不同
- 时区视图的时间切片设置会影响显示的粒度，建议根据数据量调整

# 标注模式

## 概述

标注模式模块用于对文本和多媒体内容进行基于框架的标注。该模块支持两种标注模式：**纯文本标注**和**多模态标注**（视频/音频），帮助研究者系统化地标注和分析语料数据。

## 界面布局

标注模式模块采用顶部标签页设计：

- **纯文本标注**：对纯文本进行标注
- **多模态标注**：对视频和音频进行标注
- **标注历史**：查看和管理已保存的标注存档
- **框架管理**：创建和管理标注框架
- **编码者间信度**：计算多个编码者之间的标注一致性

## 纯文本标注

### 界面布局

纯文本标注采用左右分栏布局：

- **左侧面板**（可调整宽度，默认 400px）：
  - **框架设置**：选择标注框架
  - **框架树可视化**：显示框架的层级结构，点击标签选中用于标注
  - **已选标签显示**：显示当前选中的标签及其定义
  - **语料库选择**：选择语料库和文本文件
  - **存档管理**：查看、加载、重命名、删除标注存档

- **右侧面板**（弹性宽度）：
  - **工具栏**：导出、句法可视化、保存等功能
  - **文本标注区**：分句显示文本，支持划词标注
  - **标注表格**：显示所有标注的详细信息

### 框架选择

1. **选择框架**：
   - 在「框架设置」下拉菜单中选择要使用的标注框架
   - 框架按类别组织，显示框架名称和所属类别
   - 点击「刷新框架」按钮可以重新加载框架列表

2. **框架树可视化**：
   - 框架树以层级结构显示所有标签
   - 点击标签可以选中该标签用于标注
   - 选中的标签会高亮显示
   - 标签颜色用于区分不同的标签类型

3. **已选标签**：
   - 选中标签后，会在左侧面板中间显示标签信息
   - 显示标签名称和定义（如果有）
   - 标签颜色会应用到标注中

### 语料库和文本选择

1. **选择语料库**：
   - 在「语料库选择」下拉菜单中选择语料库
   - 只有包含文本文件的语料库会显示文本列表

2. **选择文本**：
   - 在文本列表中选择要标注的文本文件
   - 点击文本文件名即可加载文本内容
   - 系统会自动加载 SpaCy 标注数据（如果有）

### 文本标注

#### 分句显示

文本按句子分割，每句单行显示：

- **自动分句**：使用 SpaCy 分句结果（如果可用），否则使用正则表达式分句
- **横向滚动**：长句子会横向滚动，不会换行
- **整体滚动**：整个文本容器可以垂直滚动

#### 划词标注

1. **选择文本**：
   - 在文本中拖动鼠标选择要标注的文本
   - 选中的文本会高亮显示

2. **创建标注**：
   - 选择文本后，如果已选中标签，会自动创建标注
   - 如果未选中标签，会显示提示信息
   - 标注会以标签块的形式显示在选中文本的正下方

3. **标签块显示**：
   - **精确对齐**：标签块的左右边界与划词边界精确对齐
   - **颜色编码**：标签块使用标签的颜色
   - **层叠显示**：如果有多个标注重叠，大标签在上层（靠近文本），小标签在下层
   - **点击删除**：点击标签块可以删除该标注

#### 交叉检测

系统禁止交叉标注，只允许完全包含或不重叠：

- **完全包含**：新标注完全包含在已有标注内，或已有标注完全包含在新标注内
- **不重叠**：新标注与已有标注完全不重叠
- **交叉重叠**：如果新标注与已有标注交叉（部分重叠但不完全包含），系统会拒绝并显示警告

#### SpaCy 预标注

如果文本已完成 SpaCy 标注，系统会自动显示：

- **词性标注（POS）**：可以切换显示/隐藏
- **命名实体（NER）**：可以切换显示/隐藏
- **预标注信息**：在标注表格中显示 POS 和 NER 信息

### 标注表格

标注表格显示所有标注的详细信息：

- **列**：
  - **文本**：标注的文本内容
  - **标签**：标注使用的标签名称
  - **位置**：标注在文本中的起始和结束位置
  - **词性（POS）**：标注文本的词性（如果可用）
  - **命名实体（NER）**：标注文本的命名实体（如果可用）
  - **备注**：标注的备注信息（可以编辑）
  - **操作**：删除按钮

- **功能**：
  - **高亮显示**：鼠标悬停在表格行上时，对应的标注会在文本中高亮
  - **编辑备注**：点击备注列可以编辑备注信息
  - **删除标注**：点击删除按钮可以删除标注

### 句法结构可视化

系统支持查看句子的句法结构：

1. **打开句法可视化**：
   - 点击工具栏的「查看句法结构」按钮
   - 只有在文本已完成 SpaCy 标注时才能使用

2. **句法类型**：
   - **成分句法（Constituency）**：使用 benepar 模型生成句法树（D3.js 可视化）
   - **依存句法（Dependency）**：使用 SpaCy displacy 渲染依存关系图

3. **切换句子**：
   - 使用上下箭头控件切换不同的句子
   - 显示当前句子在文本中的位置

### 自动标注

系统支持根据特定框架自动生成标注，减少手动标注的工作量。自动标注按钮位于「查看句法结构」按钮右侧（绿色图标）。

#### 支持的框架

自动标注功能仅在以下框架加载时可用：

| 框架 | 自动标注类型 | 说明 |
|------|-------------|------|
| **MIPVU** | 隐喻词标注 | 自动标注 `indirect`（间接隐喻）标签 |
| **Halliday-Theme** | 主题/述题标注 | 自动标注 `theme` 和 `rheme` |
| **Berry-Theme** | 主题/述题标注 | 自动标注 `theme` 和 `rheme` |

其他框架加载时，自动标注按钮会被禁用（灰色显示）。

#### MIPVU 自动标注

当加载 MIPVU 框架时，点击自动标注按钮将：

1. **读取 MIPVU 标注数据**：从语料库管理中获取该文本的 MIPVU 隐喻检测结果
2. **识别隐喻词**：找出所有 `is_metaphor=true` 的词语
3. **创建标注**：为每个隐喻词创建 `indirect`（间接隐喻）标签

**前提条件**：
- 文本必须已完成 MIPVU 隐喻检测（在语料库管理中进行）
- 如果没有 MIPVU 数据，会提示「MIPVU 标注数据不存在」

#### Theme/Rheme 自动标注

当加载 Halliday-Theme 或 Berry-Theme 框架时，点击自动标注按钮将：

1. **分析句子结构**：利用 SpaCy 的依存句法分析
2. **识别主题（Theme）**：从句首到第一个概念主题（通常是主语）的部分
3. **识别述题（Rheme）**：主题之后到句末的部分
4. **创建标注**：为每个句子创建 `theme` 和 `rheme` 标签

**理论基础**（系统功能语言学 SFL）：
- **主题（Theme）**：小句的出发点，包含文本主题（连接词）、人际主题（情态附加语）和概念主题（通常是主语）
- **述题（Rheme）**：主题之后的部分，包含句子的主要信息

**标注特点**：
- 段落级别标注，不是逐词标注（如「The Duke」整体标注为 theme）
- 自动跳过已有相同位置和标签的标注，避免重复

**前提条件**：
- 文本必须已完成 SpaCy 标注
- 如果没有 SpaCy 数据，会提示「SpaCy 标注数据不存在」

#### 使用步骤

1. **选择文本**：在左侧面板选择要标注的文本
2. **选择框架**：选择 MIPVU、Halliday-Theme 或 Berry-Theme 框架
3. **点击自动标注**：点击工具栏的绿色自动标注按钮
4. **查看结果**：系统会显示成功添加的标注数量
5. **手动调整**：可以继续手动添加、修改或删除标注
6. **保存**：点击「保存」按钮保存标注存档

### 保存标注

1. **打开保存对话框**：
   - 点击工具栏的「保存」按钮
   - 只有在选择了框架且有标注时才能保存

2. **输入编码者名称**：
   - 编码者名称是可选的
   - 用于编码者间信度分析时识别不同编码者
   - 如果已有存档，会使用已保存的编码者名称

3. **保存**：
   - 点击「保存」按钮保存标注
   - 系统会创建或更新标注存档
   - 保存成功后会显示成功消息

### 存档管理

在左侧面板的「存档」部分：

- **查看存档**：显示当前语料库的所有标注存档
- **加载存档**：点击存档名称可以加载该存档
- **重命名存档**：点击编辑图标可以重命名存档
- **删除存档**：点击删除图标可以删除存档

存档信息包括：
- 文本名称
- 标注数量
- 保存时间戳

### 导出功能

系统支持导出标注结果为图片：

1. **导出 PNG**：
   - 点击工具栏的「导出 PNG」按钮
   - 导出为 PNG 位图格式
   - 适合插入文档和演示文稿

2. **导出 SVG**：
   - 点击工具栏的「导出 SVG」按钮
   - 导出为 SVG 矢量图格式
   - 适合打印和编辑

**导出特点**：
- 自适应背景，确保在不同背景下都能清晰显示
- 自动添加边距，确保边缘不被裁剪
- 长句完整显示，不会截断

## 多模态标注

### 界面布局

多模态标注采用左右分栏布局，与纯文本标注类似：

- **左侧面板**：框架选择、框架树、语料库选择、存档管理
- **右侧面板**：
  - **媒体播放器**：视频或音频播放器
  - **转录文本标注区**：显示转录文本，支持划词标注
  - **多轨时间轴**：显示 YOLO 追踪、转录段、用户标注、关键帧

### 媒体选择

1. **选择语料库**：选择包含视频或音频文件的语料库
2. **选择媒体**：在媒体列表中选择要标注的视频或音频文件
3. **自动加载**：选择媒体后，系统会自动加载：
   - 媒体文件路径
   - 转录文本（如果有）
   - YOLO 检测结果（如果有）
   - CLIP 分类结果（如果有）

### 视频标注

#### 视频播放控制

- **播放/暂停**：控制视频播放
- **时间跳转**：-5s/+5s 按钮快速跳转
- **帧控制**：上一帧/下一帧按钮精确控制
- **时间轴**：拖动时间轴可以跳转到指定位置

#### 画框模式

1. **启用画框模式**：
   - 点击「画框模式」按钮启用
   - 在视频帧上可以绘制边界框

2. **绘制边界框**：
   - 在视频帧上拖动鼠标绘制边界框
   - 边界框使用视频原始坐标，确保准确性
   - 可以调整边界框的大小和位置

3. **帧追踪系统**：
   - **帧间隔设置**：设置追踪的帧间隔
   - **追踪到上/下一帧**：将当前框选应用到上一帧或下一帧
   - **确认框选**：确认当前帧的框选
   - **保存序列**：保存多帧的框选序列

4. **关键帧插值**：
   - 设置多个关键帧的框选
   - 系统会自动线性插值生成中间帧的框选
   - 减少手动标注的工作量

#### YOLO 叠加显示

如果视频已完成 YOLO 检测：

- **实时显示**：在视频播放时实时显示 YOLO 检测框
- **坐标映射**：使用视频原始坐标，确保检测框位置准确
- **多对象追踪**：显示多个对象的追踪轨迹

### 音频标注

> **重要**：音频波形标注功能仅支持英语音频。中文音频只能在纯文本标注模式下进行标注。

#### 英语音频波形标注

对于有强制对齐数据的英语音频，系统会显示交互式波形界面：

**波形可视化 (Wavesurfer.js)**
- **波形显示**：显示完整音频波形，支持缩放和滚动
- **词级对齐**：波形上方显示每个单词的时间位置标签
- **音高曲线**：可选显示 F0（基频）曲线叠加层
- **缩放控制**：按钮或 Ctrl+滚轮缩放，以播放指针为中心

**播放控制**
- **播放/暂停**：控制音频播放
- **时间跳转**：-5s/+5s 按钮快速跳转
- **点击跳转**：点击波形任意位置跳转到该时间点
- **DAW 模式**：播放指针保持在视图中央，波形滚动

**画框标注**
- **画框模式**：点击「画框」按钮启用画框模式
- **绘制区域**：在波形上拖拽绘制标注框
- **标签显示**：画框显示当前选中的标签名称和颜色
- **时间记录**：画框自动记录开始和结束时间
- **模式切换**：关闭画框模式后可点击波形跳转播放指针

> **提示**：画框标注与转录文本划词标注是独立的两套系统，互不影响。

#### 转录文本标注

- **转录显示**：显示音频的转录文本（如果有）
- **划词标注**：与纯文本标注相同，支持划词标注
- **时间对齐**：标注与音频时间对齐
- **当前句高亮**：播放时自动高亮当前正在播放的句子
- **点击跳转**：点击转录文本中的句子可跳转到对应时间

#### 中文音频限制

由于 Wav2Vec2 强制对齐模型仅支持英语，中文音频：
- 不显示波形可视化界面
- 不支持画框标注功能
- 只能通过纯文本标注模式对转录文本进行标注
- 在多模态标注界面中不显示中文音频

### 多轨时间轴

多轨时间轴显示多种信息：

1. **YOLO 追踪轨**：
   - 显示 YOLO 检测到的对象追踪
   - 不同对象使用不同颜色

2. **转录段轨**：
   - 显示转录文本的时间段
   - 每个段显示对应的文本内容

3. **用户标注轨**：
   - 显示用户创建的标注
   - 标注按时间顺序排列

4. **关键帧标记**：
   - 标记关键帧位置
   - 用于帧追踪和插值

#### DAW 范式时间轴

时间轴采用 DAW（数字音频工作站）范式的播放针行为：

- **开始移动**：播放针从当前位置开始移动
- **居中滚动**：播放针移动到视图中心时，时间轴开始滚动
- **结尾移动**：播放针移动到时间轴末尾时停止

### 保存和导出

多模态标注的保存和导出功能与纯文本标注相同：

- **保存标注**：保存视频/音频标注存档
- **导出功能**：导出标注结果为图片（如果适用）

## 标注历史

标注历史页面显示所有已保存的标注存档：

- **存档列表**：按语料库组织显示所有存档
- **存档信息**：
  - 文本/媒体名称
  - 标注数量
  - 编码者名称
  - 保存时间
  - 使用的框架

- **操作**：
  - **查看详情**：查看标注的详细信息
  - **删除**：删除存档

## 框架管理

框架管理页面用于创建和管理标注框架：

- **框架列表**：显示所有框架，按类别组织
- **创建框架**：创建新的标注框架
- **编辑框架**：编辑现有框架的结构和标签
- **删除框架**：删除不需要的框架
- **框架可视化**：使用 D3.js 树可视化显示框架结构

## 编码者间信度

编码者间信度（Inter-Coder Reliability）模块用于计算多个编码者之间的标注一致性，是内容分析和质性研究中评估标注质量的重要工具。

> **重要限制**：编码者间信度分析仅支持**纯文本标注存档**。视频标注和音频标注存档不支持信度计算。

### 界面布局

编码者间信度模块采用多面板布局：

- **数据源面板**：选择标注存档或上传标注文件
- **计算面板**：选择要计算的信度系数和参数
- **结果面板**：显示计算结果和详细统计

### 数据来源

#### 从语料库选取存档

1. 选择目标语料库
2. 选择要分析的标注存档（可多选）
3. 系统会自动加载存档内容

> **注意**：语料库存档列表已自动过滤，仅显示纯文本标注存档。视频和音频标注存档不会出现在列表中。

#### 上传本地文件

1. 点击上传区域选择 JSON 格式的标注文件
2. 支持拖拽上传多个文件
3. 系统会验证文件格式和内容一致性

**文件验证规则**：
- 必须是有效的 JSON 格式
- 必须包含 `annotations` 数组
- 不支持视频/音频标注存档（包含 `yoloAnnotations`、`videoBoxes`、`audioBoxes` 或 `mediaType` 为 `video`/`audio` 的存档将被拒绝）
- 上传不合规文件时会显示具体错误信息

### 索引-标签矩阵计算

本软件采用基于字符索引的二值矩阵方法计算信度系数。

#### 矩阵构建原理

对于每个编码者，系统会构建一个 `文本长度 × 标签数` 的二值矩阵：

- **行**：文本中的每个字符位置（索引）
- **列**：标注框架中的所有标签
- **值**：如果某个标签覆盖了该字符位置，则为 1，否则为 0

#### 处理重叠标注

该方法自然支持重叠标注的情况：

- **同一位置多标签**：同一字符位置可以同时被多个标签标注（矩阵中该行有多个 1）
- **嵌套标注**：大范围标签包含小范围标签时，两者在对应位置都会记为 1
- **不同编码者不同标签**：不同编码者在同一位置使用不同标签时，各自的矩阵会反映其选择

#### 计算单位

信度计算以「字符索引-标签」对作为基本单位：

- **决策数（N Decisions）**：编码者数 × 文本长度 × 标签数
- **一致**：所有编码者在某个「字符索引-标签」对上的值相同（都为 1 或都为 0）
- **不一致**：编码者在某个「字符索引-标签」对上的值不同

### 支持的信度系数

#### 1. 平均配对百分比一致（Average Pairwise Percent Agreement）

**适用场景**：快速评估两个或多个编码者的整体一致程度

**计算原理**：

对于每对编码者 i 和 j：

$$\text{Percent Agreement}(i,j) = \frac{\text{一致的位置数}}{\text{总位置数}}$$

其中：
- **一致的位置数**：两个编码者的矩阵在相同位置值相同的数量
- **总位置数**：文本长度 × 标签数

**多编码者情况**：

$$\text{Average Pairwise Percent Agreement} = \frac{\sum(\text{所有配对的一致率})}{\text{配对数}}$$

**结果解释**：
- 值域：0 到 1（通常显示为百分比）
- 0.80 以上：高一致性
- 0.60-0.80：中等一致性
- 0.60 以下：低一致性

**局限性**：不考虑偶然一致的可能性，当类别分布不均匀时可能高估实际一致程度

---

#### 2. Fleiss' Kappa

**适用场景**：评估三个及以上编码者的一致性，是 Cohen's Kappa 的多编码者扩展

**计算原理**：

$$\kappa = \frac{\bar{P} - \bar{P}_e}{1 - \bar{P}_e}$$

其中：

**观察一致性 $\bar{P}$**：

对于每个「字符索引-标签」对 i：

$$P_i = \frac{\sum n_{ij}^2 - n}{n \times (n-1)}$$

- $n_{ij}$：在位置 i 选择类别 j 的编码者数量
- n：编码者总数

然后取平均：

$$\bar{P} = \frac{1}{N} \sum P_i$$

- N：总位置数（字符索引-标签对数）

**期望一致性 $\bar{P}_e$**：

$$\bar{P}_e = \sum p_j^2$$

- $p_j$：类别 j 被选择的整体比例

**结果解释**：
- 值域：-1 到 1
- 0.81-1.00：几乎完美一致（Almost Perfect）
- 0.61-0.80：高度一致（Substantial）
- 0.41-0.60：中等一致（Moderate）
- 0.21-0.40：一般一致（Fair）
- 0.00-0.20：轻微一致（Slight）
- < 0：低于偶然水平

---

#### 3. 平均配对 Cohen's Kappa

**适用场景**：评估两个编码者的一致性，校正偶然一致

**计算原理**：

对于每对编码者：

$$\kappa = \frac{P_o - P_e}{1 - P_e}$$

其中：

**观察一致性 $P_o$**：

$$P_o = \frac{\text{一致的位置数}}{\text{总位置数}}$$

**期望一致性 $P_e$**：

$$P_e = \sum (p_{1c} \times p_{2c})$$

- $p_{1c}$：编码者 1 选择类别 c 的比例
- $p_{2c}$：编码者 2 选择类别 c 的比例

**多编码者情况**：

$$\text{Average Pairwise Cohen's Kappa} = \frac{\sum(\text{所有配对的 Kappa 值})}{\text{配对数}}$$

**结果解释**：与 Fleiss' Kappa 相同的解释标准

---

#### 4. Krippendorff's Alpha

**适用场景**：最通用的信度系数，支持多种测量层次，可处理缺失数据

**基本原理**：

$$\alpha = 1 - \frac{D_o}{D_e}$$

其中：
- $D_o$：观察到的不一致程度
- $D_e$：偶然情况下期望的不一致程度

**一致性矩阵（Coincidence Matrix）计算**：

系统首先构建一个一致性矩阵 O，其中：
- o_ck：类别 c 和类别 k 共同出现的次数（加权计数）

对于每个单元（字符位置），如果有 m 个编码者做出了标注：
- 每对值 (c, k) 贡献 1/(m-1) 到一致性矩阵

**关键统计量**：

- **Σc·o_cc（观察一致性）**：一致性矩阵对角线元素之和，表示编码者选择相同类别的程度
- **Σc·n_c(n_c-1)（期望一致性基数）**：基于边缘频率计算的期望值

**差异函数 δ（根据测量层次）**：

Krippendorff's Alpha 支持四种测量层次，每种使用不同的差异函数：

**名义层次（Nominal）**：

$$\delta^2(c, k) = \begin{cases} 0 & \text{if } c = k \\ 1 & \text{if } c \neq k \end{cases}$$

适用于：无序分类数据（如标签类型、性别等）

**顺序层次（Ordinal）**：

$$\delta^2(c, k) = \left[\sum_{g=c}^{k} n_g - \frac{n_c + n_k}{2}\right]^2$$

- $n_g$：类别 g 的边缘频率
- 求和范围从 c 到 k（含两端）

该函数考虑了中间类别的累积频率，反映了顺序尺度上的距离概念。

适用于：有序分类数据（如 Likert 量表、等级评定等）

**等距层次（Interval）**：

$$\delta^2(c, k) = (c - k)^2$$

适用于：等间隔数值数据（如温度、日期等）

**比率层次（Ratio）**：

$$\delta^2(c, k) = \left[\frac{c - k}{c + k}\right]^2$$

适用于：有绝对零点的数值数据（如频率、距离等）

**最终计算**：

$$D_o = \frac{1}{n} \sum_c \sum_k o_{ck} \times \delta^2(c, k)$$

$$D_e = \frac{1}{n(n-1)} \sum_c \sum_k n_c \times n_k \times \delta^2(c, k)$$

$$\alpha = 1 - \frac{D_o}{D_e}$$

**结果解释**：
- 值域：-1 到 1
- α ≥ 0.80：可接受的信度水平
- 0.67 ≤ α < 0.80：可以谨慎使用
- α < 0.67：信度不足

**参考文献**：Krippendorff, K. (2004). Content Analysis: An Introduction to Its Methodology. Sage Publications.

---

#### 5. 召回率与精确率（Recall & Precision）

**适用场景**：当您有一个"标准答案"（黄金标准）时，用于评估其他编码者相对于该标准的准确性

**前提条件**：需要在数据源面板中选择一个存档作为"标准答案/黄金标准"

**计算原理**：

基于标准答案计算每个编码者的标注准确性：

**召回率（Recall）**：

$$\text{Recall} = \frac{\text{正确标注数}}{\text{标准答案标注总数}}$$

- **正确标注数（True Positives）**：编码者标注与标准答案完全匹配的数量
- **标准答案标注总数**：标准答案中的所有标注数量
- 召回率衡量编码者是否"找全了"标准答案中的所有标注

**精确率（Precision）**：

$$\text{Precision} = \frac{\text{正确标注数}}{\text{编码者标注总数}}$$

- **正确标注数（True Positives）**：编码者标注与标准答案完全匹配的数量
- **编码者标注总数**：编码者创建的所有标注数量
- 精确率衡量编码者的标注是否"标对了"

**F1 分数**：

$$F_1 = \frac{2 \times \text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}}$$

- F1 分数是精确率和召回率的调和平均值
- 综合反映编码者的标注质量

**匹配规则**：
- 标注匹配基于：起始位置、结束位置、标签名称
- 三者完全一致才算匹配（True Positive）

**结果展示**：
- **按编码者**：显示每个编码者的召回率、精确率和 F1 分数
- **按标签**：显示每个标签的召回率、精确率和 F1 分数
- 编码者名称使用存档中保存的编码者名称

**结果解释**：
- 值域：0 到 1（通常显示为百分比）
- 0.90 以上：优秀（Excellent）
- 0.80-0.90：良好（Good）
- 0.70-0.80：中等偏上（Fair）
- 0.60-0.70：中等（Moderate）
- 0.50-0.60：中等偏下（Poor）
- 0.50 以下：较差（Very Poor）

**使用建议**：
- 在培训新编码者时，使用专家标注作为标准答案评估培训效果
- 在建立标注规范时，比较不同编码者与标准答案的差异
- 结合其他信度系数使用，全面评估标注质量

### 标准答案选择

在数据源面板中，当选择了多个标注存档后，可以指定其中一个作为"标准答案/黄金标准"：

1. **选择存档**：首先选择要分析的标注存档（至少 2 个）
2. **指定标准答案**：在存档列表下方，使用单选按钮选择一个存档作为标准答案
3. **启用计算**：在计算面板中，"召回率与精确率"选项会自动启用

**注意事项**：
- 只有选择了标准答案后，才能计算召回率和精确率
- 标准答案应由专家或熟练编码者创建
- 其他编码者的标注将与标准答案进行比较

### 结果面板

#### 数据概览

显示基本统计信息：
- **编码者数量（N Coders）**：参与标注的编码者数量
- **案例数量（N Cases）**：文本长度（字符索引数）
- **决策数量（N Decisions）**：总的「编码者-位置-标签」决策数
- **标签数量**：标注框架中的标签总数

#### 系数卡片

每个计算的系数显示为独立卡片：

- **系数值**：主要的信度数值
- **观察一致性**：实际观察到的一致程度
- **期望一致性**：偶然情况下期望的一致程度
- **配对详情**（仅 Percent Agreement 和 Cohen's Kappa）：每对编码者的详细分数

#### Krippendorff's Alpha 详情

- **测量层次**：显示使用的测量层次（nominal/ordinal/interval/ratio）
- **决策数**：用于计算的总决策数
- **Σc·o_cc**：一致性矩阵统计量（观察一致性）
- **Σc·n_c(n_c-1)**：边缘频率统计量（期望一致性基数）

### 报告导出

#### HTML 报告

- 包含所有计算的系数和详细统计
- 格式化的表格和解释
- 生成时间戳

#### CSV 报告

- 系数值和统计量的表格格式
- 适合进一步的数据分析

### 使用建议

#### 系数选择

- **快速评估**：使用 Percent Agreement
- **两个编码者**：使用 Cohen's Kappa
- **多个编码者**：使用 Fleiss' Kappa
- **严格评估**：使用 Krippendorff's Alpha

#### 测量层次选择（Krippendorff's Alpha）

- **标签分类**：选择 Nominal（名义）
- **等级评定**：选择 Ordinal（顺序）
- **数值评分**：选择 Interval（等距）或 Ratio（比率）

#### 结果解释注意事项

- 不同系数的解释标准可能不同
- 应结合研究背景和标注任务的复杂度解释结果
- 低一致性可能表明标注指南需要修改或编码者需要培训

## 使用技巧

### 高效标注流程

1. **准备框架**：在框架管理中创建或选择标注框架
2. **选择语料库**：选择要标注的语料库
3. **选择文本/媒体**：选择要标注的文本或媒体文件
4. **选择标签**：在框架树中点击标签选中
5. **进行标注**：在文本中划词标注，或在视频中画框标注
6. **查看表格**：在标注表格中查看和管理标注
7. **保存存档**：定期保存标注进度
8. **导出结果**：导出标注结果用于报告或论文

### 纯文本标注技巧

#### 处理长文本
1. 使用分句显示，每句单行，便于查看
2. 使用整体滚动浏览整个文本
3. 使用标注表格快速定位标注位置

#### 处理重叠标注
1. 理解层叠规则：大标签在上，小标签在下
2. 避免交叉标注：只允许完全包含或不重叠
3. 使用标注表格查看所有标注的层级关系

#### 利用 SpaCy 预标注
1. 确保文本已完成 SpaCy 标注
2. 使用 POS 和 NER 信息辅助标注
3. 在标注表格中查看预标注信息

### 多模态标注技巧

#### 视频标注
1. **使用帧追踪**：利用帧追踪系统减少重复标注
2. **关键帧插值**：设置关键帧，让系统自动插值
3. **结合 YOLO**：利用 YOLO 检测结果辅助标注
4. **精确控制**：使用帧控制按钮精确控制视频位置

#### 音频标注
1. **结合转录**：利用转录文本进行标注
2. **时间对齐**：注意标注与音频时间的对齐
3. **波形辅助**：使用波形显示辅助定位

### 常见标注场景

#### 文本情感分析
1. 创建情感分析框架（正面、负面、中性等）
2. 选择文本进行标注
3. 标注文本中的情感表达
4. 保存并导出结果

#### 视频对象检测
1. 选择视频文件
2. 启用画框模式
3. 在关键帧绘制边界框
4. 使用帧追踪应用到其他帧
5. 保存标注结果

#### 多模态内容分析
1. 选择视频或音频文件
2. 结合转录文本和视频/音频进行标注
3. 使用多轨时间轴查看时间对齐
4. 保存多模态标注结果

### 注意事项

- 标注前确保已选择框架，否则无法创建标注
- 纯文本标注禁止交叉标注，只允许完全包含或不重叠
- 多模态标注需要确保媒体文件已正确上传和处理
- 保存标注时会保存编码者名称，用于后续的信度分析
- 导出图片时，如果文本很长，可能需要一些时间
- 句法可视化需要文本已完成 SpaCy 标注
- 视频标注的帧追踪功能可以大大提高标注效率
- 标注存档与语料库绑定，切换语料库会切换存档列表
- 删除存档会永久删除标注数据，请谨慎操作
- 框架一旦创建，修改框架结构可能影响已有标注

# 主题建模

## 概述

主题建模模块提供四种主题建模方法：**BERTopic**、**LDA**、**LSA** 和 **NMF**，帮助研究者从文本语料中自动发现和提取主题。每种方法都有其特点和适用场景，用户可以根据研究需求选择合适的方法。

## 界面布局

主题建模模块采用顶部标签页设计：

- **BERTopic**：基于 SBERT 嵌入和聚类的主题建模
- **LDA**：潜在狄利克雷分配主题建模
- **LSA**：潜在语义分析主题建模
- **NMF**：非负矩阵分解主题建模

每个标签页采用左右分栏布局：

- **左侧面板**（450px）：配置面板，包含语料选择、预处理、参数配置等
- **右侧面板**（弹性宽度）：结果展示区域，包含两个标签页
  - **分析结果**：显示主题卡片、主题表格、统计信息
  - **可视化**：提供多种 D3.js 可视化图表

## BERTopic

### 概述

BERTopic 是一种基于 BERT 嵌入和聚类的主题建模方法，使用 SBERT（Sentence-BERT）模型生成文本嵌入，然后通过降维和聚类算法发现主题。

### 工作流程

BERTopic 的工作流程包括以下步骤：

1. **预处理**：对文本进行分块和预处理
2. **嵌入**：使用 SBERT 模型生成文本嵌入向量
3. **降维**：使用 UMAP 或 PCA 降低嵌入维度
4. **聚类**：使用 HDBSCAN、BIRCH 或 K-Means 进行聚类
5. **主题表示**：使用 c-TF-IDF 或其他方法提取主题关键词
6. **主题命名**：可选使用 Ollama 生成主题名称

### 语料选择

与其他分析模块相同，支持三种文本选择模式：
- **全部文本**：分析语料库中的所有文本
- **按标签筛选**：根据标签筛选文本
- **手动选择**：手动选择特定文本

### 预处理配置

预处理面板用于配置文本分块和预处理选项：

#### 分块配置

- **分块方法**：
  - **按句子**：按句子边界分块
  - **按字符数**：按字符数分块
  - **按 token 数**：按 token 数分块（推荐）

- **最大 token 数**：
  - 范围：100-512（默认：512）
  - 控制每个分块的最大 token 数量
  - 超过限制的句子会被跳过或截断

- **重叠 token 数**：
  - 范围：0-100（默认：0）
  - 设置相邻分块之间的重叠 token 数量
  - 有助于保持上下文连续性

- **最小分块大小**：
  - 范围：1-100（默认：1）
  - 设置分块的最小 token 数量
  - 小于此值的分块会被丢弃

#### 预处理预览

- **预览功能**：点击「预览预处理」按钮可以查看预处理结果
- **预览内容**：
  - 显示前 10 个分块的原始文本和处理后文本
  - 显示 token 数量和词数统计
  - 显示是否使用了 SpaCy 标注

**注意事项**：
- 如果句子超过 token 限制，会显示错误提示
- 如果分块超过 token 限制，建议减少最大 token 数设置
- 预处理预览可以帮助您了解文本处理效果

### 嵌入配置

嵌入面板用于生成和管理文本嵌入：

#### 生成嵌入

1. **点击「生成嵌入」按钮**：
   - 系统会使用 SBERT 模型（paraphrase-multilingual-MiniLM-L12-v2）生成文本嵌入
   - 嵌入过程可能需要一些时间，取决于文本数量

2. **嵌入信息**：
   - **模型名称**：显示使用的 SBERT 模型
   - **嵌入维度**：显示嵌入向量的维度（通常是 384）
   - **文档数量**：显示处理的文档数量

#### 嵌入文件管理

- **嵌入列表**：显示当前语料库的所有嵌入文件
- **嵌入信息**：
  - 嵌入 ID
  - 创建时间
  - 文档数量
  - 预处理配置摘要

- **操作**：
  - **选择嵌入**：点击嵌入文件选择用于分析
  - **重命名嵌入**：点击编辑图标重命名嵌入文件
  - **删除嵌入**：点击删除图标删除嵌入文件

**注意事项**：
- 嵌入文件与语料库和预处理配置绑定
- 如果预处理配置改变，需要重新生成嵌入
- 嵌入文件可以重复使用，避免重复计算

### 动态主题分析

动态主题分析用于分析主题随时间的变化：

#### 配置选项

- **启用动态主题**：勾选以启用动态主题分析
- **日期格式**：
  - **仅年份**：只使用年份信息
  - **完整日期**：使用完整的日期信息

- **时间段数量**：
  - 范围：2-50（默认：自动）
  - 设置时间段的数量
  - 如果设置为自动，系统会根据数据自动确定

- **演化调整**：是否对主题演化进行调整
- **全局调整**：是否进行全局调整

**注意事项**：
- 动态主题分析需要文本包含日期元数据
- 日期格式必须正确，否则可能无法分析
- 时间段数量会影响分析的粒度

### 分析配置

分析面板用于配置 BERTopic 分析参数：

#### 降维配置

- **降维方法**：
  - **UMAP**：统一流形逼近与投影（推荐）
  - **PCA**：主成分分析

- **UMAP 参数**：
  - **邻居数（n_neighbors）**：范围 2-100（默认：15）
  - **组件数（n_components）**：范围 2-10（默认：5）
  - **最小距离（min_dist）**：范围 0.0-1.0（默认：0.1）
  - **距离度量（metric）**：cosine、euclidean 等（默认：cosine）
  - **随机种子（random_state）**：用于结果可重复性

- **PCA 参数**：
  - **组件数（n_components）**：范围 2-10（默认：5）

#### 聚类配置

- **聚类方法**：
  - **HDBSCAN**：基于密度的层次聚类（推荐）
  - **BIRCH**：平衡迭代规约和聚类
  - **K-Means**：K 均值聚类

- **HDBSCAN 参数**：
  - **最小簇大小（min_cluster_size）**：范围 2-50（默认：5）
  - **最小样本数（min_samples）**：范围 1-50（默认：自动）
  - **距离度量（metric）**：euclidean、manhattan 等（默认：euclidean）
  - **簇选择方法（cluster_selection_method）**：eom、leaf（默认：eom）
  - **允许单簇（allow_single_cluster）**：是否允许所有文档在一个簇中

- **BIRCH 参数**：
  - **阈值（threshold）**：范围 0.1-2.0（默认：0.5）
  - **分支因子（branching_factor）**：范围 2-100（默认：50）
  - **簇数（n_clusters）**：范围 2-100（默认：自动）

- **K-Means 参数**：
  - **簇数（n_clusters）**：范围 2-100（默认：5）
  - **初始化方法（init）**：k-means++、random（默认：k-means++）
  - **最大迭代次数（max_iter）**：范围 10-1000（默认：300）

#### 向量器配置

- **向量器类型**：
  - **CountVectorizer**：词频向量器（默认）
  - **TfidfVectorizer**：TF-IDF 向量器

- **通用参数**：
  - **最小文档频率（min_df）**：范围 0.0-1.0 或 1-100（默认：1）
  - **最大文档频率（max_df）**：范围 0.0-1.0（默认：1.0）
  - **N-gram 范围（ngram_range）**：1-1、1-2、2-2 等（默认：1-1）
  - **移除停用词**：是否移除停用词（默认：是）

#### 主题表示模型

- **c-TF-IDF**：默认方法，基于 TF-IDF 的主题表示
- **KeyBERTInspired**：基于 KeyBERT 的关键词提取
  - **Top N 词数**：范围 5-20（默认：10）
  - **代表性文档数**：范围 1-20（默认：5）
  - **样本数**：范围 100-1000（默认：500）
  - **候选词数**：范围 50-200（默认：100）

- **MaximalMarginalRelevance (MMR)**：最大边际相关性，优化多样性
  - **多样性（diversity）**：范围 0.0-1.0（默认：0.3）
  - **Top N 词数**：范围 5-20（默认：10）

- **PartOfSpeech**：基于词性的主题表示（仅支持 lg 模型）
  - **Top N 词数**：范围 5-20（默认：10）
  - **词性模式**：如 [['NOUN'], ['ADJ', 'NOUN']]

#### 离群值处理

- **启用离群值处理**：是否处理离群值文档
- **处理策略**：
  - **distributions**：基于分布的策略
  - **probabilities**：基于概率的策略
  - **c-tf-idf**：基于 c-TF-IDF 的策略
  - **embeddings**：基于嵌入的策略

- **阈值**：范围 0.0-1.0（默认：0.0）
- **离群值预估**：
  - 点击「预估离群值」按钮可以预估处理后的离群值数量
  - 需要先运行分析才能进行预估
  - 预估结果会显示当前离群值数量和预估后的数量

### 运行分析

配置完成后，点击「开始分析」按钮：

1. 系统会显示加载进度
2. 分析完成后，结果会自动显示在右侧面板
3. 如果出现错误，会在配置面板显示错误信息

**注意事项**：
- 分析过程可能需要较长时间，取决于文本数量和参数设置
- 大语料库分析可能需要几分钟甚至更长时间
- 建议先在小规模数据上测试参数设置

### 分析结果

#### 统计摘要

结果顶部显示统计摘要：

- **主题数量**：识别出的主题数量
- **文档数量**：分析的文档总数
- **离群值数量**：被标记为离群值的文档数量
- **平均主题概率**：文档属于主题的平均概率

#### 主题卡片

每个主题显示为一个卡片：

- **主题 ID**：主题的编号（-1 表示离群值）
- **主题名称**：主题的名称（可以自定义或使用 Ollama 生成）
- **文档数量**：属于该主题的文档数量
- **关键词**：主题的关键词及其权重
- **操作**：
  - **编辑标签**：自定义主题名称
  - **生成名称**：使用 Ollama 生成主题名称（需要连接 Ollama）
  - **查看文档**：查看属于该主题的文档
  - **合并主题**：选择多个主题进行合并

#### 主题表格

主题表格显示所有主题的详细信息：

- **列**：
  - **主题 ID**：主题编号
  - **主题名称**：主题名称
  - **文档数量**：属于该主题的文档数量
  - **关键词**：主题的关键词（可配置显示数量，1-20）

- **功能**：
  - **排序**：可以按文档数量排序
  - **导出 CSV**：导出主题数据为 CSV 文件
  - **关键词数量配置**：设置表格中显示的关键词数量

#### 主题操作

- **生成主题名称**：
  - 点击「生成主题名称」按钮
  - 需要连接 Ollama 服务
  - 系统会为所有主题生成名称

- **合并主题**：
  - 选择多个主题
  - 点击「合并主题」按钮
  - 系统会合并选中的主题

- **编辑标签**：
  - 点击主题卡片或表格中的编辑图标
  - 输入自定义标签
  - 保存后标签会应用到可视化和导出

### 可视化

可视化面板提供多种 D3.js 可视化图表：

#### 主题词条形图（Barchart）

- **特点**：横向条形图，显示每个主题的关键词及其权重
- **配置参数**：
  - **Top N 主题**：显示前 N 个主题（默认：8）
  - **每个主题的词数**：每个主题显示的关键词数量（默认：5）
  - **使用自定义标签**：是否使用自定义主题标签

#### 文档散点图（Documents）

- **特点**：UMAP 降维后的文档分布图，显示文档在主题空间中的位置
- **配置参数**：
  - **隐藏文档悬停**：是否隐藏文档悬停提示
  - **样本大小**：显示的文档数量（默认：2000）
  - **使用自定义标签**：是否使用自定义主题标签

- **交互功能**：
  - **点击图例**：切换主题的显示/隐藏
  - **悬停文档**：显示文档信息
  - **离群值显示**：离群值文档显示为灰色

#### 相似性热图（Heatmap）

- **特点**：主题相似度矩阵，显示主题之间的相似性
- **配置参数**：
  - **Top N 主题**：显示前 N 个主题
  - **聚类数**：对主题进行聚类显示
  - **使用自定义标签**：是否使用自定义主题标签

#### 词项排名（Term Rank）

- **特点**：词权重衰减折线图，显示每个主题中词的权重排名
- **配置参数**：
  - **对数刻度**：是否使用对数刻度
  - **词数量**：显示的词数量（默认：30）
  - **使用自定义标签**：是否使用自定义主题标签

- **交互功能**：
  - **悬停主题**：高亮显示该主题，淡化其他主题
  - **点击图例**：切换主题的显示/隐藏

#### 主题时间演化（Topics over Time）

- **特点**：主题频率随时间变化的折线图（需要启用动态主题）
- **配置参数**：
  - **Top N 主题**：显示前 N 个主题
  - **标准化频率**：是否标准化频率
  - **使用自定义标签**：是否使用自定义主题标签

- **交互功能**：
  - **点击图例**：切换主题的显示/隐藏
  - **悬停数据点**：显示详细的时间频率信息

#### 导出功能

所有可视化图表都支持导出：

- **导出 SVG**：导出为矢量图格式，适合打印和编辑
- **导出 PNG**：导出为位图格式，适合插入文档

## LDA

### 概述

LDA（Latent Dirichlet Allocation，潜在狄利克雷分配）是一种基于概率的主题建模方法，假设文档是由多个主题的混合生成的，每个主题由一组词的分布表示。

### 工作流程

LDA 的工作流程包括以下步骤：

1. **预处理**：对文本进行分词、词性筛选、停用词移除等
2. **构建文档-词矩阵**：将文本转换为文档-词频矩阵
3. **训练 LDA 模型**：使用 Gensim 或 Scikit-learn 训练 LDA 模型
4. **提取主题**：从训练好的模型中提取主题和关键词
5. **动态主题分析**（可选）：分析主题随时间的变化

### 语料选择

与其他分析模块相同，支持三种文本选择模式。

### 预处理配置

LDA 预处理面板用于配置文本预处理选项：

#### 语言相关预处理

- **中文**：使用 jieba 分词
- **英文**：使用 SpaCy 分词

#### 词性筛选

- **词性筛选模式**：
  - **保留模式**：只保留选中的词性
  - **过滤模式**：过滤掉选中的词性

- **词性选项**：
  - **SpaCy Universal POS**：NOUN、VERB、ADJ、ADV 等
  - 支持多选

#### 其他预处理选项

- **移除停用词**：是否移除停用词
- **词形还原**：是否进行词形还原
- **转小写**：是否转换为小写
- **最小词长**：设置最小词长度

#### 预处理预览

- **预览功能**：点击「预览预处理」按钮可以查看预处理结果
- **预览内容**：
  - 显示原始文本和处理后文本
  - 显示词数统计
  - 显示是否使用了 SpaCy 标注

### 动态主题分析

LDA 支持动态主题分析，用于分析主题随时间的变化：

#### 配置选项

- **启用动态主题**：勾选以启用动态主题分析
- **日期格式**：
  - **仅年份**：只使用年份信息
  - **完整日期**：使用完整的日期信息

- **时间段数量（nr_bins）**：
  - 范围：2-50（默认：自动）
  - 设置时间段的数量

- **演化调整（evolution_tuning）**：是否对主题演化进行调整
- **全局调整（global_tuning）**：是否进行全局调整

### LDA 参数配置

#### 基本参数

- **主题数（num_topics）**：范围 2-100（默认：5）
- **关键词数（num_keywords）**：范围 5-50（默认：10）

#### 引擎选择

- **Gensim**：使用 Gensim 库（推荐，支持更多功能）
- **Scikit-learn**：使用 Scikit-learn 库

#### Alpha 参数（主题分布的先验）

- **模式**：
  - **symmetric**：对称分布（所有主题权重相等）
  - **asymmetric**：非对称分布（主题权重可以不同）
  - **auto**：自动选择
  - **custom**：自定义分布

- **自定义 Alpha**：
  - 点击「自定义」可以设置每个主题的 Alpha 值
  - 使用滑块调整每个主题的权重
  - 可以重置为均匀分布

#### Eta 参数（词分布的先验）

- **模式**：
  - **symmetric**：对称分布
  - **auto**：自动选择

#### 迭代参数

- **迭代次数（iterations）**：范围 10-1000（默认：50）
- **更新频率（update_every）**：范围 1-100（默认：0，表示不更新）
- **评估频率（eval_every）**：范围 1-100（默认：10）

#### 随机种子

- **随机种子（random_state）**：用于结果可重复性

### 模型评估指标

LDA 提供多种评估指标：

- **困惑度（Perplexity）**：衡量模型对数据的拟合程度，越低越好
- **一致性（Coherence）**：衡量主题的语义一致性，越高越好
- **对数似然（Log Likelihood）**：模型的似然值

### 主题数优化

LDA 支持主题数优化功能：

1. **打开优化对话框**：点击「优化主题数」按钮
2. **设置范围**：
   - **最小主题数**：范围 2-20（默认：2）
   - **最大主题数**：范围 2-50（默认：20）
   - **步长**：范围 1-10（默认：2）

3. **运行优化**：
   - 系统会在指定范围内测试不同的主题数
   - 计算每个主题数的一致性分数
   - 显示一致性曲线图

4. **选择最优主题数**：
   - 查看一致性曲线
   - 选择一致性最高的主题数
   - 应用到参数配置

### 运行分析

配置完成后，点击「开始分析」按钮运行 LDA 分析。

### 分析结果

#### 统计摘要

- **主题数量**：识别出的主题数量
- **文档数量**：分析的文档总数
- **评估指标**：困惑度、一致性、对数似然

#### 主题卡片和表格

与 BERTopic 类似，显示主题的关键词、文档数量等信息。

#### 动态主题结果

如果启用了动态主题分析，会显示：

- **主题时间演化折线图**：显示主题频率随时间的变化
- **主题相似度热力图**：显示不同时间段主题之间的相似性
- **主题演化桑基图**：显示主题在不同时间段之间的流转

### 可视化

LDA 可视化面板提供多种图表：

- **主题词条形图**：显示每个主题的关键词及其权重
- **主题分布饼图**：显示主题的文档分布
- **文档分布图**：显示文档的主题分布
- **主题时间演化图**（如果启用动态主题）：显示主题随时间的变化
- **主题相似度热力图**（如果启用动态主题）：显示主题相似性
- **主题演化桑基图**（如果启用动态主题）：显示主题流转

## LSA

### 概述

LSA（Latent Semantic Analysis，潜在语义分析）是一种基于矩阵分解的主题建模方法，使用奇异值分解（SVD）来发现文档和词之间的潜在语义关系。

### 工作流程

LSA 的工作流程包括以下步骤：

1. **预处理**：对文本进行分词、词性筛选、停用词移除等
2. **构建 TF-IDF 矩阵**：将文本转换为 TF-IDF 文档-词矩阵
3. **SVD 分解**：使用 TruncatedSVD 进行奇异值分解
4. **提取主题**：从分解结果中提取主题和关键词

### 语料选择

与其他分析模块相同，支持三种文本选择模式。

### 预处理配置

LSA 预处理配置与 LDA 相同，包括语言相关预处理、词性筛选等。

### LSA 参数配置

#### 基本参数

- **主题数（num_topics）**：范围 2-100（默认：5）
- **关键词数（num_keywords）**：范围 5-50（默认：10）

#### SVD 算法

- **算法类型**：
  - **arpack**：使用 ARPACK 求解器（推荐）
  - **randomized**：使用随机化算法（适合大规模数据）

#### 高级参数

- **最大特征数（n_components）**：范围 2-1000（默认：自动）
- **迭代次数（n_iter）**：范围 1-100（默认：5）
- **收敛容差（tol）**：范围 0.0-1.0（默认：0.0）
- **随机种子（random_state）**：用于结果可重复性

### 模型评估指标

LSA 提供以下评估指标：

- **解释方差比**：每个主题解释的方差比例
- **累积方差**：累积解释的方差比例
- **奇异值和**：所有奇异值的总和

### 主题数优化

LSA 支持主题数优化功能：

1. **打开优化对话框**：点击「优化主题数」按钮
2. **设置范围**：设置最小、最大主题数和步长
3. **运行优化**：
   - 系统会在指定范围内测试不同的主题数
   - 计算每个主题数的解释方差
   - 显示解释方差曲线图

4. **选择最优主题数**：
   - 查看解释方差曲线
   - 选择解释方差较高的主题数（通常选择曲线拐点）
   - 应用到参数配置

### 运行分析

配置完成后，点击「开始分析」按钮运行 LSA 分析。

### 分析结果

#### 统计摘要

- **主题数量**：识别出的主题数量
- **文档数量**：分析的文档总数
- **评估指标**：解释方差比、累积方差、奇异值和

#### 主题卡片和表格

与 BERTopic 类似，显示主题的关键词、文档数量等信息。

### 可视化

LSA 可视化面板提供多种图表：

- **主题词条形图**：显示每个主题的关键词及其权重
- **方差图**：显示每个主题的解释方差
- **文档分布图**：显示文档的主题分布

## NMF

### 概述

NMF（Non-negative Matrix Factorization，非负矩阵分解）是一种基于矩阵分解的主题建模方法，将文档-词矩阵分解为两个非负矩阵，分别表示文档-主题分布和主题-词分布。

### 工作流程

NMF 的工作流程包括以下步骤：

1. **预处理**：对文本进行分词、词性筛选、停用词移除等
2. **构建 TF-IDF 矩阵**：将文本转换为 TF-IDF 文档-词矩阵
3. **NMF 分解**：使用 NMF 进行非负矩阵分解
4. **提取主题**：从分解结果中提取主题和关键词

### 语料选择

与其他分析模块相同，支持三种文本选择模式。

### 预处理配置

NMF 预处理配置与 LDA 相同，包括语言相关预处理、词性筛选等。

### NMF 参数配置

#### 基本参数

- **主题数（num_topics）**：范围 2-100（默认：5）
- **关键词数（num_keywords）**：范围 5-50（默认：10）

#### 初始化方法

- **NNDSVD**：非负双奇异值分解（推荐）
- **NNDSVDA**：NNDSVD 变体 A
- **NNDSVDAR**：NNDSVD 变体 AR
- **Random**：随机初始化

#### 求解器

- **cd**：坐标下降法（推荐）
- **mu**：乘法更新法

#### 高级参数

- **最大迭代次数（max_iter）**：范围 10-1000（默认：200）
- **收敛容差（tol）**：范围 0.0-1.0（默认：1e-4）
- **Alpha_W**：W 矩阵的正则化参数（范围：0.0-1.0，默认：0.0）
- **Alpha_H**：H 矩阵的正则化参数（范围：0.0-1.0，默认：0.0）
- **L1 比率（l1_ratio）**：L1 正则化的比例（范围：0.0-1.0，默认：0.0）
- **随机种子（random_state）**：用于结果可重复性

#### 求解器特定参数

- **MU 求解器**：
  - **Beta 损失（beta_loss）**：frobenius、kullback-leibler 等（默认：frobenius）

- **CD 求解器**：
  - **Shuffle**：是否在每次迭代时打乱数据

### 模型评估指标

NMF 提供以下评估指标：

- **重构误差**：矩阵重构的误差，越低越好
- **稀疏度**：矩阵的稀疏程度
- **迭代次数**：实际迭代次数

### 主题数优化

NMF 支持主题数优化功能：

1. **打开优化对话框**：点击「优化主题数」按钮
2. **设置范围**：设置最小、最大主题数和步长
3. **运行优化**：
   - 系统会在指定范围内测试不同的主题数
   - 计算每个主题数的重构误差
   - 显示重构误差曲线图

4. **选择最优主题数**：
   - 查看重构误差曲线
   - 选择重构误差较低的主题数（通常选择曲线拐点）
   - 应用到参数配置

### 运行分析

配置完成后，点击「开始分析」按钮运行 NMF 分析。

### 分析结果

#### 统计摘要

- **主题数量**：识别出的主题数量
- **文档数量**：分析的文档总数
- **评估指标**：重构误差、稀疏度、迭代次数

#### 主题卡片和表格

与 BERTopic 类似，显示主题的关键词、文档数量等信息。

### 可视化

NMF 可视化面板提供多种图表：

- **主题词条形图**：显示每个主题的关键词及其权重
- **主题分布饼图**：显示主题的文档分布
- **文档分布图**：显示文档的主题分布

## 使用技巧

### 方法选择建议

#### 选择 BERTopic 的场景

- 需要利用语义相似性的场景
- 文本数量较大，需要自动确定主题数
- 需要处理多语言文本
- 需要灵活的主题表示方法

#### 选择 LDA 的场景

- 需要概率解释的场景
- 需要分析主题的时间演化
- 需要控制主题的先验分布
- 需要详细的模型评估指标

#### 选择 LSA 的场景

- 需要快速分析
- 文本数量很大
- 需要降维和语义分析
- 不需要概率解释

#### 选择 NMF 的场景

- 需要非负分解的场景
- 需要稀疏的主题表示
- 需要快速分析
- 文本数量较大

### 参数调优建议

#### BERTopic 参数调优

1. **降维参数**：
   - 如果主题重叠较多，可以增加 `n_components`
   - 如果主题分离不够，可以调整 `min_dist`

2. **聚类参数**：
   - 如果主题数量太多，可以增加 `min_cluster_size`
   - 如果主题数量太少，可以减少 `min_cluster_size`

3. **向量器参数**：
   - 如果关键词不够相关，可以调整 `min_df` 和 `max_df`
   - 如果需要短语，可以设置 `ngram_range` 为 1-2

#### LDA 参数调优

1. **主题数**：
   - 使用主题数优化功能找到最优主题数
   - 查看一致性曲线选择拐点

2. **Alpha 和 Eta**：
   - 对称分布适合主题数量相近的场景
   - 非对称分布适合主题数量差异较大的场景

3. **迭代次数**：
   - 增加迭代次数可以提高模型质量，但会增加计算时间
   - 通常 50-100 次迭代已经足够

#### LSA 参数调优

1. **主题数**：
   - 使用主题数优化功能找到最优主题数
   - 查看解释方差曲线选择拐点

2. **SVD 算法**：
   - 小规模数据使用 `arpack`
   - 大规模数据使用 `randomized`

#### NMF 参数调优

1. **主题数**：
   - 使用主题数优化功能找到最优主题数
   - 查看重构误差曲线选择拐点

2. **初始化方法**：
   - 推荐使用 `NNDSVD` 或 `NNDSVDA`
   - `Random` 初始化结果可能不稳定

3. **正则化参数**：
   - 增加 `Alpha_W` 和 `Alpha_H` 可以增加稀疏性
   - 调整 `l1_ratio` 可以控制 L1/L2 正则化的比例

### 常见分析场景

#### 发现研究主题

1. 选择 BERTopic 或 LDA
2. 使用主题数优化功能确定主题数
3. 查看主题关键词理解主题内容
4. 使用可视化图表分析主题分布

#### 分析主题演化

1. 选择 LDA 并启用动态主题分析
2. 配置日期格式和时间段数量
3. 查看主题时间演化图
4. 分析主题相似度热力图和演化桑基图

#### 文档分类

1. 选择 BERTopic 或 NMF
2. 设置合适的主题数
3. 查看文档的主题分布
4. 根据主题分布对文档进行分类

#### 关键词提取

1. 选择任意方法
2. 查看主题的关键词
3. 使用主题表示模型优化关键词
4. 导出关键词用于其他分析

### 注意事项

- 主题建模是探索性分析，结果需要人工解释和验证
- 不同的参数设置会产生不同的结果，建议多次尝试
- 主题数选择很重要，过多或过少都会影响结果质量
- 预处理对结果影响很大，建议仔细配置预处理选项
- 大语料库分析可能需要较长时间，请耐心等待
- 动态主题分析需要文本包含日期元数据
- 主题数优化功能会测试多个主题数，可能需要较长时间
- 可视化图表在主题数量很多时可能显示较慢
- 导出功能在数据量大时可能需要一些时间
- BERTopic 的嵌入文件可以重复使用，避免重复计算
- LDA 和 NMF 的主题数优化功能可以帮助找到最优主题数

# 词典查询

## 支持的词典

## 搜索功能

# 应用设置

应用设置模块用于个性化配置 Meta-Lingo 的各项功能和外观。

## 界面语言

切换应用的界面语言：

- **中文 (zh)**：简体中文界面
- **英文 (en)**：英文界面

切换语言后，所有界面文字、提示信息、帮助文档都会相应更改。

## 壁纸设置

### 主题模式

选择应用的主题模式：

- **浅色模式**：白色背景，适合明亮环境
- **深色模式**：深色背景，适合夜间使用，减少眼睛疲劳

### 自定义壁纸

上传自定义壁纸作为应用背景：

1. 点击「上传壁纸」按钮选择图片文件
2. 支持 JPG、PNG 等常见图片格式
3. 壁纸会自动适应窗口大小（等比缩放，不会扭曲）
4. 点击「更换壁纸」可以替换当前壁纸
5. 点击「移除壁纸」可以删除壁纸

### 壁纸透明度

调节壁纸的透明度：

- **范围**：5% - 50%
- **步长**：5%
- **默认**：30%
- 较低的透明度使壁纸更隐约，不影响阅读
- 较高的透明度使壁纸更明显

## Ollama 连接

配置本地大语言模型（LLM）连接，用于主题建模中的主题标签生成等功能。

### 连接设置

- **URL**：Ollama 服务地址（默认：`http://localhost:11434`）
- **连接状态**：显示当前连接状态（已连接/未连接）

### 模型选择

连接成功后，可以从可用模型列表中选择：

- 系统会自动获取 Ollama 中已安装的模型
- 选择模型后，主题建模等功能将使用该模型

### 使用说明

1. 确保已安装并运行 Ollama（[https://ollama.ai](https://ollama.ai)）
2. 在 Ollama 中下载所需模型（如 `ollama pull llama2`）
3. 在 Meta-Lingo 中配置连接并选择模型

## USAS 标注模式

选择 USAS 语义域标注使用的方法：

### 规则模式 (Rule-based)

使用 PyMUSAS 规则标注器：

- 基于词典和规则进行标注
- 支持多词表达式（MWE）识别
- 结合话语域识别和一文一义规则进行消歧
- **优点**：速度快，可解释性强
- **适用**：一般场景

### 神经网络模式 (Neural)

使用神经网络模型进行标注：

- 基于预训练的语义标注模型
- 能够处理更复杂的语境
- **优点**：上下文理解能力强
- **适用**：需要更准确语义理解的场景

### 混合模式 (Hybrid)

结合规则和神经网络方法：

- 首先使用规则方法进行初步标注
- 对于不确定的情况使用神经网络辅助消歧
- **优点**：兼顾速度和准确性
- **适用**：追求最佳效果的场景

## USAS 语义域配置

为不同文本类型配置优先语义域，用于 USAS 标注时的消歧。

### 文本类型配置

#### 预设文本类型

系统预设了常见的文本类型：

- **GEN**（一般文本）：通用配置
- 可以编辑每种类型的优先语义域

#### 自定义文本类型

1. 点击「添加类型」创建自定义文本类型
2. 输入类型名称（如"医学文献"、"法律文本"）
3. 配置该类型的优先语义域

### 优先语义域配置

1. 选择要编辑的文本类型
2. 点击「编辑」按钮
3. 在弹出的选择器中选择优先语义域
4. 可以选择多个语义域
5. 配置完成后保存

**作用**：上传语料时选择的文本类型将决定 USAS 标注使用的优先语义域，有助于提高标注准确性。

## 许可证

点击「查看许可证」可以查看应用的许可协议：

- 根据当前语言显示中文或英文版本
- 在弹窗中显示完整许可证内容

## 恢复出厂设置

**警告**：此操作不可撤销！

### 可选重置项

以下项目可以选择是否重置：

- **数据库记录**：语料库元数据、文本条目、标签、处理任务记录
- **语料库文件**：文本、音频、视频、转录文件等实际文件
- **标注存档**：所有保存的标注存档
- **标注框架**：所有自定义的标注框架

### 自动重置项

以下项目始终会被重置：

- 主题建模数据（嵌入、分析结果等）
- Word2Vec 模型
- USAS 配置

### 执行重置

1. 勾选要重置的项目
2. 在确认框中输入 "RESET"
3. 点击确认按钮

**注意**：如果不使用恢复出厂设置功能，删除应用或重装新版本不会丢失您的语料库数据。

